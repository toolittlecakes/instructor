{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Instructor","text":"<p>Structured outputs powered by llms. Designed for simplicity, transparency, and control.</p> <p> </p> <p>Instructor makes it easy to reliably get structured data like JSON from Large Language Models (LLMs) like GPT-3.5, GPT-4, GPT-4-Vision, including open source models like Mistral/Mixtral from Together, Anyscale, Ollama, and llama-cpp-python.</p> <p>By leveraging various modes like Function Calling, Tool Calling and even constrained sampling modes like JSON mode, JSON Schema; Instructor stands out for its simplicity, transparency, and user-centric design. We leverage Pydantic to do the heavy lifting, and we've built a simple, easy-to-use API on top of it by helping you manage validation context, retries with Tenacity, and streaming Lists and Partial responses.</p> <p>We also provide library in Typescript and Elixir.</p>"},{"location":"#usage","title":"Usage","text":"<pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# This enables response_model keyword\n# from client.chat.completions.create\nclient = instructor.patch(OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)\n\nassert isinstance(user, UserDetail)\nassert user.name == \"Jason\"\nassert user.age == 25\nprint(user.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Jason\",\n  \"age\": 25\n}\n\"\"\"\n</code></pre> <p>Using async clients</p> <p>For async clients you must use <code>apatch</code> vs <code>patch</code> like so:</p> <pre><code>import asyncio\nimport instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\naclient = instructor.apatch(AsyncOpenAI())\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\ntask = aclient.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nresponse = asyncio.run(task)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Jason\",\n  \"age\": 25\n}\n\"\"\"\n</code></pre> <p>Accessing the original response and usage tokens</p> <p>If you want to access anything like usage or other metadata, the original response is available on the <code>Model._raw_response</code> attribute.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.patch(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)\n\nprint(user._raw_response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"id\": \"chatcmpl-8u9e2TV3ehCgLsRxNLLeAbzpEmBuZ\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": null,\n        \"role\": \"assistant\",\n        \"function_call\": null,\n        \"tool_calls\": [\n          {\n            \"id\": \"call_3ZuQhfteTLEy7CUokjwnLBHr\",\n            \"function\": {\n              \"arguments\": \"{\\\"name\\\":\\\"Jason\\\",\\\"age\\\":25}\",\n              \"name\": \"UserDetail\"\n            },\n            \"type\": \"function\"\n          }\n        ]\n      }\n    }\n  ],\n  \"created\": 1708394134,\n  \"model\": \"gpt-3.5-turbo-0125\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": \"fp_69829325d0\",\n  \"usage\": {\n    \"completion_tokens\": 9,\n    \"prompt_tokens\": 81,\n    \"total_tokens\": 90\n  }\n}\n\"\"\"\n</code></pre>"},{"location":"#why-use-instructor","title":"Why use Instructor?","text":"<p>The question of using Instructor is fundamentally a question of why to use Pydantic.</p> <ol> <li> <p>Powered by type hints \u2014 Instructor is powered by Pydantic, which is powered by type hints. Schema validation, prompting is controlled by type annotations; less to learn, less code to write, and integrates with your IDE.</p> </li> <li> <p>Powered by OpenAI \u2014 Instructor is powered by OpenAI's function calling API. This means you can use the same API for both prompting and extraction.</p> </li> <li> <p>Customizable \u2014 Pydantic is highly customizable. You can define your own validators, custom error messages, and more.</p> </li> <li> <p>Ecosystem Pydantic is the most widely used data validation library for Python. It's used by FastAPI, Typer, and many other popular libraries.</p> </li> <li> <p>Battle Tested \u2014 Pydantic is downloaded over 100M times per month, and supported by a large community of contributors.</p> </li> <li> <p>Easy Integration with CLI - We offer a variety of CLI tools like <code>instructor jobs</code>, <code>instructor files</code> and <code>instructor usage</code> to track your OpenAI usage, fine-tuning jobs and more, just check out our CLI Documentation to find out more.</p> </li> </ol>"},{"location":"#more-examples","title":"More Examples","text":"<p>If you'd like to see more check out our cookbook.</p> <p>Installing Instructor is a breeze. Just run <code>pip install instructor</code>.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you want to help out, checkout some of the issues marked as <code>good-first-issue</code> or <code>help-wanted</code>. Found here. They could be anything from code improvements, a guest blog post, or a new cook book.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT License.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#instructor.patch.apatch","title":"<code>apatch(client, mode=Mode.TOOLS)</code>","text":"<p>No longer necessary, use <code>patch</code> instead.</p> <p>Patch the <code>client.chat.completions.create</code> method</p> <p>Enables the following features:</p> <ul> <li><code>response_model</code> parameter to parse the response from OpenAI's API</li> <li><code>max_retries</code> parameter to retry the function if the response is not valid</li> <li><code>validation_context</code> parameter to validate the response using the pydantic model</li> <li><code>strict</code> parameter to use strict json parsing</li> </ul> Source code in <code>instructor/patch.py</code> <pre><code>def apatch(client: AsyncOpenAI, mode: Mode = Mode.TOOLS):\n    \"\"\"\n    No longer necessary, use `patch` instead.\n\n    Patch the `client.chat.completions.create` method\n\n    Enables the following features:\n\n    - `response_model` parameter to parse the response from OpenAI's API\n    - `max_retries` parameter to retry the function if the response is not valid\n    - `validation_context` parameter to validate the response using the pydantic model\n    - `strict` parameter to use strict json parsing\n    \"\"\"\n    import warnings\n\n    warnings.warn(\n        \"apatch is deprecated, use patch instead\", DeprecationWarning, stacklevel=2\n    )\n    return patch(client, mode=mode)\n</code></pre>"},{"location":"api/#instructor.patch.dump_message","title":"<code>dump_message(message)</code>","text":"<p>Dumps a message to a dict, to be returned to the OpenAI API. Workaround for an issue with the OpenAI API, where the <code>tool_calls</code> field isn't allowed to be present in requests if it isn't used.</p> Source code in <code>instructor/patch.py</code> <pre><code>def dump_message(message: ChatCompletionMessage) -&gt; ChatCompletionMessageParam:\n    \"\"\"Dumps a message to a dict, to be returned to the OpenAI API.\n    Workaround for an issue with the OpenAI API, where the `tool_calls` field isn't allowed to be present in requests\n    if it isn't used.\n    \"\"\"\n    ret: ChatCompletionMessageParam = {\n        \"role\": message.role,\n        \"content\": message.content or \"\",\n    }\n    if hasattr(message, \"tool_calls\") and message.tool_calls is not None:\n        ret[\"tool_calls\"] = message.model_dump()[\"tool_calls\"]\n    if hasattr(message, \"function_call\") and message.function_call is not None:\n        ret[\"content\"] += json.dumps(message.model_dump()[\"function_call\"])\n    return ret\n</code></pre>"},{"location":"api/#instructor.patch.handle_response_model","title":"<code>handle_response_model(response_model, mode=Mode.TOOLS, **kwargs)</code>","text":"<p>Prepare the response model type hint, and returns the response_model along with the new modified kwargs needed to be able to use the response_model parameter with the patch function.</p> <p>Parameters:</p> Name Type Description Default <code>response_model</code> <code>T</code> <p>The response model to use for parsing the response</p> required <code>mode</code> <code>Mode</code> <p>The openai completion mode. Defaults to Mode.TOOLS.</p> <code>TOOLS</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>When using stream=True with a non-iterable response_model</p> <code>ValueError</code> <p>When using an invalid patch mode</p> <p>Returns:</p> Type Description <code>Union[Type[OpenAISchema], dict]</code> <p>Union[Type[OpenAISchema], dict]: The response model to use for parsing the response</p> Source code in <code>instructor/patch.py</code> <pre><code>def handle_response_model(\n    response_model: T, mode: Mode = Mode.TOOLS, **kwargs\n) -&gt; Union[Type[OpenAISchema], dict]:\n    \"\"\"Prepare the response model type hint, and returns the response_model\n    along with the new modified kwargs needed to be able to use the response_model\n    parameter with the patch function.\n\n\n    Args:\n        response_model (T): The response model to use for parsing the response\n        mode (Mode, optional): The openai completion mode. Defaults to Mode.TOOLS.\n\n    Raises:\n        NotImplementedError: When using stream=True with a non-iterable response_model\n        ValueError: When using an invalid patch mode\n\n    Returns:\n        Union[Type[OpenAISchema], dict]: The response model to use for parsing the response\n    \"\"\"\n    new_kwargs = kwargs.copy()\n    if response_model is not None:\n        # Handles the case where the response_model is a simple type\n        # Literal, Annotated, Union, str, int, float, bool, Enum\n        # We wrap the response_model in a ModelAdapter that sets 'content' as the response\n        if is_simple_type(response_model):\n            response_model = ModelAdapter[response_model]\n\n        # This a special case for parallel tools\n        if mode == Mode.PARALLEL_TOOLS:\n            assert (\n                new_kwargs.get(\"stream\", False) is False\n            ), \"stream=True is not supported when using PARALLEL_TOOLS mode\"\n            new_kwargs[\"tools\"] = handle_parallel_model(response_model)\n            new_kwargs[\"tool_choice\"] = \"auto\"\n\n            # This is a special case for parallel models\n            response_model = ParallelModel(typehint=response_model)\n            return response_model, new_kwargs\n\n        # This is for all other single model cases\n        if get_origin(response_model) is Iterable:\n            iterable_element_class = get_args(response_model)[0]\n            response_model = IterableModel(iterable_element_class)\n        if not issubclass(response_model, OpenAISchema):\n            response_model = openai_schema(response_model)  # type: ignore\n\n        if new_kwargs.get(\"stream\", False) and not issubclass(\n            response_model, (IterableBase, PartialBase)\n        ):\n            raise NotImplementedError(\n                \"stream=True is not supported when using response_model parameter for non-iterables\"\n            )\n\n        if mode == Mode.FUNCTIONS:\n            new_kwargs[\"functions\"] = [response_model.openai_schema]  # type: ignore\n            new_kwargs[\"function_call\"] = {\"name\": response_model.openai_schema[\"name\"]}  # type: ignore\n        elif mode == Mode.TOOLS:\n            new_kwargs[\"tools\"] = [\n                {\n                    \"type\": \"function\",\n                    \"function\": response_model.openai_schema,\n                }\n            ]\n            new_kwargs[\"tool_choice\"] = {\n                \"type\": \"function\",\n                \"function\": {\"name\": response_model.openai_schema[\"name\"]},\n            }\n        elif mode in {Mode.JSON, Mode.MD_JSON, Mode.JSON_SCHEMA}:\n            # If its a JSON Mode we need to massage the prompt a bit\n            # in order to get the response we want in a json format\n            message = f\"\"\"\n                As a genius expert, your task is to understand the content and provide\n                the parsed objects in json that match the following json_schema:\\n\n                {response_model.model_json_schema()['properties']}\n                \"\"\"\n            # Check for nested models\n            if \"$defs\" in response_model.model_json_schema():\n                message += f\"\\nHere are some more definitions to adhere too:\\n{response_model.model_json_schema()['$defs']}\"\n\n            if mode == Mode.JSON:\n                new_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n\n            elif mode == Mode.JSON_SCHEMA:\n                new_kwargs[\"response_format\"] = {\n                    \"type\": \"json_object\",\n                    \"schema\": response_model.model_json_schema(),\n                }\n\n            elif mode == Mode.MD_JSON:\n                new_kwargs[\"messages\"].append(\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": \"Here is the perfectly correctly formatted JSON\\n```json\",\n                    },\n                )\n                new_kwargs[\"stop\"] = \"```\"\n            # check that the first message is a system message\n            # if it is not, add a system message to the beginning\n            if new_kwargs[\"messages\"][0][\"role\"] != \"system\":\n                new_kwargs[\"messages\"].insert(\n                    0,\n                    {\n                        \"role\": \"system\",\n                        \"content\": message,\n                    },\n                )\n            # if it is, system append the schema to the end\n            else:\n                new_kwargs[\"messages\"][0][\"content\"] += f\"\\n\\n{message}\"\n        else:\n            raise ValueError(f\"Invalid patch mode: {mode}\")\n    return response_model, new_kwargs\n</code></pre>"},{"location":"api/#instructor.patch.is_async","title":"<code>is_async(func)</code>","text":"<p>Returns true if the callable is async, accounting for wrapped callables</p> Source code in <code>instructor/patch.py</code> <pre><code>def is_async(func: Callable) -&gt; bool:\n    \"\"\"Returns true if the callable is async, accounting for wrapped callables\"\"\"\n    return inspect.iscoroutinefunction(func) or (\n        hasattr(func, \"__wrapped__\") and inspect.iscoroutinefunction(func.__wrapped__)\n    )\n</code></pre>"},{"location":"api/#instructor.patch.patch","title":"<code>patch(client=None, create=None, mode=Mode.TOOLS)</code>","text":"<p>Patch the <code>client.chat.completions.create</code> method</p> <p>Enables the following features:</p> <ul> <li><code>response_model</code> parameter to parse the response from OpenAI's API</li> <li><code>max_retries</code> parameter to retry the function if the response is not valid</li> <li><code>validation_context</code> parameter to validate the response using the pydantic model</li> <li><code>strict</code> parameter to use strict json parsing</li> </ul> Source code in <code>instructor/patch.py</code> <pre><code>def patch(\n    client: Union[OpenAI, AsyncOpenAI] = None,\n    create: Callable[T_ParamSpec, T_Retval] = None,\n    mode: Mode = Mode.TOOLS,\n) -&gt; Union[OpenAI, AsyncOpenAI]:\n    \"\"\"\n    Patch the `client.chat.completions.create` method\n\n    Enables the following features:\n\n    - `response_model` parameter to parse the response from OpenAI's API\n    - `max_retries` parameter to retry the function if the response is not valid\n    - `validation_context` parameter to validate the response using the pydantic model\n    - `strict` parameter to use strict json parsing\n    \"\"\"\n\n    logger.debug(f\"Patching `client.chat.completions.create` with {mode=}\")\n\n    if create is not None:\n        func = create\n    elif client is not None:\n        func = client.chat.completions.create\n    else:\n        raise ValueError(\"Either client or create must be provided\")\n\n    func_is_async = is_async(func)\n\n    @wraps(func)\n    async def new_create_async(\n        response_model: Type[T_Model] = None,\n        validation_context: dict = None,\n        max_retries: int = 1,\n        *args: T_ParamSpec.args,\n        **kwargs: T_ParamSpec.kwargs,\n    ) -&gt; T_Model:\n        response_model, new_kwargs = handle_response_model(\n            response_model=response_model, mode=mode, **kwargs\n        )\n        response = await retry_async(\n            func=func,\n            response_model=response_model,\n            validation_context=validation_context,\n            max_retries=max_retries,\n            args=args,\n            kwargs=new_kwargs,\n            mode=mode,\n        )  # type: ignore\n        return response\n\n    @wraps(func)\n    def new_create_sync(\n        response_model: Type[T_Model] = None,\n        validation_context: dict = None,\n        max_retries: int = 1,\n        *args: T_ParamSpec.args,\n        **kwargs: T_ParamSpec.kwargs,\n    ) -&gt; T_Model:\n        response_model, new_kwargs = handle_response_model(\n            response_model=response_model, mode=mode, **kwargs\n        )\n        response = retry_sync(\n            func=func,\n            response_model=response_model,\n            validation_context=validation_context,\n            max_retries=max_retries,\n            args=args,\n            kwargs=new_kwargs,\n            mode=mode,\n        )\n        return response\n\n    new_create = new_create_async if func_is_async else new_create_sync\n    new_create.__doc__ = OVERRIDE_DOCS\n\n    if client is not None:\n        client.chat.completions.create = new_create\n        return client\n    else:\n        return new_create\n</code></pre>"},{"location":"api/#instructor.patch.process_response","title":"<code>process_response(response, *, response_model, stream, validation_context=None, strict=None, mode=Mode.TOOLS)</code>","text":"<p>Processes a OpenAI response with the response model, if available.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>T</code> <p>The response from OpenAI's API</p> required <code>response_model</code> <code>Type[T_Model]</code> <p>The response model to use for parsing the response</p> required <code>stream</code> <code>bool</code> <p>Whether the response is a stream</p> required <code>validation_context</code> <code>dict</code> <p>The validation context to use for validating the response. Defaults to None.</p> <code>None</code> <code>strict</code> <code>_type_</code> <p>Whether to use strict json parsing. Defaults to None.</p> <code>None</code> <code>mode</code> <code>Mode</code> <p>The openai completion mode. Defaults to Mode.FUNCTIONS.</p> <code>TOOLS</code> <p>Returns:</p> Type Description <code>Union[T_Model, T]</code> <p>Union[T_Model, T]: The parsed response, if a response model is available, otherwise the response as is from the SDK</p> Source code in <code>instructor/patch.py</code> <pre><code>def process_response(\n    response: T,\n    *,\n    response_model: Type[T_Model],\n    stream: bool,\n    validation_context: dict = None,\n    strict=None,\n    mode: Mode = Mode.TOOLS,\n) -&gt; Union[T_Model, T]:\n    \"\"\"Processes a OpenAI response with the response model, if available.\n\n    Args:\n        response (T): The response from OpenAI's API\n        response_model (Type[T_Model]): The response model to use for parsing the response\n        stream (bool): Whether the response is a stream\n        validation_context (dict, optional): The validation context to use for validating the response. Defaults to None.\n        strict (_type_, optional): Whether to use strict json parsing. Defaults to None.\n        mode (Mode, optional): The openai completion mode. Defaults to Mode.FUNCTIONS.\n\n    Returns:\n        Union[T_Model, T]: The parsed response, if a response model is available, otherwise the response as is from the SDK\n    \"\"\"\n    if response_model is None:\n        return response\n\n    if (\n        inspect.isclass(response_model)\n        and issubclass(response_model, (IterableBase, PartialBase))\n        and stream\n    ):\n        model = response_model.from_streaming_response(\n            response,\n            mode=mode,\n        )\n        return model\n\n    model = response_model.from_response(\n        response,\n        validation_context=validation_context,\n        strict=strict,\n        mode=mode,\n    )\n\n    # ? This really hints at the fact that we need a better way of\n    # ? attaching usage data and the raw response to the model we return.\n    if isinstance(model, IterableBase):\n        logger.debug(f\"Returning takes from IterableBase\")\n        return [task for task in model.tasks]\n\n    if isinstance(response_model, ParallelBase):\n        logger.debug(f\"Returning model from ParallelBase\")\n        return model\n\n    if isinstance(model, AdapterBase):\n        logger.debug(f\"Returning model from AdapterBase\")\n        return model.content\n\n    model._raw_response = response\n    return model\n</code></pre>"},{"location":"api/#instructor.patch.process_response_async","title":"<code>process_response_async(response, *, response_model, stream=False, validation_context=None, strict=None, mode=Mode.TOOLS)</code>  <code>async</code>","text":"<p>Processes a OpenAI response with the response model, if available. It can use <code>validation_context</code> and <code>strict</code> to validate the response via the pydantic model</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>ChatCompletion</code> <p>The response from OpenAI's API</p> required <code>response_model</code> <code>BaseModel</code> <p>The response model to use for parsing the response</p> required <code>stream</code> <code>bool</code> <p>Whether the response is a stream</p> <code>False</code> <code>validation_context</code> <code>dict</code> <p>The validation context to use for validating the response. Defaults to None.</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to use strict json parsing. Defaults to None.</p> <code>None</code> Source code in <code>instructor/patch.py</code> <pre><code>async def process_response_async(\n    response: ChatCompletion,\n    *,\n    response_model: Type[T_Model],\n    stream: bool = False,\n    validation_context: dict = None,\n    strict: Optional[bool] = None,\n    mode: Mode = Mode.TOOLS,\n) -&gt; T:\n    \"\"\"Processes a OpenAI response with the response model, if available.\n    It can use `validation_context` and `strict` to validate the response\n    via the pydantic model\n\n    Args:\n        response (ChatCompletion): The response from OpenAI's API\n        response_model (BaseModel): The response model to use for parsing the response\n        stream (bool): Whether the response is a stream\n        validation_context (dict, optional): The validation context to use for validating the response. Defaults to None.\n        strict (bool, optional): Whether to use strict json parsing. Defaults to None.\n    \"\"\"\n    if response_model is None:\n        return response\n\n    if (\n        inspect.isclass(response_model)\n        and issubclass(response_model, (IterableBase, PartialBase))\n        and stream\n    ):\n        model = await response_model.from_streaming_response_async(\n            response,\n            mode=mode,\n        )\n        return model\n\n    model = response_model.from_response(\n        response,\n        validation_context=validation_context,\n        strict=strict,\n        mode=mode,\n    )\n\n    # ? This really hints at the fact that we need a better way of\n    # ? attaching usage data and the raw response to the model we return.\n    if isinstance(model, IterableBase):\n        logger.debug(f\"Returning takes from IterableBase\")\n        return [task for task in model.tasks]\n\n    if isinstance(response_model, ParallelBase):\n        logger.debug(f\"Returning model from ParallelBase\")\n        return model\n\n    if isinstance(model, AdapterBase):\n        logger.debug(f\"Returning model from AdapterBase\")\n        return model.content\n\n    model._raw_response = response\n    return model\n</code></pre>"},{"location":"api/#instructor.dsl.validators.Validator","title":"<code>Validator</code>","text":"<p>             Bases: <code>OpenAISchema</code></p> <p>Validate if an attribute is correct and if not, return a new value with an error message</p> Source code in <code>instructor/dsl/validators.py</code> <pre><code>class Validator(OpenAISchema):\n    \"\"\"\n    Validate if an attribute is correct and if not,\n    return a new value with an error message\n    \"\"\"\n\n    is_valid: bool = Field(\n        default=True,\n        description=\"Whether the attribute is valid based on the requirements\",\n    )\n    reason: Optional[str] = Field(\n        default=None,\n        description=\"The error message if the attribute is not valid, otherwise None\",\n    )\n    fixed_value: Optional[str] = Field(\n        default=None,\n        description=\"If the attribute is not valid, suggest a new value for the attribute\",\n    )\n</code></pre>"},{"location":"api/#instructor.dsl.validators.llm_validator","title":"<code>llm_validator(statement, allow_override=False, model='gpt-3.5-turbo', temperature=0, openai_client=None)</code>","text":"<p>Create a validator that uses the LLM to validate an attribute</p>"},{"location":"api/#instructor.dsl.validators.llm_validator--usage","title":"Usage","text":"<pre><code>from instructor import llm_validator\nfrom pydantic import BaseModel, Field, field_validator\n\nclass User(BaseModel):\n    name: str = Annotated[str, llm_validator(\"The name must be a full name all lowercase\")\n    age: int = Field(description=\"The age of the person\")\n\ntry:\n    user = User(name=\"Jason Liu\", age=20)\nexcept ValidationError as e:\n    print(e)\n</code></pre> <pre><code>1 validation error for User\nname\n  The name is valid but not all lowercase (type=value_error.llm_validator)\n</code></pre> <p>Note that there, the error message is written by the LLM, and the error type is <code>value_error.llm_validator</code>.</p> <p>Parameters:</p> Name Type Description Default <code>statement</code> <code>str</code> <p>The statement to validate</p> required <code>model</code> <code>str</code> <p>The LLM to use for validation (default: \"gpt-3.5-turbo-0613\")</p> <code>'gpt-3.5-turbo'</code> <code>temperature</code> <code>float</code> <p>The temperature to use for the LLM (default: 0)</p> <code>0</code> <code>openai_client</code> <code>OpenAI</code> <p>The OpenAI client to use (default: None)</p> <code>None</code> Source code in <code>instructor/dsl/validators.py</code> <pre><code>def llm_validator(\n    statement: str,\n    allow_override: bool = False,\n    model: str = \"gpt-3.5-turbo\",\n    temperature: float = 0,\n    openai_client: OpenAI = None,\n) -&gt; Callable[[str], str]:\n    \"\"\"\n    Create a validator that uses the LLM to validate an attribute\n\n    ## Usage\n\n    ```python\n    from instructor import llm_validator\n    from pydantic import BaseModel, Field, field_validator\n\n    class User(BaseModel):\n        name: str = Annotated[str, llm_validator(\"The name must be a full name all lowercase\")\n        age: int = Field(description=\"The age of the person\")\n\n    try:\n        user = User(name=\"Jason Liu\", age=20)\n    except ValidationError as e:\n        print(e)\n    ```\n\n    ```\n    1 validation error for User\n    name\n      The name is valid but not all lowercase (type=value_error.llm_validator)\n    ```\n\n    Note that there, the error message is written by the LLM, and the error type is `value_error.llm_validator`.\n\n    Parameters:\n        statement (str): The statement to validate\n        model (str): The LLM to use for validation (default: \"gpt-3.5-turbo-0613\")\n        temperature (float): The temperature to use for the LLM (default: 0)\n        openai_client (OpenAI): The OpenAI client to use (default: None)\n    \"\"\"\n\n    openai_client = openai_client if openai_client else patch(OpenAI())\n\n    def llm(v: str) -&gt; str:\n        resp = openai_client.chat.completions.create(\n            response_model=Validator,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a world class validation model. Capable to determine if the following value is valid for the statement, if it is not, explain why and suggest a new value.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does `{v}` follow the rules: {statement}\",\n                },\n            ],\n            model=model,\n            temperature=temperature,\n        )\n\n        # If the response is  not valid, return the reason, this could be used in\n        # the future to generate a better response, via reasking mechanism.\n        assert resp.is_valid, resp.reason\n\n        if allow_override and not resp.is_valid and resp.fixed_value is not None:\n            # If the value is not valid, but we allow override, return the fixed value\n            return resp.fixed_value\n        return v\n\n    return llm\n</code></pre>"},{"location":"api/#instructor.dsl.validators.openai_moderation","title":"<code>openai_moderation(client=None)</code>","text":"<p>Validates a message using OpenAI moderation model.</p> <p>Should only be used for monitoring inputs and outputs of OpenAI APIs Other use cases are disallowed as per: https://platform.openai.com/docs/guides/moderation/overview</p> <p>Example: <pre><code>from instructor import OpenAIModeration\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(OpenAIModeration(openai_client=client))]\n\nResponse(message=\"I hate you\")\n</code></pre></p> <pre><code> ValidationError: 1 validation error for Response\n message\nValue error, `I hate you.` was flagged for ['harassment'] [type=value_error, input_value='I hate you.', input_type=str]\n</code></pre> <p>client (OpenAI): The OpenAI client to use, must be sync (default: None)</p> Source code in <code>instructor/dsl/validators.py</code> <pre><code>def openai_moderation(client: Optional[OpenAI] = None) -&gt; Callable[[str], str]:\n    \"\"\"\n    Validates a message using OpenAI moderation model.\n\n    Should only be used for monitoring inputs and outputs of OpenAI APIs\n    Other use cases are disallowed as per:\n    https://platform.openai.com/docs/guides/moderation/overview\n\n    Example:\n    ```python\n    from instructor import OpenAIModeration\n\n    class Response(BaseModel):\n        message: Annotated[str, AfterValidator(OpenAIModeration(openai_client=client))]\n\n    Response(message=\"I hate you\")\n    ```\n\n    ```\n     ValidationError: 1 validation error for Response\n     message\n    Value error, `I hate you.` was flagged for ['harassment'] [type=value_error, input_value='I hate you.', input_type=str]\n    ```\n\n    client (OpenAI): The OpenAI client to use, must be sync (default: None)\n    \"\"\"\n\n    client = client or OpenAI()\n\n    def validate_message_with_openai_mod(v: str) -&gt; str:\n        response = client.moderations.create(input=v)\n        out = response.results[0]\n        cats = out.categories.model_dump()\n        if out.flagged:\n            raise ValueError(\n                f\"`{v}` was flagged for {', '.join(cat for cat in cats if cats[cat])}\"\n            )\n\n        return v\n\n    return validate_message_with_openai_mod\n</code></pre>"},{"location":"api/#instructor.dsl.iterable.IterableModel","title":"<code>IterableModel(subtask_class, name=None, description=None)</code>","text":"<p>Dynamically create a IterableModel OpenAISchema that can be used to segment multiple tasks given a base class. This creates class that can be used to create a toolkit for a specific task, names and descriptions are automatically generated. However they can be overridden.</p>"},{"location":"api/#instructor.dsl.iterable.IterableModel--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom instructor import IterableModel\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\nMultiUser = IterableModel(User)\n</code></pre>"},{"location":"api/#instructor.dsl.iterable.IterableModel--result","title":"Result","text":"<pre><code>class MultiUser(OpenAISchema, MultiTaskBase):\n    tasks: List[User] = Field(\n        default_factory=list,\n        repr=False,\n        description=\"Correctly segmented list of `User` tasks\",\n    )\n\n    @classmethod\n    def from_streaming_response(cls, completion) -&gt; Generator[User]:\n        '''\n        Parse the streaming response from OpenAI and yield a `User` object\n        for each task in the response\n        '''\n        json_chunks = cls.extract_json(completion)\n        yield from cls.tasks_from_chunks(json_chunks)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>subtask_class</code> <code>Type[OpenAISchema]</code> <p>The base class to use for the MultiTask</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the MultiTask class, if None then the name of the subtask class is used as <code>Multi{subtask_class.__name__}</code></p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>The description of the MultiTask class, if None then the description is set to <code>Correct segmentation of</code>{subtask_class.name}<code>tasks</code></p> <code>None</code> <p>Returns:</p> Name Type Description <code>schema</code> <code>OpenAISchema</code> <p>A new class that can be used to segment multiple tasks</p> Source code in <code>instructor/dsl/iterable.py</code> <pre><code>def IterableModel(\n    subtask_class: Type[BaseModel],\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n) -&gt; Type[BaseModel]:\n    \"\"\"\n    Dynamically create a IterableModel OpenAISchema that can be used to segment multiple\n    tasks given a base class. This creates class that can be used to create a toolkit\n    for a specific task, names and descriptions are automatically generated. However\n    they can be overridden.\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    from instructor import IterableModel\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n    MultiUser = IterableModel(User)\n    ```\n\n    ## Result\n\n    ```python\n    class MultiUser(OpenAISchema, MultiTaskBase):\n        tasks: List[User] = Field(\n            default_factory=list,\n            repr=False,\n            description=\"Correctly segmented list of `User` tasks\",\n        )\n\n        @classmethod\n        def from_streaming_response(cls, completion) -&gt; Generator[User]:\n            '''\n            Parse the streaming response from OpenAI and yield a `User` object\n            for each task in the response\n            '''\n            json_chunks = cls.extract_json(completion)\n            yield from cls.tasks_from_chunks(json_chunks)\n    ```\n\n    Parameters:\n        subtask_class (Type[OpenAISchema]): The base class to use for the MultiTask\n        name (Optional[str]): The name of the MultiTask class, if None then the name\n            of the subtask class is used as `Multi{subtask_class.__name__}`\n        description (Optional[str]): The description of the MultiTask class, if None\n            then the description is set to `Correct segmentation of `{subtask_class.__name__}` tasks`\n\n    Returns:\n        schema (OpenAISchema): A new class that can be used to segment multiple tasks\n    \"\"\"\n    task_name = subtask_class.__name__ if name is None else name\n\n    name = f\"Iterable{task_name}\"\n\n    list_tasks = (\n        List[subtask_class],  # type: ignore[valid-type]\n        Field(\n            default_factory=list,\n            repr=False,\n            description=f\"Correctly segmented list of `{task_name}` tasks\",\n        ),\n    )\n\n    new_cls = create_model(\n        name,\n        tasks=list_tasks,\n        __base__=(OpenAISchema, IterableBase),\n    )\n    # set the class constructor BaseModel\n    new_cls.task_type = subtask_class\n\n    new_cls.__doc__ = (\n        f\"Correct segmentation of `{task_name}` tasks\"\n        if description is None\n        else description\n    )\n    assert issubclass(\n        new_cls, OpenAISchema\n    ), \"The new class should be a subclass of OpenAISchema\"\n    return new_cls\n</code></pre>"},{"location":"api/#instructor.dsl.partial.Partial","title":"<code>Partial</code>","text":"<p>             Bases: <code>Generic[Model]</code></p> <p>Generate a new class with all attributes optionals.</p> Notes <p>This will wrap a class inheriting form BaseModel and will recursively convert all its attributes and its children's attributes to optionals.</p> Example <p>Partial[SomeModel]</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>class Partial(Generic[Model]):\n    \"\"\"Generate a new class with all attributes optionals.\n\n    Notes:\n        This will wrap a class inheriting form BaseModel and will recursively\n        convert all its attributes and its children's attributes to optionals.\n\n    Example:\n        Partial[SomeModel]\n    \"\"\"\n\n    def __new__(\n        cls,\n        *args: object,  # noqa :ARG003\n        **kwargs: object,  # noqa :ARG003\n    ) -&gt; \"Partial[Model]\":\n        \"\"\"Cannot instantiate.\n\n        Raises:\n            TypeError: Direct instantiation not allowed.\n        \"\"\"\n        raise TypeError(\"Cannot instantiate abstract Partial class.\")\n\n    def __init_subclass__(\n        cls,\n        *args: object,\n        **kwargs: object,\n    ) -&gt; NoReturn:\n        \"\"\"Cannot subclass.\n\n        Raises:\n           TypeError: Subclassing not allowed.\n        \"\"\"\n        raise TypeError(\"Cannot subclass {}.Partial\".format(cls.__module__))\n\n    def __class_getitem__(  # type: ignore[override]\n        cls,\n        wrapped_class: type[Model],\n    ) -&gt; type[Model]:\n        \"\"\"Convert model to a partial model with all fields being optionals.\"\"\"\n\n        def _make_field_optional(\n            field: FieldInfo,\n        ) -&gt; tuple[object, FieldInfo]:\n            tmp_field = deepcopy(field)\n\n            annotation = field.annotation\n\n            # Handle generics (like List, Dict, etc.)\n            if get_origin(annotation) is not None:\n                # Get the generic base (like List, Dict) and its arguments (like User in List[User])\n                generic_base = get_origin(annotation)\n                generic_args = get_args(annotation)\n\n                # Recursively apply Partial to each of the generic arguments\n                modified_args = tuple(\n                    Partial[arg]  # type: ignore[valid-type]\n                    if isinstance(arg, type) and issubclass(arg, BaseModel)\n                    else arg\n                    for arg in generic_args\n                )\n\n                # Reconstruct the generic type with modified arguments\n                tmp_field.annotation = Optional[generic_base[modified_args]]\n                tmp_field.default = None\n            # If the field is a BaseModel, then recursively convert it's\n            # attributes to optionals.\n            elif isinstance(annotation, type) and issubclass(annotation, BaseModel):\n                tmp_field.annotation = Optional[Partial[annotation]]  # type: ignore[assignment, valid-type]\n                tmp_field.default = {}\n            else:\n                tmp_field.annotation = Optional[field.annotation]  # type: ignore[assignment]\n                tmp_field.default = None\n            return tmp_field.annotation, tmp_field\n\n        return create_model(  # type: ignore[no-any-return, call-overload]\n            f\"Partial{wrapped_class.__name__}\",\n            __base__=(wrapped_class, PartialBase),\n            __module__=wrapped_class.__module__,\n            **{\n                field_name: _make_field_optional(field_info)\n                for field_name, field_info in wrapped_class.model_fields.items()\n            },\n        )\n</code></pre>"},{"location":"api/#instructor.dsl.partial.Partial.__class_getitem__","title":"<code>__class_getitem__(wrapped_class)</code>","text":"<p>Convert model to a partial model with all fields being optionals.</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>def __class_getitem__(  # type: ignore[override]\n    cls,\n    wrapped_class: type[Model],\n) -&gt; type[Model]:\n    \"\"\"Convert model to a partial model with all fields being optionals.\"\"\"\n\n    def _make_field_optional(\n        field: FieldInfo,\n    ) -&gt; tuple[object, FieldInfo]:\n        tmp_field = deepcopy(field)\n\n        annotation = field.annotation\n\n        # Handle generics (like List, Dict, etc.)\n        if get_origin(annotation) is not None:\n            # Get the generic base (like List, Dict) and its arguments (like User in List[User])\n            generic_base = get_origin(annotation)\n            generic_args = get_args(annotation)\n\n            # Recursively apply Partial to each of the generic arguments\n            modified_args = tuple(\n                Partial[arg]  # type: ignore[valid-type]\n                if isinstance(arg, type) and issubclass(arg, BaseModel)\n                else arg\n                for arg in generic_args\n            )\n\n            # Reconstruct the generic type with modified arguments\n            tmp_field.annotation = Optional[generic_base[modified_args]]\n            tmp_field.default = None\n        # If the field is a BaseModel, then recursively convert it's\n        # attributes to optionals.\n        elif isinstance(annotation, type) and issubclass(annotation, BaseModel):\n            tmp_field.annotation = Optional[Partial[annotation]]  # type: ignore[assignment, valid-type]\n            tmp_field.default = {}\n        else:\n            tmp_field.annotation = Optional[field.annotation]  # type: ignore[assignment]\n            tmp_field.default = None\n        return tmp_field.annotation, tmp_field\n\n    return create_model(  # type: ignore[no-any-return, call-overload]\n        f\"Partial{wrapped_class.__name__}\",\n        __base__=(wrapped_class, PartialBase),\n        __module__=wrapped_class.__module__,\n        **{\n            field_name: _make_field_optional(field_info)\n            for field_name, field_info in wrapped_class.model_fields.items()\n        },\n    )\n</code></pre>"},{"location":"api/#instructor.dsl.partial.Partial.__init_subclass__","title":"<code>__init_subclass__(*args, **kwargs)</code>","text":"<p>Cannot subclass.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Subclassing not allowed.</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>def __init_subclass__(\n    cls,\n    *args: object,\n    **kwargs: object,\n) -&gt; NoReturn:\n    \"\"\"Cannot subclass.\n\n    Raises:\n       TypeError: Subclassing not allowed.\n    \"\"\"\n    raise TypeError(\"Cannot subclass {}.Partial\".format(cls.__module__))\n</code></pre>"},{"location":"api/#instructor.dsl.partial.Partial.__new__","title":"<code>__new__(*args, **kwargs)</code>","text":"<p>Cannot instantiate.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Direct instantiation not allowed.</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>def __new__(\n    cls,\n    *args: object,  # noqa :ARG003\n    **kwargs: object,  # noqa :ARG003\n) -&gt; \"Partial[Model]\":\n    \"\"\"Cannot instantiate.\n\n    Raises:\n        TypeError: Direct instantiation not allowed.\n    \"\"\"\n    raise TypeError(\"Cannot instantiate abstract Partial class.\")\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.MaybeBase","title":"<code>MaybeBase</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[T]</code></p> <p>Extract a result from a model, if any, otherwise set the error and message fields.</p> Source code in <code>instructor/dsl/maybe.py</code> <pre><code>class MaybeBase(BaseModel, Generic[T]):  # type: ignore[misc]\n    \"\"\"\n    Extract a result from a model, if any, otherwise set the error and message fields.\n    \"\"\"\n\n    result: Optional[T]\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self) -&gt; bool:\n        return self.result is not None\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.Maybe","title":"<code>Maybe(model)</code>","text":"<p>Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for <code>result</code>, <code>error</code>, and <code>message</code> for sitatations where the data may not be present in the context.</p>"},{"location":"api/#instructor.dsl.maybe.Maybe--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom instructor import Maybe\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\nMaybeUser = Maybe(User)\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.Maybe--result","title":"Result","text":"<pre><code>class MaybeUser(BaseModel):\n    result: Optional[User]\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseModel]</code> <p>The Pydantic model to wrap with Maybe.</p> required <p>Returns:</p> Name Type Description <code>MaybeModel</code> <code>Type[BaseModel]</code> <p>A new Pydantic model that includes fields for <code>result</code>, <code>error</code>, and <code>message</code>.</p> Source code in <code>instructor/dsl/maybe.py</code> <pre><code>def Maybe(model: Type[T]) -&gt; Type[MaybeBase[T]]:\n    \"\"\"\n    Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for `result`, `error`, and `message` for sitatations where the data may not be present in the context.\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    from instructor import Maybe\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n    MaybeUser = Maybe(User)\n    ```\n\n    ## Result\n\n    ```python\n    class MaybeUser(BaseModel):\n        result: Optional[User]\n        error: bool = Field(default=False)\n        message: Optional[str]\n\n        def __bool__(self):\n            return self.result is not None\n    ```\n\n    Parameters:\n        model (Type[BaseModel]): The Pydantic model to wrap with Maybe.\n\n    Returns:\n        MaybeModel (Type[BaseModel]): A new Pydantic model that includes fields for `result`, `error`, and `message`.\n    \"\"\"\n\n    fields = {\n        \"result\": (\n            Optional[model],\n            Field(\n                default=None,\n                description=\"Correctly extracted result from the model, if any, otherwise None\",\n            ),\n        ),\n        \"error\": (bool, Field(default=False)),\n        \"message\": (\n            Optional[str],\n            Field(\n                default=None,\n                description=\"Error message if no result was found, should be short and concise\",\n            ),\n        ),\n    }\n\n    return create_model(f\"Maybe{model.__name__}\", __base__=MaybeBase, **fields)\n</code></pre>"},{"location":"api/#instructor.function_calls.Mode","title":"<code>Mode</code>","text":"<p>             Bases: <code>Enum</code></p> <p>The mode to use for patching the client</p> Source code in <code>instructor/function_calls.py</code> <pre><code>class Mode(enum.Enum):\n    \"\"\"The mode to use for patching the client\"\"\"\n\n    FUNCTIONS: str = \"function_call\"\n    PARALLEL_TOOLS: str = \"parallel_tool_call\"\n    TOOLS: str = \"tool_call\"\n    JSON: str = \"json_mode\"\n    MD_JSON: str = \"markdown_json_mode\"\n    JSON_SCHEMA: str = \"json_schema_mode\"\n\n    def __new__(cls, value: str) -&gt; \"Mode\":\n        member = object.__new__(cls)\n        member._value_ = value\n\n        # Deprecation warning for FUNCTIONS\n        if value == \"function_call\":\n            warnings.warn(\n                \"FUNCTIONS is deprecated and will be removed in future versions\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        return member\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema","title":"<code>OpenAISchema</code>","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>instructor/function_calls.py</code> <pre><code>class OpenAISchema(BaseModel):  # type: ignore[misc]\n    @classmethod  # type: ignore[misc]\n    @property\n    def openai_schema(cls) -&gt; Dict[str, Any]:\n        \"\"\"\n        Return the schema in the format of OpenAI's schema as jsonschema\n\n        Note:\n            Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.\n\n        Returns:\n            model_json_schema (dict): A dictionary in the format of OpenAI's schema as jsonschema\n        \"\"\"\n        schema = cls.model_json_schema()\n        docstring = parse(cls.__doc__ or \"\")\n        parameters = {\n            k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n        }\n        for param in docstring.params:\n            if (name := param.arg_name) in parameters[\"properties\"] and (\n                description := param.description\n            ):\n                if \"description\" not in parameters[\"properties\"][name]:\n                    parameters[\"properties\"][name][\"description\"] = description\n\n        parameters[\"required\"] = sorted(\n            k for k, v in parameters[\"properties\"].items() if \"default\" not in v\n        )\n\n        if \"description\" not in schema:\n            if docstring.short_description:\n                schema[\"description\"] = docstring.short_description\n            else:\n                schema[\"description\"] = (\n                    f\"Correctly extracted `{cls.__name__}` with all \"\n                    f\"the required parameters with correct types\"\n                )\n\n        return {\n            \"name\": schema[\"title\"],\n            \"description\": schema[\"description\"],\n            \"parameters\": parameters,\n        }\n\n    @classmethod\n    def from_response(\n        cls,\n        completion: T,\n        validation_context: Optional[Dict[str, Any]] = None,\n        strict: Optional[bool] = None,\n        mode: Mode = Mode.TOOLS,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the function from the response of an openai chat completion\n\n        Parameters:\n            completion (openai.ChatCompletion): The response from an openai chat completion\n            throw_error (bool): Whether to throw an error if the function call is not detected\n            validation_context (dict): The validation context to use for validating the response\n            strict (bool): Whether to use strict json parsing\n            mode (Mode): The openai completion mode\n\n        Returns:\n            cls (OpenAISchema): An instance of the class\n        \"\"\"\n        assert hasattr(completion, \"choices\")\n\n        if completion.choices[0].finish_reason == \"length\":\n            raise IncompleteOutputException()\n\n        message = completion.choices[0].message\n\n        if mode == Mode.FUNCTIONS:\n            assert (\n                message.function_call.name == cls.openai_schema[\"name\"]  # type: ignore[index]\n            ), \"Function name does not match\"\n            return cls.model_validate_json(\n                message.function_call.arguments,\n                context=validation_context,\n                strict=strict,\n            )\n        elif mode == Mode.TOOLS:\n            assert (\n                len(message.tool_calls) == 1\n            ), \"Instructor does not support multiple tool calls, use List[Model] instead.\"\n            tool_call = message.tool_calls[0]\n            assert (\n                tool_call.function.name == cls.openai_schema[\"name\"]  # type: ignore[index]\n            ), \"Tool name does not match\"\n            return cls.model_validate_json(\n                tool_call.function.arguments,\n                context=validation_context,\n                strict=strict,\n            )\n        elif mode in {Mode.JSON, Mode.JSON_SCHEMA, Mode.MD_JSON}:\n            return cls.model_validate_json(\n                message.content,\n                context=validation_context,\n                strict=strict,\n            )\n        else:\n            raise ValueError(f\"Invalid patch mode: {mode}\")\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema.openai_schema","title":"<code>openai_schema: Dict[str, Any]</code>  <code>classmethod</code> <code>property</code>","text":"<p>Return the schema in the format of OpenAI's schema as jsonschema</p> Note <p>Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.</p> <p>Returns:</p> Name Type Description <code>model_json_schema</code> <code>dict</code> <p>A dictionary in the format of OpenAI's schema as jsonschema</p>"},{"location":"api/#instructor.function_calls.OpenAISchema.from_response","title":"<code>from_response(completion, validation_context=None, strict=None, mode=Mode.TOOLS)</code>  <code>classmethod</code>","text":"<p>Execute the function from the response of an openai chat completion</p> <p>Parameters:</p> Name Type Description Default <code>completion</code> <code>ChatCompletion</code> <p>The response from an openai chat completion</p> required <code>throw_error</code> <code>bool</code> <p>Whether to throw an error if the function call is not detected</p> required <code>validation_context</code> <code>dict</code> <p>The validation context to use for validating the response</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to use strict json parsing</p> <code>None</code> <code>mode</code> <code>Mode</code> <p>The openai completion mode</p> <code>TOOLS</code> <p>Returns:</p> Name Type Description <code>cls</code> <code>OpenAISchema</code> <p>An instance of the class</p> Source code in <code>instructor/function_calls.py</code> <pre><code>@classmethod\ndef from_response(\n    cls,\n    completion: T,\n    validation_context: Optional[Dict[str, Any]] = None,\n    strict: Optional[bool] = None,\n    mode: Mode = Mode.TOOLS,\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the function from the response of an openai chat completion\n\n    Parameters:\n        completion (openai.ChatCompletion): The response from an openai chat completion\n        throw_error (bool): Whether to throw an error if the function call is not detected\n        validation_context (dict): The validation context to use for validating the response\n        strict (bool): Whether to use strict json parsing\n        mode (Mode): The openai completion mode\n\n    Returns:\n        cls (OpenAISchema): An instance of the class\n    \"\"\"\n    assert hasattr(completion, \"choices\")\n\n    if completion.choices[0].finish_reason == \"length\":\n        raise IncompleteOutputException()\n\n    message = completion.choices[0].message\n\n    if mode == Mode.FUNCTIONS:\n        assert (\n            message.function_call.name == cls.openai_schema[\"name\"]  # type: ignore[index]\n        ), \"Function name does not match\"\n        return cls.model_validate_json(\n            message.function_call.arguments,\n            context=validation_context,\n            strict=strict,\n        )\n    elif mode == Mode.TOOLS:\n        assert (\n            len(message.tool_calls) == 1\n        ), \"Instructor does not support multiple tool calls, use List[Model] instead.\"\n        tool_call = message.tool_calls[0]\n        assert (\n            tool_call.function.name == cls.openai_schema[\"name\"]  # type: ignore[index]\n        ), \"Tool name does not match\"\n        return cls.model_validate_json(\n            tool_call.function.arguments,\n            context=validation_context,\n            strict=strict,\n        )\n    elif mode in {Mode.JSON, Mode.JSON_SCHEMA, Mode.MD_JSON}:\n        return cls.model_validate_json(\n            message.content,\n            context=validation_context,\n            strict=strict,\n        )\n    else:\n        raise ValueError(f\"Invalid patch mode: {mode}\")\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>We would love for you to contribute to <code>Instructor</code>.</p>"},{"location":"contributing/#evals","title":"Evals","text":"<p>We invite you to contribute evals in pytest as a way to monitor the quality of the openai models and the instructor library. To get started check out the jxnl/instructor/tests/evals and contribute your own evals in the form of pytest tests. These evals will be run once a week and the results will be posted.</p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>If you find a bug, please file an issue on our issue tracker on GitHub.</p> <p>To help us reproduce the bug, please provide a minimal reproducible example, including a code snippet and the full error message.</p> <ol> <li>The <code>response_model</code> you are using.</li> <li>The <code>messages</code> you are using.</li> <li>The <code>model</code> you are using.</li> </ol>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>We welcome pull requests! There is plenty to do, and we are happy to discuss any contributions you would like to make.</p> <p>If it is not a small change, please start by filing an issue first.</p> <p>If you need ideas, you can check out the help wanted or good first issue labels.</p> <p>Grit is used to enforce best practices. You can run <code>grit check</code> to check your code before submitting a pull request.</p>"},{"location":"contributing/#contributors","title":"Contributors","text":""},{"location":"contributing/#additional-resources","title":"Additional Resources","text":"<p>To enhance your understanding of the documentation, here are some useful references:</p> <ul> <li> <p>mkdocs serve: The <code>mkdocs serve</code> command is used to preview your documentation locally during the development phase. When you run this command in your terminal, MkDocs starts a development server, allowing you to view and interact with your documentation in a web browser. This is helpful for checking how your changes look before publishing the documentation. Learn more in the mkdocs serve documentation.</p> </li> <li> <p>hl_lines in Code Blocks: The <code>hl_lines</code> feature in code blocks allows you to highlight specific lines within the code block. This is useful for drawing attention to particular lines of code when explaining examples or providing instructions. You can specify the lines to highlight using the <code>hl_lines</code> option in your code block configuration. For more details and examples, you can refer to the hl_lines documentation.</p> </li> <li> <p>Admonitions: Admonitions are a way to visually emphasize or call attention to certain pieces of information in your documentation. They come in various styles, such as notes, warnings, tips, etc. Admonitions provide a structured and consistent way to present important content. For usage examples and details on incorporating admonitions into your documentation, you can refer to the admonitions documentation.</p> </li> </ul> <p>For more details about the documentation structure and features, refer to the MkDocs Material documentation.</p> <p>Thank you for your contributions, and happy coding!</p>"},{"location":"help/","title":"Getting help with Instructor","text":"<p>If you need help getting started with Instructor or with advanced usage, the following sources may be useful.</p>"},{"location":"help/#material-discord-discord","title":":material-discord: Discord","text":"<p>The Discord is a great place to ask questions and get help from the community.</p>"},{"location":"help/#concepts","title":"Concepts","text":"<p>The concepts section explains the core concepts of Instructor and how to prompt with models.</p>"},{"location":"help/#cookbooks","title":"Cookbooks","text":"<p>The cookbooks are a great place to start. They contain a variety of examples that demonstrate how to use Instructor in different scenarios.</p>"},{"location":"help/#blog","title":"Blog","text":"<p>The blog contains articles that explain how to use Instructor in different scenarios.</p>"},{"location":"help/#github-discussions","title":"GitHub Discussions","text":"<p>GitHub discussions are useful for asking questions, your question and the answer will help everyone.</p>"},{"location":"help/#github-issues","title":"GitHub Issues","text":"<p>GitHub issues are useful for reporting bugs or requesting new features.</p>"},{"location":"help/#twitter","title":"Twitter","text":"<p>You can also reach out to me on Twitter if you have any questions or ideas.</p>"},{"location":"installation/","title":"Installation","text":"<p>Installation is as simple as:</p> <pre><code>pip install instructor\n</code></pre> <p>Instructor has a few dependencies:</p> <ul> <li><code>openai</code>: OpenAI's Python client.</li> <li><code>typer</code>: Build great CLIs. Easy to code. Based on Python type hints.</li> <li><code>docstring-parser</code>: A parser for Python docstrings, to improve the experience of working with docstrings in jsonschema.</li> <li><code>pydantic</code>: Data validation and settings management using python type annotations.</li> </ul> <p>If you've got Python 3.9+ and <code>pip</code> installed, you're good to go.</p>"},{"location":"why/","title":"Why use Instructor?","text":"Why use Pydantic? <p>Its hard to answer the question of why use Instructor without first answering why use Pydantic.:</p> <ul> <li> <p>Powered by type hints \u2014 with Pydantic, schema validation and serialization are controlled by type annotations; less to learn, less code to write, and integration with your IDE and static analysis tools.</p> </li> <li> <p>Speed \u2014 Pydantic's core validation logic is written in Rust. As a result, Pydantic is among the fastest data validation libraries for Python.</p> </li> <li> <p>JSON Schema \u2014 Pydantic models can emit JSON Schema, allowing for easy integration with other tools. [Learn more\u2026]</p> </li> <li> <p>Customisation \u2014 Pydantic allows custom validators and serializers to alter how data is processed in many powerful ways.</p> </li> <li> <p>Ecosystem \u2014 around 8,000 packages on PyPI use Pydantic, including massively popular libraries like FastAPI, huggingface, Django Ninja, SQLModel, &amp; LangChain.</p> </li> <li> <p>Battle tested \u2014 Pydantic is downloaded over 70M times/month and is used by all FAANG companies and 20 of the 25 largest companies on NASDAQ. If you're trying to do something with Pydantic, someone else has probably already done it.</p> </li> </ul> <p>Our <code>instructor.patch</code> for the <code>OpenAI</code> class introduces three key enhancements:</p> <ul> <li>Response Mode: Specify a Pydantic model to streamline data extraction.</li> <li>Max Retries: Set your desired number of retry attempts for requests.</li> <li>Validation Context: Provide a context object for enhanced validator access.   A Glimpse into Instructor's Capabilities</li> </ul> <p>Using Validators</p> <p>Learn more about validators checkout our blog post Good llm validation is just good validation</p> <p>With Instructor, your code becomes more efficient and readable. Here\u2019s a quick peek:</p>"},{"location":"why/#understanding-the-patch","title":"Understanding the <code>patch</code>","text":"<p>Lets go over the <code>patch</code> function. And see how we can leverage it to make use of instructor</p>"},{"location":"why/#step-1-patch-the-client","title":"Step 1: Patch the client","text":"<p>First, import the required libraries and apply the <code>patch</code> function to the OpenAI module. This exposes new functionality with the <code>response_model</code> parameter.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# This enables response_model keyword\n# from client.chat.completions.create\nclient = instructor.patch(OpenAI())\n</code></pre>"},{"location":"why/#step-2-define-the-pydantic-model","title":"Step 2: Define the Pydantic Model","text":"<p>Create a Pydantic model to define the structure of the data you want to extract. This model will map directly to the information in the prompt.</p> <pre><code>from pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n</code></pre>"},{"location":"why/#step-3-extract","title":"Step 3: Extract","text":"<p>Use the <code>client.chat.completions.create</code> method to send a prompt and extract the data into the Pydantic object. The <code>response_model</code> parameter specifies the Pydantic model to use for extraction. Its helpful to annotate the variable with the type of the response model, which will help your IDE provide autocomplete and spell check.</p> <pre><code>user: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)\n\nassert user.name == \"Jason\"\nassert user.age == 25\n</code></pre>"},{"location":"why/#understanding-validation","title":"Understanding Validation","text":"<p>Validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error.</p> <pre><code>from pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\nfrom instructor import llm_validator\n\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str, BeforeValidator(llm_validator(\"don't say objectionable things\"))\n    ]\n\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for QuestionAnswer\n    answer\n      Assertion failed, The statement promotes objectionable behavior. [type=assertion_error, input_value='The meaning of life is to be evil and steal', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n    \"\"\"\n</code></pre> <p>Its important to note here that the error message is generated by the LLM, not the code, so it'll be helpful for re-asking the model.</p> <pre><code>1 validation error for QuestionAnswer\nanswer\n   Assertion failed, The statement is objectionable. (type=assertion_error)\n</code></pre>"},{"location":"why/#self-correcting-on-validation-error","title":"Self Correcting on Validation Error","text":"<p>Here, the <code>UserDetails</code> model is passed as the <code>response_model</code>, and <code>max_retries</code> is set to 2.</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.patch(OpenAI())\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n\n\nmodel = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert model.name == \"JASON\"\n</code></pre>"},{"location":"why/#iterables-and-lists","title":"Iterables and Lists","text":"<p>We can also generate tasks as the tokens are streamed in by defining an <code>Iterable[T]</code> type.</p> <p>Lets look at an example in action with the same class</p> <pre><code>from typing import Iterable\n\nUsers = Iterable[User]\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Users,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f\"Consider the data below:\\n{input}\"\n                \"Correctly segment it into entitites\"\n                \"Make sure the JSON is correct\"\n            ),\n        },\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    assert isinstance(user, User)\n    print(user)\n\n#&gt; name=\"Jason\" \"age\"=10\n#&gt; name=\"John\" \"age\"=10\n</code></pre>"},{"location":"why/#partial-extraction","title":"Partial Extraction","text":"<p>We also support partial extraction, which is useful for streaming in data that is incomplete.</p> <pre><code>import instructor\n\nfrom instructor import Partial\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\nfrom rich.console import Console\n\nclient = instructor.patch(OpenAI())\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nextraction_stream = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=Partial[MeetingInfo],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)\n\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n</code></pre> <p>This will output the following:</p> <p></p> <p>As you can see, we've baked in a self correcting mechanism into the model. This is a powerful way to make your models more robust and less brittle without including a lot of extra code or prompts.</p>"},{"location":"blog/","title":"Welcome to the Instructor Blog","text":"<p>The goal of the blog is to capture some content that does not neatly fit within documentation or the cookbooks.</p>"},{"location":"blog/#advanced-topics","title":"Advanced Topics","text":"<ol> <li>What is Query Understanding, how does it go beyond embeddings?</li> <li>How can one achieve GPT-4 level summaries using GPT-3.5-turbo?</li> <li>What are the basics of Guardrails and Validation in AI models?</li> <li>How does one validate citations in AI-generated content?</li> <li>What are the methods and benefits of fine-tuning and distillation in AI models?</li> </ol>"},{"location":"blog/#learning-python","title":"Learning Python","text":"<ul> <li>How can I effectively cache my functions in Python?</li> <li>What are the fundamentals of batch processing with async in Python?</li> <li>How can I stream models to improve latency?</li> </ul>"},{"location":"blog/#integrations","title":"Integrations","text":"<ul> <li>Ollama</li> <li>llama-cpp-python</li> <li>Anyscale</li> <li>Together Compute</li> </ul>"},{"location":"blog/#media","title":"Media","text":"<ul> <li>Course: Structured Outputs w/ Instructor</li> <li>Keynote: Pydantic is all you need</li> </ul>"},{"location":"blog/2023/11/02/ai-engineer-keynote-pydantic-is-all-you-need/","title":"AI Engineer Keynote: Pydantic is all you need","text":"<p>Click here to watch the full talk</p> <p>Last month, I ventured back onto the speaking circuit at the inaugural AI Engineer Summit, sharing insights on leveraging Pydantic for effective prompt engineering. I dove deep into what is covered in our documentation and standard blog posts,</p> <p>I'd genuinely appreciate any feedback on the talk \u2013 every bit helps in refining the art. So, take a moment to check out the full talk here, and let's continue pushing the boundaries of what's possible.</p>","tags":["python","talks","prompt engineering","video"]},{"location":"blog/2023/11/26/python-caching/","title":"Introduction to Caching in Python","text":"<p>Instructor makes working with language models easy, but they are still computationally expensive.</p> <p>Today, we're diving into optimizing instructor code while maintaining the excellent DX offered by Pydantic models. We'll tackle the challenges of caching Pydantic models, typically incompatible with <code>pickle</code>, and explore solutions that use <code>decorators</code> like <code>functools.cache</code>. Then, we'll craft custom decorators with <code>diskcache</code> and <code>redis</code> to support persistent caching and distributed systems.</p> <p>Let's first consider our canonical example, using the <code>OpenAI</code> Python client to extract user details.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Enables `response_model`\nclient = instructor.patch(OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <p>Now imagine batch processing data, running tests or experiments, or simply calling <code>extract</code> multiple times over a workflow. We'll quickly run into performance issues, as the function may be called repeatedly, and the same data will be processed over and over again, costing us time and money.</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#1-functoolscache-for-simple-in-memory-caching","title":"1. <code>functools.cache</code> for Simple In-Memory Caching","text":"<p>When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session or in an application where we don't need to persist the cache between sessions.</p> <pre><code>import functools\n\n\n@functools.cache\ndef extract(data):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <p>Changing the Model does not Invalidate the Cache</p> <p>Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result.</p> <p>Now we can call <code>extract</code> multiple times with the same argument, and the result will be cached in memory for faster access.</p> <pre><code>import time\n\nstart = time.perf_counter()  # (1)\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\")  # (2)\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\n#&gt; Time taken: 0.92\n#&gt; Time taken: 1.20e-06 # (3)\n</code></pre> <ol> <li>Using <code>time.perf_counter()</code> to measure the time taken to run the function is better than using <code>time.time()</code> because it's more accurate and less susceptible to system clock changes.</li> <li>The second time we call <code>extract</code>, the result is returned from the cache, and the function is not called.</li> <li>The second call to <code>extract</code> is much faster because the result is returned from the cache!</li> </ol> <p>Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries.</p> What is a decorator? <p>A decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it. In Python, decorators are functions that take a function as an argument and return a closure.</p> <pre><code>def decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Do something before\")  # (1)\n        result = func(*args, **kwargs)\n        print(\"Do something after\")  # (2)\n        return result\n\n    return wrapper\n\n\n@decorator\ndef say_hello():\n    print(\"Hello!\")\n\n\nsay_hello()\n#&gt; \"Do something before\"\n#&gt; \"Hello!\"\n#&gt; \"Do something after\"\n</code></pre> <ol> <li>The code is executed before the function is called</li> <li>The code is executed after the function is called</li> </ol>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#2-diskcache-for-persistent-large-data-caching","title":"2. <code>diskcache</code> for Persistent, Large Data Caching","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport diskcache\n\ncache = diskcache.Cache('./my_cache_directory')  # (1)\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (2)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <ol> <li>We create a new <code>diskcache.Cache</code> instance to store the cached data. This will create a new directory called <code>my_cache_directory</code> in the current working directory.</li> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic in this example code</li> </ol> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data!</p> <pre><code>import functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation  # (4)\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (\n            f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  #  (2)\n        )\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type (3)\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> <li>We use Pydantic's <code>model_validate_json</code> to deserialize the cached result into a Pydantic model.</li> <li>We use <code>inspect.signature</code> to get the function's return type annotation, which we use to validate the cached result.</li> </ol> <p>Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence.</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#2-redis-caching-decorator-for-distributed-systems","title":"2. Redis Caching Decorator for Distributed Systems","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures.</p> <pre><code>import redis\nimport functools\nimport inspect\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  # (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> </ol> <p>Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types.</p> <p>Looking carefully</p> <p>If you look carefully at the code above you'll notice that we're using the same <code>instructor_cache</code> decorator as before. The implementation is the same, but we're using a different caching backend!</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#conclusion","title":"Conclusion","text":"<p>Choosing the right caching strategy depends on your application's specific needs, such as the size and type of data, the need for persistence, and the system's architecture. Whether it's optimizing a function's performance in a small application or managing large datasets in a distributed environment, Python offers robust solutions to improve efficiency and reduce computational overhead.</p> <p>If you'd like to use this code, try to send it over to ChatGPT to understand it more, and to add additional features that might matter for you, for example, the cache isn't invalidated when your BaseModel changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>If you like the content check out our GitHub as give us a star and checkout the library.</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/05/chain-of-density/","title":"Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density","text":"<p>Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor</p> <p>In this article, we'll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4's iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density.</p> <p>By the end you'll end up with a GPT 3.5 model, (fine-tuned using Instructor's great tooling), capable of producing summaries that rival the effectiveness of Chain of Density [Adams et al. (2023)]. As always, all code is readily available in our <code>examples/chain-of-density</code> folder in our repo for your reference.</p> Datasets and Colab Notebook <p>We've also uploaded all our generated data to Hugging Face here for you to use if you'd like to try reproducing these experiments. We've also added a Colab Instance for you to check our generated values.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#part-1-chain-of-density","title":"Part 1) Chain of Density","text":"<p>Summarizing extensive texts with AI can be challenging, often relying on inconsistent techniques. Their novel method, Chain Of Density prompting, enhances AI-based text summarization, outperforming human-generated summaries.</p> <p>Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density.</p> <p>First introduced in the paper - From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. The team has found that this method is able to consistently beats similar summaries written by human annotators.</p> Implementation Details <p>Note that our implementation uses a validator to ensure that the rewritten summary has a minimum length rather than a prompt. We also perform just 3 and not 5 rounds of rewrites, resulting in a lower final entity density.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#original-prompt","title":"Original Prompt","text":"<p>We can break down the original process into smaller api calls. This allows us to introduce validation at each step to ensure that we're getting the results that we want.</p> Original Chain of Density Prompt <pre><code>Article: {{ARTICLE}}\n\nYou will generate increasingly concise, entity-dense summaries of the\nabove Article.\n\nRepeat the following 2 steps 5 times.\n\nStep 1. Identify 1-3 informative Entities (\";\" delimited) from the\nArticle which are missing from the previously generated summary.\nStep 2. Write a new, denser summary of identical length which covers\nevery entity and detail from the previous summary plus the Missing\nEntities.\n\nA Missing Entity is:\n- Relevant: to the main story.\n- Specific: descriptive yet concise (5 words or fewer).\n- Novel; not in the previous summary.\n- Faithful: present in the Article.\n- Anywhere: located anywhere in the Article.\n\nGuidelines:\n- The first summary should be long (4-5 sentences, -80 words) yet\nhighly non-specific, containing little information beyond the\nentities marked as missing. Use overly verbose language and fillers\n(e.g., \"this article discusses\") to reach -80 words.\n- Make every word count: re-write the previous summary to improve\nflow and make space for additional entities.\n- Make space with fusion, compression, and removal of uninformative\nphrases like \"the article discusses\"\n- The summaries should become highly dense and concise yet\nself-contained, e.g., easily understood without the Article.\n- Missing entities can appear anywhere in the new summary.\n- Never drop entities from the previous summary. If space cannot be\nmade, add fewer new entities.\n\nRemember, use the exact same number of words for each summary.\n\nAnswer in JSON. The JSON should be a list (length 5) of dictionaries\nwhose keys are \"Missing_Entities\" and \"Denser_Summary\"\n</code></pre> <p> </p> Improved process with Instructor","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#data-modelling","title":"Data Modelling","text":"<p>Before we begin modelling the data, let's make sure we install all of our dependencies</p> <pre><code>pip install instructor aiohttp rich\n</code></pre>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#initial-summary","title":"Initial Summary","text":"<p>Let's start by walking through some of the data models that we'll be using as the <code>response_model</code> for our open ai function calls</p> <p>Firstly, we'll need a data model for the initial summary that we will be generating. We'll take the description of this class straight from the original prompt. It's important to note that these docstrings serve a purpose, they are directly used by the LLM when generating the outputs.</p> A quick note on Docstrings <p>Under the hood, Instructor parses the <code>response_model</code> that you give us into a function call for OpenAI to execute. This means that the final output will be closely linked to the Pydantic model you specify.</p> <p>For instance, this simple model that we later use in fine-tuning.</p> <pre><code>class GeneratedSummary(BaseModel):\n    \"\"\"\n    This represents a highly concise summary that includes as many entities as possible from the original source article.\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n\n    Guidelines\n    - Make every word count\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n    )\n</code></pre> <p>We eventually transform it into an OpenAI function call as seen below.</p> <pre><code>{\n\"functions\": [\n    {\n    \"name\": \"GeneratedSummary\",\n    \"description\": \"This represents a highly concise summary that includes as many entities as possible from the original source article.\\n\\nAn Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\\n\\nGuidelines\\n- Make every word count\\n- The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\\n- Make space with fusion, compression, and removal of uninformative phrases like \\\"the article discusses\\\"\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n        \"summary\": {\n            \"description\": \"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n            \"title\": \"Summary\",\n            \"type\": \"string\"\n        }\n        },\n        \"required\": [\n        \"summary\"\n        ]\n\n    }\n    }\n]\n}\n}\n</code></pre> <p>Therefore this means that the more elaborate and detailed your descriptions are, the better the outputs you will be able to get back. But we don't just stop there, since it's all Pydantic under the hood, you can validate and parse the resulting output to make sure it is exactly what you specify. It's all python all the way down.</p> <pre><code>class InitialSummary(BaseModel):\n    \"\"\"\n    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n    yet highly non-specific, containing little information beyond the entities marked as missing.\n    Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a summary of the article provided which is overly verbose and uses fillers. It should be roughly 80 words in length\",\n    )\n</code></pre>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#rewritten-summary","title":"Rewritten Summary","text":"<p>We'll also need one additional class to help model the rewritten schema</p> <pre><code>class RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: List[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: List[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n</code></pre> <p>Using Pydantic Validators with Instructor</p> <p>For a more in-depth walkthrough on how to use <code>Pydantic</code> validators with the <code>Instructor</code> library, we recommend checking out our previous article on LLM validation - Good LLM Validation is just Good Validation</p> <p>Ideally, we'd like for <code>Missing</code> to have a length between 1 and 3, <code>Absent</code> to be an empty list and for our rewritten summaries to keep a minimum entity density. With <code>Instructor</code>, we can implement this logic using native <code>Pydantic</code> validators that are simply declared as part of the class itself.</p> <pre><code>import nltk\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n@field_validator(\"summary\")\ndef min_length(cls, v: str):\n    tokens = nltk.word_tokenize(v) #(1)!\n    num_tokens = len(tokens)\n    if num_tokens &lt; 60:\n        raise ValueError(\n            \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"\n        )\n    return v\n\n@field_validator(\"missing\")\ndef has_missing_entities(cls, missing_entities: List[str]):\n    if len(missing_entities) == 0:\n        raise ValueError(\n            \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"\n        )\n    return missing_entities\n\n@field_validator(\"absent\")\ndef has_no_absent_entities(cls, absent_entities: List[str]):\n    absent_entity_string = \",\".join(absent_entities)\n    if len(absent_entities) &gt; 0:\n        print(f\"Detected absent entities of {absent_entity_string}\")\n        raise ValueError(\n            f\"Do not omit the following Entities {absent_entity_string} from the new summary\"\n        )\n    return absent_entities\n\n@field_validator(\"summary\")\ndef min_entity_density(cls, v: str):\n    tokens = nltk.word_tokenize(v)\n    num_tokens = len(tokens)\n\n    # Extract Entities\n    doc = nlp(v) #(2)!\n    num_entities = len(doc.ents)\n\n    density = num_entities / num_tokens\n    if density &lt; 0.08: #(3)!\n        raise ValueError(\n            f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"\n        )\n\n    return v\n</code></pre> <ol> <li> <p>Similar to the original paper, we utilize the <code>NLTK</code> word tokenizer to count the number of tokens within our generated sentences.     We aim for at least 60 tokens in our generated summary so that we don't lose information.</p> </li> <li> <p>We also use the spaCy library to calculate the entity density of the generated summary.</p> </li> <li> <p>We also implement a minimum entity density so that we stay within a given range. 0.08 is arbitrarily chosen in this case</p> </li> </ol>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#putting-it-all-together","title":"Putting it all Together","text":"<p>Now that we have our models and the rough flow figured out, let's implement a function to summarize a piece of text using <code>Chain Of Density</code> summarization.</p> <pre><code>from openai import OpenAI\nimport instructor\n\nclient = instructor.patch(OpenAI()) #(1)!\n\ndef summarize_article(article: str, summary_steps: int = 3):\n    summary_chain = []\n    # We first generate an initial summary\n    summary: InitialSummary = client.chat.completions.create(  # (2)!\n        model=\"gpt-4-0613\",\n        response_model=InitialSummary,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",\n            },\n            {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n            {\n                \"role\": \"user\",\n                \"content\": \"The generated summary should be about 80 words.\",\n            },\n        ],\n        max_retries=2,\n    )\n    prev_summary = None\n    summary_chain.append(summary.summary)\n    for i in range(summary_steps):\n        missing_entity_message = (\n            []\n            if prev_summary is None\n            else [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",\n                },\n            ]\n        )\n        new_summary: RewrittenSummary = client.chat.completions.create( # (3)!\n            model=\"gpt-4-0613\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                You are going to generate an increasingly concise,entity-dense summary of the following article.\n\n                Perform the following two tasks\n                - Identify 1-3 informative entities from the following article which is missing from the previous summary\n                - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities\n\n                Guidelines\n                - Make every word count: re-write the previous summary to improve flow and make space for additional entities\n                - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n                - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n                - Missing entities can appear anywhere in the new summary\n                - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n                \"\"\",\n                },\n                {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\n                },\n                *missing_entity_message,\n            ],\n            max_retries=3, #(4)!\n            max_tokens=1000,\n            response_model=RewrittenSummary,\n        )\n        summary_chain.append(new_summary.summary)\n        prev_summary = new_summary\n\n    return summary_chain\n</code></pre> <ol> <li> <p>We need to apply a <code>patch</code> function on the <code>OpenAI</code> client for us to get all     of the benefits that <code>Instructor</code> provides. With a simple <code>patch</code>, we can get     automatic type coercion of our outputs and automatic retries for invalid outputs     out of the box!</p> </li> <li> <p>We first generate an initial summary. Note here that we explictly ask for a summary that has     80 words and is lengthy with overly verbose fillers in the system prompt</p> </li> <li> <p>We slightly modify the original system prompt used in the original paper to perform a rewrite of the summary.     Using <code>Instructor</code>, we also get validation of the generated output with our <code>field_validator</code>s that we defined above</p> </li> <li> <p>If you've chosen a value that is larger than 0.08, make sure to increase this value in case you need to do multiple rewrites</p> </li> </ol> <p>This summarization function yields a result which triples the number of entities while maintaining the same number of tokens. We can also see that stylistically, the summary is a lot more natural.</p> <p>First Iteration</p> <p>This article discusses the highly-anticipated boxing match between Manny Pacquiao and Floyd Mayweather. The article revolves around Manny Pacquiao's statements about his upcoming fight and his preparations for the same. A portion of the article provides details about the financial stipulations of the match and its significance in the sporting arena. Quotes from Pacquiao illustrating his determination and his battle strategy are highlighted. The tone of the article is largely centered around creating a build-up to the upcoming mega event.</p> <p>Final Iteration</p> <p>Manny Pacquiao, the Filipino boxer, anticipates the forthcoming May 2 showdown at the MGM Grand as the fight of his life, against the undefeated American Floyd Mayweather, in a $300m bout. Despite being seen as the underdog in this high-stakes Las Vegas match, Pacquiao is confident, promising a warrior's spirit and assuring the fans who have been awaiting this encounter for a decade, that it will indeed be the biggest sporting spectacle in history worthy of their anticipation</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#part-2-fine-tuning","title":"Part 2) Fine-Tuning","text":"<p>In this section, we'll look into how to fine-tune a GPT 3.5 model so that it is able to perform at an equivalent level as a GPT-4 model. We'll then compare the performance of our model against that of <code>GPT-4</code> to see how it stacks up.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#creating-a-training-set","title":"Creating a Training Set","text":"<p>In order to prevent any contamination of data during testing, we randomly sampled 120 articles from the <code>griffin/chain-of-density</code> dataset and split these articles into a <code>train.csv</code> and a <code>test.csv</code> file which we uploaded to Hugging Face. Now, we just neeed to import the <code>Instructions</code> module from the <code>Instructor</code> package which allows you to generate a nicely formatted <code>.jsonl</code> file to be used for fine-tuning</p> <pre><code>from typing import List\nfrom chain_of_density import summarize_article #(1)!\nimport csv\nimport logging\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI()) # (2)!\n\nlogging.basicConfig(level=logging.INFO) #(3)!\n\ninstructions = instructor.Instructions( #(4)!\n    name=\"Chain Of Density\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"generated.jsonl\")],\n    openai_client=client,\n)\n\nclass GeneratedSummary(BaseModel):\n    \"\"\"\n    This represents a highly concise summary that includes as many entities as possible from the original source article.\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n\n    Guidelines\n    - Make every word count\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n    )\n\n@instructions.distil #(4)!\ndef distil_summarization(text: str) -&gt; GeneratedSummary:\n    summary_chain: List[str] = summarize_article(text)\n    return GeneratedSummary(summary=summary_chain[-1]) #(5)!\n\nwith open(\"train.csv\", \"r\") as file:\n    reader = csv.reader(file)\n    next(reader)  # Skip the header\n    for article, summary in reader:\n        # Run Distillisation to generate the values\n        distil_summarization(article)\n</code></pre> <ol> <li> <p>In this example, we're using the summarize_article that we defined up above. We saved it in a local file called <code>chain_of_density.py</code>,     hence the import</p> </li> <li> <p>We patch the default OpenAI client so that we can use the Instructor library with it</p> </li> <li> <p>We also need to configure logging at the <code>INFO</code> level. This is very important, if this is not configured, your output will not be generated.</p> </li> <li> <p>We instantiate a <code>Instruction</code> object which will help us handle the conversion of our function calls into a valid <code>.jsonl</code> file. We also define     the name of the <code>.jsonl</code> file in the <code>log_handlers</code> parameter</p> </li> <li> <p>We add in an <code>instructions.distil</code> annotation so that we automatically capture the input and output of the function we'd like to     fine-tune our model to output</p> </li> <li> <p>We return a <code>Pydantic</code> object which matches the annotation that we use on our function. Note that we must specify a <code>Pydantic</code> object to     be returned when using the <code>instructions.distil</code> annotation</p> </li> </ol> <p>Rate Limiting</p> <p>We recommend running this script on a small subset of the dataset first to test you've got everything configured nicely. Don't forget to add in rate limiting error handling with <code>tenacity</code> and set the <code>OPENAI_API_KEY</code> shell environment variable before running any subsequent commands</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#creating-fine-tuning-jobs","title":"Creating Fine-Tuning Jobs","text":"<p>Once we run this script, we'll have a new file called <code>generated.jsonl</code> in our local repository. Now all that's left is to run the command below to start fine-tuning your first model!</p> <pre><code>instructor jobs create-from-file generated.jsonl\n</code></pre> Finetuning Reference <p>Checking out our Finetuning CLI to learn about other hyperparameters that you can tune to improve your model's performance.</p> <p>Once the job is complete, all we need to do is to then change the annotation in the function call to <code>distil_summarization</code> in our original file above to start using our new model.</p> <pre><code>@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")  # (1)!\ndef distil_summarization(text: str) -&gt; GeneratedSummary:\n    summary_chain: List[str] = summarize_article(text)\n    return GeneratedSummary(summary=summary_chain[-1])\n</code></pre> <ol> <li>Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of    ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard <p>With that, you've now got your own fine-tuned model ready to go and serve data in production. We've seen how Instructor can make your life easier, from fine-tuning to distillation.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#results-and-benchmarks","title":"Results and Benchmarks","text":"<p>We'll be comparing the following models in 3 ways using 20 articles that were not used for fine-tuning.</p> <ul> <li>Entity Density : This is entities per token, the higher the better for density.</li> <li>Latency : Time to last token generated in seconds</li> <li>Costs : Total cost to generate outputs - we break down the cost into training and inference costs for easy reference</li> </ul> <code>3.5 Finetuned (n)</code> <p>This is a GPT 3.5 model that we fine-tuned on <code>n</code> examples. Each model was finetuned for 4-5 epochs ( This was automatically decided by the OpenAI scheduler )</p> <code>GPT-4 (COD)</code> <p>This is a GPT4 model which we applied 3 rounds of Chain Of Density rewrites to generate a summary with using the methodology above</p> <code>GPT-3.5 (Vanilla)</code> <p>This is a GPT 3.5 model that we asked to generate entity-dense summaries which were concise. Summaries were generated in a single pass targetting about 80-90 tokens.</p> Model Mean Latency (s) Mean Entity Density 3.5 Finetuned (20) 2.1 0.15 3.5 Finetuned (50) 2.1 0.14 3.5 Finetuned (76) 2.1 0.14 GPT-3.5 (Vanilla) 16.8 0.12 GPT-4 (COD) 49.5 0.15 Finetuning Datasets <p>For our finetuned models, we did a few optimisations to raise the performance.</p> <p>We only included summaries that had a minimum density of 0.15 in the dataset, took the summary in the entire chain with the highest density as the final one, forced every regenerated summary to have a minimum density of 0.12 and regenerated summaries up to three times if they didn't meet the summaries. This is a much more expensive strategy and can cost up to 2.5x or more what we do in this tutorial</p> <p>This resulted in the total cost of $63.46 to generate just 75 examples due to the stringent requirements, translating to about $0.85 per generated summary example.</p> <p>Using the OpenAI Usage Dashboard, we can calculate the cost of generating 20 summaries as seen below.</p> Model Training Cost ($) Inference Cost ($) Tokens Used Total Cost ($) GPT-3.5 (Vanilla) - 0.20 51,162 0.2 3.5 Finetuned (20) 0.7 0.20 56,573 0.8 3.5 Finetuned (50) 1.4 0.17 49,057 1.3 3.5 Finetuned (76) 1.8 0.17 51,583 2.5 GPT-4 (COD) - 12.9 409,062 12.9 <p>Here, we can see that <code>GPT-4</code> has an approximate inference cost of <code>0.65</code> per summary while our finetuned models have an inference cost of <code>0.0091</code> per summary which is ~ <code>72x</code> cheaper.</p> <p>Interestingly, the model finetuned with the least examples seems to outperform the others. While the reason for this is unknown, a few potential reasons could be that either we didn't train for sufficient epochs ( We chose the default 5 epochs ) or that the models started learning to imitate other behaviour such as more abstract writing styles from the larger variety of samples, resulting in a decrease in entity density.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#conclusions","title":"Conclusions","text":"<p>Finetuning this iterative method was 20-40x faster while improving overall performance, resulting in massive efficiency gains by finetuning and distilling capabilities into specialized models.</p> <p>We've seen how <code>Instructor</code> can make your life easier, from data modeling to distillation and finetuning. If you enjoy the content or want to try out <code>instructor</code> check out the github and don't forget to give us a star!</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/18/validate-citations/","title":"Verifying LLM Citations with Pydantic","text":"<p>Ensuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification.</p> <p>We'll start with using a simple substring check to verify citations. Then we'll use <code>instructor</code> itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#example-1-simple-substring-check","title":"Example 1: Simple Substring Check","text":"<p>In this example, we use the <code>Statements</code> class to verify if a given substring quote exists within a text chunk. If the substring is not found, an error is raised.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#code-example","title":"Code Example:","text":"<pre><code>from typing import List\nfrom openai import OpenAI\nfrom pydantic import BaseModel, ValidationInfo, field_validator\nimport instructor\n\nclient = instructor.patch(OpenAI())\n\n\nclass Statements(BaseModel):\n    body: str\n    substring_quote: str\n\n    @field_validator(\"substring_quote\")\n    @classmethod\n    def substring_quote_exists(cls, v: str, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        for text_chunk in context.values():\n            if v in text_chunk:  # (1)\n                return v\n        raise ValueError(\"Could not find substring_quote `{v}` in contexts\")\n\n\nclass AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n</code></pre> <ol> <li>While we use a simple substring check in this example, we can use more complex techniques like regex or Levenshtein distance.</li> </ol> <p>Once the class is defined, we can use it to validate the context and raise an error if the substring is not found.</p> <pre><code>try:\n    AnswerWithCitaton.model_validate(\n        {\n            \"question\": \"What is the capital of France?\",\n            \"answer\": [\n                {\"body\": \"Paris\", \"substring_quote\": \"Paris is the capital of France\"},\n            ],\n        },\n        context={\n            \"text_chunks\": {\n                1: \"Jason is a pirate\",\n                2: \"Paris is not the capital of France\",\n                3: \"Irrelevant data\",\n            }\n        },\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#error-message-example","title":"Error Message Example:","text":"<pre><code>answer.0.substring_quote\n  Value error, Could not find substring_quote `Paris is the capital of France` in contexts [type=value_error, input_value='Paris is the capital of France', input_type=str]\n    For further information visit [https://errors.pydantic.dev/2.4/v/value_error](https://errors.pydantic.dev/2.4/v/value_error)\n</code></pre> <p>Pydantic raises a validation error when the <code>substring_quote</code> attribute does not exist in the context. This approach can be used to validate more complex data using techniques like regex or Levenshtein distance.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#example-2-using-llm-for-verification","title":"Example 2: Using LLM for Verification","text":"<p>This approach leverages OpenAI's LLM to validate citations. If the citation does not exist in the context, the LLM returns an error message.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#code-example_1","title":"Code Example:","text":"<pre><code>class Validation(BaseModel):\n    is_valid: bool\n    error_messages: Optional[str] = Field(None, description=\"Error messages if any\")\n\n\nclass Statements(BaseModel):\n    body: str\n    substring_quote: str\n\n    @model_validator(mode=\"after\")\n    def substring_quote_exists(self, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        resp: Validation = client.chat.completions.create(\n            response_model=Validation,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does the following citation exist in the following context?\\n\\nCitation: {self.substring_quote}\\n\\nContext: {context}\",\n                }\n            ],\n            model=\"gpt-3.5-turbo\",\n        )\n\n        if resp.is_valid:\n            return self\n\n        raise ValueError(resp.error_messages)\n\n\nclass AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n</code></pre> <p>Now when we use a correct citation, the LLM returns a valid response.</p> <pre><code>resp = AnswerWithCitaton.model_validate(\n    {\n        \"question\": \"What is the capital of France?\",\n        \"answer\": [\n            {\"body\": \"Paris\", \"substring_quote\": \"Paris is the capital of France\"},\n        ],\n    },\n    context={\n        \"text_chunks\": {\n            1: \"Jason is a pirate\",\n            2: \"Paris is the capital of France\",\n            3: \"Irrelevant data\",\n        }\n    },\n)\nprint(resp.model_dump_json(indent=2))\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#result","title":"Result:","text":"<pre><code>{\n  \"question\": \"What is the capital of France?\",\n  \"answer\": [\n    {\n      \"body\": \"Paris\",\n      \"substring_quote\": \"Paris is the capital of France\"\n    }\n  ]\n}\n</code></pre> <p>When we have citations that don't exist in the context, the LLM returns an error message.</p> <pre><code>try:\n    AnswerWithCitaton.model_validate(\n        {\n            \"question\": \"What is the capital of France?\",\n            \"answer\": [\n                {\"body\": \"Paris\", \"substring_quote\": \"Paris is the capital of France\"},\n            ],\n        },\n        context={\n            \"text_chunks\": {\n                1: \"Jason is a pirate\",\n                2: \"Paris is not the capital of France\",\n                3: \"Irrelevant data\",\n            }\n        },\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#error-message-example_1","title":"Error Message Example:","text":"<pre><code>1 validation error for AnswerWithCitaton\nanswer.0\n  Value error, Citation not found in context [type=value_error, input_value={'body': 'Paris', 'substr... the capital of France'}, input_type=dict]\n    For further information visit [https://errors.pydantic.dev/2.4/v/value_error](https://errors.pydantic.dev/2.4/v/value_error)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#example-3-aligning-citations-and-answers","title":"Example 3: Aligning Citations and Answers","text":"<p>In this example, we ensure that the provided answers are aligned with the given citations and context. The LLM is used to verify the alignment.</p> <p>We use the same <code>Statements</code> model as above, but we add a new model for the answer that also verifies the alignment of citations.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#code-example_2","title":"Code Example:","text":"<pre><code>class AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n\n    @model_validator(mode=\"after\")\n    def validate_answer(self, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        resp: Validation = client.chat.completions.create(\n            response_model=Validation,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does the following answers match the question and the context?\\n\\nQuestion: {self.question}\\n\\nAnswer: {self.answer}\\n\\nContext: {context}\",\n                }\n            ],\n            model=\"gpt-3.5-turbo\",\n        )\n\n        if resp.is_valid:\n            return self\n\n        raise ValueError(resp.error_messages)\n</code></pre> <p>When we have a mismatch between the answer and the citation, the LLM returns an error message.</p> <pre><code>try:\n    AnswerWithCitaton.model_validate(\n        {\n            \"question\": \"What is the capital of France?\",\n            \"answer\": [\n                {\"body\": \"Texas\", \"substring_quote\": \"Paris is the capital of France\"},\n            ],\n        },\n        context={\n            \"text_chunks\": {\n                1: \"Jason is a pirate\",\n                2: \"Paris is the capital of France\",\n                3: \"Irrelevant data\",\n            }\n        },\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#error-message-example_2","title":"Error Message Example:","text":"<pre><code>1 validation error for AnswerWithCitaton\n  Value error, The answer does not match the question and context [type=value_error, input_value={'question': 'What is the...he capital of France'}]}, input_type=dict]\n    For further information visit [https://errors.pydantic.dev/2.4/v/value_error](https://errors.pydantic.dev/2.4/v/value_error)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#conclusion","title":"Conclusion","text":"<p>These examples demonstrate the potential of using Pydantic and OpenAI to enhance data accuracy through citation verification. While the LLM-based approach may not be efficient for runtime operations, it has exciting implications for generating a dataset of accurate responses. By leveraging this method during data generation, we can fine-tune a model that excels in citation accuracy. Similar to our last post on finetuning a better summarizer.</p> <p>If you like the content check out our GitHub as give us a star and checkout the library.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2024/02/14/weights-and-biases-course/","title":"Free course on Weights and Biases","text":"<p>I just released a free course on wits and biases. It goes over the material from tutorial. Check it out at wandb.courses its free and open to everyone and just under an hour long!</p> <p></p> <p>Click the image to access the course</p>","tags":["open source"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/","title":"Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation","text":"","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#introduction","title":"Introduction","text":"<p>Get ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the <code>instructor.instructions</code> streamlines this process, making the task you want to distil more efficient and powerful while preserving its original functionality and backwards compatibility.</p> <p>If you want to see the full example checkout examples/distillation</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#why-use-instructor","title":"Why use Instructor?","text":"<p>Imagine you're developing a backend service that uses a mix old and new school ML practises, it may involve pipelines with multiple function calls, validations, and data processing. Sounds cumbersome, right? That's where <code>Instructor</code> comes in. It simplifies complex procedures, making them more efficient and easier to manage by adding a decorator to your function that will automatically generate a dataset for fine-tuning and help you swap out the function implementation.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#quick-start-how-to-use-instructors-distillation-feature","title":"Quick Start: How to Use Instructor's Distillation Feature","text":"<p>Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file.</p> <pre><code>import logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions  # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")],\n)\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n\n# Generate some data\nfor _ in range(10):\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n    #&gt; a=873 b=234 result=204282\n    #&gt; a=902 b=203 result=183106\n    #&gt; a=962 b=284 result=273208\n    #&gt; a=491 b=739 result=362849\n    #&gt; a=193 b=400 result=77200\n    #&gt; a=300 b=448 result=134400\n    #&gt; a=952 b=528 result=502656\n    #&gt; a=574 b=797 result=457478\n    #&gt; a=482 b=204 result=98328\n    #&gt; a=781 b=278 result=217118\n</code></pre>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#the-intricacies-of-fine-tuning-language-models","title":"The Intricacies of Fine-tuning Language Models","text":"<p>Fine-tuning isn't just about writing a function like <code>def f(a, b): return a * b</code>. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#why-instructor-and-distillation-are-game-changers","title":"Why Instructor and Distillation are Game Changers","text":"<p>The library offers two main benefits:</p> <ol> <li>Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code.</li> <li>Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions.</li> </ol>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#role-of-instructor-in-simplifying-fine-tuning","title":"Role of Instructor in Simplifying Fine-Tuning","text":"<p>The <code>from instructor import Instructions</code> feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#logging-output-and-running-a-finetune","title":"Logging Output and Running a Finetune","text":"<p>Here's how the logging output would look:</p> <pre><code>{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\n        {\n            \"role\": \"assistant\",\n            \"function_call\": {\n                \"name\": \"Multiply\",\n                \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}',\n            },\n        },\n    ],\n    \"functions\": [\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\n    ],\n}\n</code></pre> <p>Run a finetune like this:</p> <p>Don't forget to set your OpenAI Key as an environment variable</p> <p>All of the <code>instructor jobs</code> commands assume you've set an environment variable of <code>OPENAI_API_KEY</code> in your shell. You can set this by running the command <code>export OPENAI_API_KEY=&lt;Insert API Key Here&gt;</code> in your shell</p> <pre><code>instructor jobs create-from-file math_finetunes.jsonl\n</code></pre>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#next-steps-and-future-plans","title":"Next Steps and Future Plans","text":"<p>Here's a sneak peek of what I'm planning:</p> <pre><code>from instructor import Instructions, patch\n\npatch()  # (1)!\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")  # (2)!\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n</code></pre> <ol> <li> <p>Don't forget to run the <code>patch()</code> command that we provide with the <code>Instructor</code> package. This helps     automatically serialize the content back into the `Pydantic`` model that we're looking for.</p> </li> <li> <p>Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id     of <code>ft:gpt-3.5-turbo-0613:personal::&lt;id&gt;</code> under their Fine-tuning tab on their dashboard</p> </li> </ol> <p>With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#conclusion","title":"Conclusion","text":"<p>We've seen how <code>Instructor</code> can make your life easier, from fine-tuning to distillation. Now if you're thinking wow, I'd love a backend service to do this for continously, you're in luck! Please check out the survey at useinstructor.com and let us know who you are.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/","title":"Generators and LLM Streaming","text":"<p>Latency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times.</p> <p>And what makes streaming possible? Generators!</p> <p>In this post, we're going to dive into the cool world of Python generators \u2014 these tools are more than just a coding syntax trick. We'll explore Python generators from the ground up and then delve into LLM streaming using the Instructor library.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#python-generators-an-efficient-approach-to-iterables","title":"Python Generators: An Efficient Approach to Iterables","text":"<p>Generators in Python are a game-changer for handling large data sets and stream processing. They allow functions to yield values one at a time, pausing and resuming their state, which is a faster and more memory-efficient approach compared to traditional collections that store all elements in memory.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#the-basics-yielding-values","title":"The Basics: Yielding Values","text":"<p>A generator function in Python uses the <code>yield</code> keyword. It yields values one at a time, allowing the function to pause and resume its state.</p> <pre><code>def count_to_3():\n    yield 1\n    yield 2\n    yield 3\n\n\nfor num in count_to_3():\n    print(num)\n    #&gt; 1\n    #&gt; 2\n    #&gt; 3\n</code></pre> <pre><code>1\n2\n3\n</code></pre>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#advantages-over-traditional-collections","title":"Advantages Over Traditional Collections","text":"<ul> <li>Lazy Evaluation &amp; reduced latency: The time to get the first element (or time-to-first-token in LLM land) from a generator is significantly lower. Generators only produce one value at a time, whereas accessing the first element of a collection will require that the whole collection be created first.</li> <li>Memory Efficiency: Only one item is in memory at a time.</li> <li>Maintain State: Automatically maintains state between executions.</li> </ul> <p>Let's see how much faster generators are and where they really shine:</p> <pre><code>import time\n\n\ndef expensive_func(x):\n    \"\"\"Simulate an expensive operation.\"\"\"\n    time.sleep(1)\n    return x**2\n\n\ndef calculate_time_for_first_result_with_list(func_input, func):\n    \"\"\"Calculate using a list comprehension and return the first result with its computation time.\"\"\"\n    start_perf = time.perf_counter()\n    result = [func(x) for x in func_input][0]\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (list): {end_perf - start_perf:.2f} seconds\")\n    #&gt; Time for first result (list): 5.02 seconds\n    return result\n\n\ndef calculate_time_for_first_result_with_generator(func_input, func):\n    \"\"\"Calculate using a generator and return the first result with its computation time.\"\"\"\n    start_perf = time.perf_counter()\n    result = next(func(x) for x in func_input)\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (generator): {end_perf - start_perf:.2f} seconds\")\n    #&gt; Time for first result (generator): 1.01 seconds\n    return result\n\n\n# Prepare inputs for the function\nnumbers = [1, 2, 3, 4, 5]\n\n# Benchmarking\nfirst_result_list = calculate_time_for_first_result_with_list(numbers, expensive_func)\nfirst_result_gen = calculate_time_for_first_result_with_generator(\n    numbers, expensive_func\n)\n</code></pre> <pre><code>Time for first result (list): 5.02 seconds\nTime for first result (generator): 1.01 seconds\n</code></pre> <p>The generator computes one expensive operation and returns the first result immediately, while the list comprehension computes the expensive operation for all elements in the list before returning the first result.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#generator-expressions-a-shortcut","title":"Generator Expressions: A Shortcut","text":"<p>Python also allows creating generators in a single line of code, known as generator expressions. They are syntactically similar to list comprehensions but use parentheses.</p> <pre><code>squares = (x * x for x in range(10))\n</code></pre>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#use-cases-in-real-world-applications","title":"Use Cases in Real-World Applications","text":"<p>Generators shine in scenarios like reading large files, data streaming (eg. llm token streaming), and pipeline creation for data processing.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#llm-streaming","title":"LLM Streaming","text":"<p>If you've used ChatGPT, you'll see that the tokens are streamed out one by one, instead of the full response being shown at the end (can you imagine waiting for the full response??). This is made possible by generators.</p> <p>Here's how a vanilla openai generator looks:</p> <pre><code>from openai import OpenAI\n\n# Set your OpenAI API key\nclient = OpenAI(\n    api_key=\"My API Key\",\n)\n\nresponse_generator = client.chat.completions.create(\n    model='gpt-3.5-turbo',\n    messages=[{'role': 'user', 'content': \"What are some good reasons to smile?\"}],\n    temperature=0,\n    stream=True,\n)\n\nfor chunk in response_generator:\n    print(chunk.choices[0].delta.content, end=\"\")\n</code></pre> <p>This is great, but what if we want to do some structured extraction on this stream? For instance, we might want to render frontend components based on product rankings that are streamed out by an LLM.</p> <p>Should we wait for the entire stream to finish before extracting &amp; validating the list of components or can we extract &amp; validate the components in real time as they are streamed?</p> <p>In e-commerce, every millisecond matters so the time-to-first-render can differentiate a successful and not-so-successful e commerce store (and i know how a failing e commerce store feels :/ ).</p> <p>Let's see how we can use Instructor to handle extraction from this real time stream!</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#e-commerce-product-ranking","title":"E-commerce Product Ranking","text":"","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#scenario","title":"Scenario","text":"<p>Imagine an e-commerce platform where we have:</p> <p>\u2022 a customer profile: this includes a detailed history of purchases, browsing behavior, product ratings, preferences in various categories, search history, and even responses to previous recommendations. This extensive data is crucial for generating highly personalized and relevant product suggestions.</p> <p>\u2022 a list of candidate products: these could be some shortlisted products we think the customer would like.</p> <p>Our goal is to re-rerank these candidate products for the best conversion and we'll use an LLM!</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#stream-processing","title":"Stream Processing","text":"<p>User Data:</p> <p>Let's assume we have the following user profile:</p> <pre><code>profile_data = \"\"\"\nCustomer ID: 12345\nRecent Purchases: [Laptop, Wireless Headphones, Smart Watch]\nFrequently Browsed Categories: [Electronics, Books, Fitness Equipment]\nProduct Ratings: {Laptop: 5 stars, Wireless Headphones: 4 stars}\nRecent Search History: [best budget laptops 2023, latest sci-fi books, yoga mats]\nPreferred Brands: [Apple, AllBirds, Bench]\nResponses to Previous Recommendations: {Philips: Not Interested, Adidas: Not Interested}\nLoyalty Program Status: Gold Member\nAverage Monthly Spend: $500\nPreferred Shopping Times: Weekend Evenings\n...\n\"\"\"\n</code></pre> <p>We want to rank the following products for this user:</p> <pre><code>products = [\n    {\n        \"product_id\": 1,\n        \"product_name\": \"Apple MacBook Air (2023) - Latest model, high performance, portable\",\n    },\n    {\n        \"product_id\": 2,\n        \"product_name\": \"Sony WH-1000XM4 Wireless Headphones - Noise-canceling, long battery life\",\n    },\n    {\n        \"product_id\": 3,\n        \"product_name\": \"Apple Watch Series 7 - Advanced fitness tracking, seamless integration with Apple ecosystem\",\n    },\n    {\n        \"product_id\": 4,\n        \"product_name\": \"Kindle Oasis - Premium e-reader with adjustable warm light\",\n    },\n    {\n        \"product_id\": 5,\n        \"product_name\": \"AllBirds Wool Runners - Comfortable, eco-friendly sneakers\",\n    },\n    {\n        \"product_id\": 6,\n        \"product_name\": \"Manduka PRO Yoga Mat - High-quality, durable, eco-friendly\",\n    },\n    {\n        \"product_id\": 7,\n        \"product_name\": \"Bench Hooded Jacket - Stylish, durable, suitable for outdoor activities\",\n    },\n    {\n        \"product_id\": 8,\n        \"product_name\": \"GoPro HERO9 Black - 5K video, waterproof, for action photography\",\n    },\n    {\n        \"product_id\": 9,\n        \"product_name\": \"Nespresso Vertuo Next Coffee Machine - Quality coffee, easy to use, compact design\",\n    },\n    {\n        \"product_id\": 10,\n        \"product_name\": \"Project Hail Mary by Andy Weir - Latest sci-fi book from a renowned author\",\n    },\n]\n</code></pre> <p>Let's now define our models for structured extraction. Note: instructor will conveniently let us use <code>Iterable</code> to model an iterable of our class. In this case, once we define our product recommendation model, we can slap on <code>Iterable</code> to define what we ultimately want - a (ranked) list of product recommendations.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\n\nclass ProductRecommendation(BaseModel):\n    product_id: str\n    product_name: str\n\n\nRecommendations = Iterable[ProductRecommendation]\n</code></pre> <p>Now let's use our instructor patch. Since we don't want to wait for all the tokens to finish, will set stream to <code>True</code> and process each product recommendation as it comes in:</p> <pre><code>prompt = (\n    f\"Based on the following user profile:\\n{profile_data}\\nRank the following products from most relevant to least relevant:\\n\"\n    + '\\n'.join(\n        f\"{product['product_id']} {product['product_name']}\" for product in products\n    )\n)\n\nstart_perf = time.perf_counter()\nrecommendations_stream = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[ProductRecommendation],\n    stream=True,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\nfor product in recommendations_stream:\n    print(product)\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (generator): {end_perf - start_perf:.2f} seconds\")\n    break\n</code></pre> <pre><code>product_id='1' product_name='Apple MacBook Air (2023)'\nTime for first result (generator): 4.33 seconds\n</code></pre> <p><code>recommendations_stream</code> is a generator! It yields the extracted products as it's processing the stream in real-time. Now let's get the same response without streaming and see how they compare.</p> <pre><code>start_perf = time.perf_counter()\nrecommendations_list = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[ProductRecommendation],\n    stream=False,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\nprint(recommendations_list[0])\nend_perf = time.perf_counter()\nprint(f\"Time for first result (list): {end_perf - start_perf:.2f} seconds\")\n</code></pre> <pre><code>product_id='1' product_name='Apple MacBook Air (2023)'\nTime for first result (list): 8.63 seconds\n</code></pre> <p>Our web application now displays results faster. Even a 100ms improvement can lead to a 1% increase in revenue.</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#fastapi","title":"FastAPI","text":"<p>We can also take this and set up a streaming LLM API endpoint using FastAPI. Check out our docs on using FastAPI here!</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#key-takeaways","title":"Key Takeaways","text":"<p>To summarize, we looked at:</p> <p>\u2022 Generators in Python: A powerful feature that allows for efficient data handling with reduced latency</p> <p>\u2022 LLM Streaming: LLMs provide us generators to stream tokens and Instructor can let us validate and extract data from this stream. Real-time data validation ftw!</p> <p>Don't forget to check our GitHub for more resources and give us a star if you find the library helpful!</p> <p>If you have any questions or need further clarifications, feel free to reach out or dive into the Instructor library's documentation for more detailed information. Happy coding!</p>","tags":["generators","streaming","python"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/","title":"Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls","text":"<p>Language models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#the-problem-with-existing-llm-frameworks","title":"The Problem with Existing LLM Frameworks","text":"<p>Current frameworks for Language Learning Models (LLMs) have complex setups. Developers find it hard to control interactions with language models. Some frameworks require complex JSON Schema setups.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#the-openai-function-calling-game-changer","title":"The OpenAI Function Calling Game-Changer","text":"<p>OpenAI's Function Calling feature provides a constrained interaction model. However, it has its own complexities, mostly around JSON Schema.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#why-pydantic","title":"Why Pydantic?","text":"<p>Instructor uses Pydantic to simplify the interaction between the programmer and the language model.</p> <ul> <li>Widespread Adoption: Pydantic is a popular tool among Python developers.</li> <li>Simplicity: Pydantic allows model definition in Python.</li> <li>Framework Compatibility: Many Python frameworks already use Pydantic.</li> </ul> <pre><code>import pydantic\nimport instructor\nfrom openai import OpenAI\n\n# Enables the response_model\nclient = instructor.patch(OpenAI())\n\n\nclass UserDetail(pydantic.BaseModel):\n    name: str\n    age: int\n\n    def introduce(self):\n        return f\"Hello I'm {self.name} and I'm {self.age} years old\"\n\n\nuser: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#simplifying-validation-flow-with-pydantic","title":"Simplifying Validation Flow with Pydantic","text":"<p>Pydantic validators simplify features like re-asking or self-critique. This makes these tasks less complex compared to other frameworks.</p> <pre><code>from typing_extensions import Annotated\nfrom pydantic import BaseModel, BeforeValidator\nfrom instructor import llm_validator\n\n\nclass QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(llm_validator(\"don't say objectionable things\")),\n    ]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#the-modular-approach","title":"The Modular Approach","text":"<p>Pydantic allows for modular output schemas. This leads to more organized code.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#composition-of-schemas","title":"Composition of Schemas","text":"<pre><code>class UserDetails(BaseModel):\n    name: str\n    age: int\n\n\nclass UserWithAddress(UserDetails):\n    address: str\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#defining-relationships","title":"Defining Relationships","text":"<pre><code>class UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n    friends: List[int]\n\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#using-enums","title":"Using Enums","text":"<pre><code>from enum import Enum, auto\n\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#flexible-schemas","title":"Flexible Schemas","text":"<pre><code>from typing import List\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#chain-of-thought","title":"Chain of Thought","text":"<pre><code>class TimeRange(BaseModel):\n    chain_of_thought: str\n    start_time: int\n    end_time: int\n\n\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n    work_time: TimeRange\n    leisure_time: TimeRange\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#language-models-as-microservices","title":"Language Models as Microservices","text":"<p>The architecture resembles FastAPI. Most code can be written as Python functions that use Pydantic objects. This eliminates the need for prompt chains.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#fastapi-stub","title":"FastAPI Stub","text":"<pre><code>import fastapi\nfrom pydantic import BaseModel\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\napp = fastapi.FastAPI()\n\n@app.get(\"/user/{user_id}\", response_model=UserDetails)\nasync def get_user(user_id: int) -&gt; UserDetails:\n    return ...\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#using-instructor-as-a-function","title":"Using Instructor as a Function","text":"<pre><code>def extract_user(str) -&gt; UserDetails:\n    return client.chat.completions(\n           response_model=UserDetails,\n           messages=[]\n    )\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#response-modeling","title":"Response Modeling","text":"<pre><code>class MaybeUser(BaseModel):\n    result: Optional[UserDetail]\n    error: bool\n    message: Optional[str]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#conclusion","title":"Conclusion","text":"<p>Instructor, with Pydantic, simplifies interaction with language models. It is usable for both experienced and new developers.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>","tags":["Introduction"]},{"location":"blog/2024/02/18/seamless-support-with-langsmith/","title":"Seamless Support with Langsmith","text":"<p>Its a common misconception that LangChain's LangSmith is only compatible with LangChain's models. In reality, LangSmith is a unified DevOps platform for developing, collaborating, testing, deploying, and monitoring LLM applications. In this blog we will explore how LangSmith can be used to enhance the OpenAI client alongside <code>instructor</code>.</p>","tags":["langsmith"]},{"location":"blog/2024/02/18/seamless-support-with-langsmith/#langsmith","title":"LangSmith","text":"<p>In order to use langsmith, you first need to set your LangSmith API key.</p> <pre><code>export LANGCHAIN_API_KEY=&lt;your-api-key&gt;\n</code></pre> <p>Next, you will need to install the LangSmith SDK:</p> <pre><code>pip install -U langsmith\npip install -U instructor\n</code></pre> <p>If you want to pull this example down from instructor-hub you can use the following command:</p> <pre><code>instructor hub pull --slug batch_classification_langsmith --py &gt; batch_classification_langsmith.py\n</code></pre> <p>In this example we'll use the <code>wrap_openai</code> function to wrap the OpenAI client with LangSmith. This will allow us to use LangSmith's observability and monitoring features with the OpenAI client. Then we'll use <code>instructor</code> to patch the client with the <code>TOOLS</code> mode. This will allow us to use <code>instructor</code> to add additional functionality to the client.</p> <pre><code>import instructor\nimport asyncio\n\nfrom langsmith import traceable\nfrom langsmith.wrappers import wrap_openai\n\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import List\nfrom enum import Enum\n\n# Wrap the OpenAI client with LangSmith\nclient = wrap_openai(AsyncOpenAI())\n\n# Patch the client with instructor\nclient = instructor.patch(client, mode=instructor.Mode.TOOLS)\n\n# Rate limit the number of requests\nsem = asyncio.Semaphore(5)\n\n# Use an Enum to define the types of questions\nclass QuestionType(Enum):\n    CONTACT = \"CONTACT\"\n    TIMELINE_QUERY = \"TIMELINE_QUERY\"\n    DOCUMENT_SEARCH = \"DOCUMENT_SEARCH\"\n    COMPARE_CONTRAST = \"COMPARE_CONTRAST\"\n    EMAIL = \"EMAIL\"\n    PHOTOS = \"PHOTOS\"\n    SUMMARY = \"SUMMARY\"\n\n\n# You can add more instructions and examples in the description\n# or you can put it in the prompt in `messages=[...]`\nclass QuestionClassification(BaseModel):\n    \"\"\"\n    Predict the type of question that is being asked.\n    Here are some tips on how to predict the question type:\n    CONTACT: Searches for some contact information.\n    TIMELINE_QUERY: \"When did something happen?\n    DOCUMENT_SEARCH: \"Find me a document\"\n    COMPARE_CONTRAST: \"Compare and contrast two things\"\n    EMAIL: \"Find me an email, search for an email\"\n    PHOTOS: \"Find me a photo, search for a photo\"\n    SUMMARY: \"Summarize a large amount of data\"\n    \"\"\"\n\n    # If you want only one classification, just change it to\n    #   `classification: QuestionType` rather than `classifications: List[QuestionType]``\n    chain_of_thought: str = Field(\n        ..., description=\"The chain of thought that led to the classification\"\n    )\n    classification: List[QuestionType] = Field(\n        description=f\"An accuracy and correct prediction predicted class of question. Only allowed types: {[t.value for t in QuestionType]}, should be used\",\n    )\n\n    @field_validator(\"classification\", mode=\"before\")\n    def validate_classification(cls, v):\n        # sometimes the API returns a single value, just make sure it's a list\n        if not isinstance(v, list):\n            v = [v]\n        return v\n\n\n@traceable(name=\"classify-question\")\nasync def classify(data: str) -&gt; QuestionClassification:\n    \"\"\"\n    Perform multi-label classification on the input text.\n    Change the prompt to fit your use case.\n\n    Args:\n        data (str): The input text to classify.\n    \"\"\"\n    async with sem:  # some simple rate limiting\n        return data, await client.chat.completions.create(\n            model=\"gpt-4-turbo-preview\",\n            response_model=QuestionClassification,\n            max_retries=2,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Classify the following question: {data}\",\n                },\n            ],\n        )\n\n\nasync def main(questions: List[str]):\n    tasks = [classify(question) for question in questions]\n\n    for task in asyncio.as_completed(tasks):\n        question, label = await task\n        resp = {\n            \"question\": question,\n            \"classification\": [c.value for c in label.classification],\n            \"chain_of_thought\": label.chain_of_thought,\n        }\n        resps.append(resp)\n    return resps\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    questions = [\n        \"What was that ai app that i saw on the news the other day?\",\n        \"Can you find the trainline booking email?\",\n        \"what did I do on Monday?\",\n        \"Tell me about todays meeting and how it relates to the email on Monday\",\n    ]\n\n    resp = asyncio.run(main(questions))\n\n    for r in resp:\n        print(\"q:\", r[\"question\"])\n        #&gt; q: what did I do on Monday?\n        print(\"c:\", r[\"classification\"])\n        #&gt; c: ['SUMMARY']\n</code></pre> <p>If you follow what we've done is wrapped the client and proceeded to quickly use asyncio to classify a list of questions. This is a simple example of how you can use LangSmith to enhance the OpenAI client. You can use LangSmith to monitor and observe the client, and use <code>instructor</code> to add additional functionality to the client.</p> <p>To take a look at trace of this run check out this shareable link.</p> <p></p>","tags":["langsmith"]},{"location":"blog/2023/11/13/learn-async/","title":"Introduction to Batch Processing using <code>asyncio</code> and <code>Instructor</code>","text":"<p>Today, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using <code>instructor</code> and learn how to use <code>asyncio.gather</code> and <code>asyncio.as_completed</code> for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using <code>asyncio.Semaphore</code>.</p> <p>Github Example</p> <p>If you want to run the code examples in this article, you can find them on jxnl/instructor</p> <p>We will start by defining an <code>async</code> function that calls <code>openai</code> to extract data, and then examine four different ways to execute it. We will discuss the pros and cons of each approach and analyze the results of running them on a small batch.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#understanding-asyncio","title":"Understanding <code>asyncio</code>","text":"<p><code>asyncio</code> is a Python library that enables writing concurrent code using the async/await syntax. It is particularly useful for IO-bound and structured network code. If you are familiar with OpenAI's SDK, you might have encountered two classes: <code>OpenAI()</code> and <code>AsyncOpenAI()</code>. Today, we will be using the <code>AsyncOpenAI()</code> class, which processes data asynchronously.</p> <p>By utilizing these tools in web applications or batch processing, we can significantly improve performance by handling multiple requests concurrently instead of sequentially.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#understanding-async-and-await","title":"Understanding <code>async</code> and <code>await</code>","text":"<p>We will be using the <code>async</code> and <code>await</code> keywords to define asynchronous functions. The <code>async</code> keyword is used to define a function that returns a coroutine object. The <code>await</code> keyword is used to wait for the result of a coroutine object.</p> <p>If you want to understand the deeper details of <code>asyncio</code>, I recommend reading this article by Real Python.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#understanding-gather-vs-as_completed","title":"Understanding <code>gather</code> vs <code>as_completed</code>","text":"<p>In this post we'll show two ways to run tasks concurrently: <code>asyncio.gather</code> and <code>asyncio.as_completed</code>. The <code>gather</code> method is used to run multiple tasks concurrently and return the results as a <code>list</code>. The <code>as_completed</code> returns a <code>iterable</code> is used to run multiple tasks concurrently and return the results as they complete. Another great resource on the differences between the two can be found here.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#example-batch-processing","title":"Example: Batch Processing","text":"<p>In this example, we will demonstrate how to use <code>asyncio</code> for batch processing tasks, specifically for extracting and processing data concurrently. The script will extract data from a list of texts and process it concurrently using <code>asyncio</code>.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables `response_model` in `create` method\nclient = instructor.apatch(AsyncOpenAI())  # (1)!\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_person(text: str) -&gt; Person:\n    return await client.chat.completions.create(  # (2)!\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": text},\n        ],\n        response_model=Person,\n    )\n</code></pre> <ol> <li>We use <code>instructor.apatch</code> to patch the <code>create</code> method of <code>AsyncOpenAI</code> to accept a <code>response_model</code> argument. This is because the <code>create</code> method of <code>AsyncOpenAI</code> does not accept a <code>response_model</code> argument without this patch.</li> <li>We use <code>await</code> here to wait for the response from the server before we return the result. This is because <code>create</code> returns a coroutine object, not the result of the coroutine.</li> </ol> <p>Notice that now there are <code>async</code> and <code>await</code> keywords in the function definition. This is because we're using the <code>asyncio</code> library to run the function concurrently. Now let's define a batch of texts to process.</p> <pre><code>dataset = [\n    \"My name is John and I am 20 years old\",\n    \"My name is Mary and I am 21 years old\",\n    \"My name is Bob and I am 22 years old\",\n    \"My name is Alice and I am 23 years old\",\n    \"My name is Jane and I am 24 years old\",\n    \"My name is Joe and I am 25 years old\",\n    \"My name is Jill and I am 26 years old\",\n]\n</code></pre>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#for-loop-running-tasks-sequentially","title":"<code>for loop</code>: Running tasks sequentially.","text":"<pre><code>persons = []\nfor text in dataset:\n    person = await extract_person(text)\n    persons.append(person)\n</code></pre> <p>Even though there is an <code>await</code> keyword, we still have to wait for each task to finish before starting the next one. This is because we're using a <code>for</code> loop to iterate over the dataset. This method, which uses a <code>for</code> loop, will be the slowest among the four methods discussed today.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#asynciogather-running-tasks-concurrently","title":"<code>asyncio.gather</code>: Running tasks concurrently.","text":"<pre><code>async def gather():\n    tasks_get_persons = [extract_person(text) for text in dataset]\n    all_persons = await asyncio.gather(*tasks_get_persons)  # (1)!\n</code></pre> <ol> <li>We use <code>await</code> here to wait for all the tasks to finish before assigning the result to <code>all_persons</code>. This is because <code>asyncio.gather</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.as_completed</code> to achieve the same result.</li> </ol> <p>Using <code>asyncio.gather</code> allows us to return all the results at once. It is an effective way to speed up our code, but it's not the only way. Particularly, if we have a large dataset, we might not want to wait for everything to finish before starting to process the results. This is where <code>asyncio.as_completed</code> comes into play.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#asyncioas_completed-handling-tasks-as-they-complete","title":"<code>asyncio.as_completed</code>: Handling tasks as they complete.","text":"<pre><code>async def as_completed():\n    all_persons = []\n    tasks_get_persons = [extract_person(text) for text in dataset]\n    for person in asyncio.as_completed(tasks_get_persons):\n        all_persons.append(await person)  # (1)!\n</code></pre> <ol> <li>We use <code>await</code> here to wait for each task to complete before appending it to the list. This is because <code>as_completed</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.gather</code> to achieve the same result.</li> </ol> <p>This method is a great way to handle large datasets. We can start processing the results as they come in, especially if we are streaming data back to a client.</p> <p>However, these methods aim to complete as many tasks as possible as quickly as possible. This can be problematic if we want to be considerate to the server we're making requests to. This is where rate limiting comes into play. While there are libraries available to assist with rate limiting, for our initial defense, we will use a semaphore to limit the number of concurrent requests we make.</p> <p>Ordering of results</p> <p>It is important to note that the order of the results will not be the same as the order of the dataset. This is because the tasks are completed in the order they finish, not the order they were started. If you need to preserve the order of the results, you can use <code>asyncio.gather</code> instead.</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#rate-limited-gather-using-semaphores-to-limit-concurrency","title":"Rate-Limited Gather: Using semaphores to limit concurrency.","text":"<pre><code>sem = asyncio.Semaphore(2)\n\n\nasync def rate_limited_extract_person(text: str, sem: Semaphore) -&gt; Person:\n    async with sem:  # (1)!\n        return await extract_person(text)\n\n\nasync def rate_limited_gather(sem: Semaphore):\n    tasks_get_persons = [rate_limited_extract_person(text, sem) for text in dataset]\n    resp = await asyncio.gather(*tasks_get_persons)\n</code></pre> <ol> <li>We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to.</li> </ol>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#rate-limited-as-completed-using-semaphores-to-limit-concurrency","title":"Rate-Limited As Completed: Using semaphores to limit concurrency.","text":"<pre><code>sem = asyncio.Semaphore(2)\n\n\nasync def rate_limited_extract_person(text: str, sem: Semaphore) -&gt; Person:\n    async with sem:  # (1)!\n        return await extract_person(text)\n\n\nasync def rate_limited_as_completed(sem: Semaphore):\n    all_persons = []\n    tasks_get_persons = [rate_limited_extract_person(text, sem) for text in dataset]\n    for person in asyncio.as_completed(tasks_get_persons):\n        all_persons.append(await person)  # (2)!\n</code></pre> <ol> <li> <p>We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to.</p> </li> <li> <p>We use <code>await</code> here to wait for each task to complete before appending it to the list. This is because <code>as_completed</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.gather</code> to achieve the same result.</p> </li> </ol> <p>Now that we have seen the code, let's examine the results of processing 7 texts. As the prompts become longer or if we use GPT-4, the differences between these methods will become more pronounced.</p> <p>Other Options</p> <p>It is important to also note that here we are using a <code>semaphore</code> to limit the number of concurrent requests. However, there are other ways to limit concurrency especially since we have rate limit information from the <code>openai</code> request. You can imagine using a library like <code>ratelimit</code> to limit the number of requests per second. OR catching rate limit exceptions and using <code>tenacity</code> to retry the request after a certain amount of time.</p> <ul> <li>tenacity</li> <li>aiolimiter</li> </ul>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#results","title":"Results","text":"<p>As you can see, the <code>for</code> loop is the slowest, while <code>asyncio.as_completed</code> and <code>asyncio.gather</code> are the fastest without any rate limiting.</p> Method Execution Time Rate Limited (Semaphore) For Loop 6.17 seconds Asyncio.gather 0.85 seconds Asyncio.as_completed 0.95 seconds Asyncio.gather 3.04 seconds 2 Asyncio.as_completed 3.26 seconds 2","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/11/13/learn-async/#practical-implications-of-batch-processing","title":"Practical implications of batch processing","text":"<p>The choice of approach depends on the task's nature and the desired balance between speed and resource utilization.</p> <p>Here are some guidelines to consider:</p> <ul> <li>Use <code>asyncio.gather</code> for handling multiple independent tasks quickly.</li> <li>Apply <code>asyncio.as_completed</code> for large datasets to process tasks as they complete.</li> <li>Implement rate-limiting to avoid overwhelming servers or API endpoints.</li> </ul> <p>If you find the content helpful or want to try out <code>Instructor</code>, please visit our GitHub page and give us a star!</p>","tags":["python","batch","asyncio","async","async/await"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/","title":"RAG is more than just embedding search","text":"<p>With the advent of large language models (LLM), retrieval augmented generation (RAG) has become a hot topic. However throughout the past year of helping startups integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware.</p> <p>What is RAG?</p> <p>Retrieval augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized.</p> <p> </p> Simple RAG that embedded the user query and makes a search. <p>So let's kick things off by examining what I like to call the 'Dumb' RAG Model\u2014a basic setup that's more common than you'd think.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#the-dumb-rag-model","title":"The 'Dumb' RAG Model","text":"<p>When you ask a question like, \"what is the capital of France?\" The RAG 'dumb' model embeds the query and searches in some unopinionated search endpoint. Limited to a single method API like <code>search(query: str) -&gt; List[str]</code>. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#why-is-this-a-problem","title":"Why is this a problem?","text":"<ul> <li> <p>Query-Document Mismatch: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation!</p> </li> <li> <p>Monolithic Search Backend: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more.</p> </li> <li> <p>Limitation of text search: Restricts complex queries to a single string (<code>{query: str}</code>), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, asking <code>what problems did we fix last week</code> cannot be answered by a simple text search since documents that contain <code>problem, last week</code> are going to be present at every week.</p> </li> <li> <p>Limited ability to plan: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context it is able to plan a suite of queries to execute to return the best results.</p> </li> </ul> <p>Now let's dive into how we can make it smarter with query understanding. This is where things get interesting.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#improving-the-rag-model-with-query-understanding","title":"Improving the RAG Model with Query Understanding","text":"<p>Shoutouts</p> <p>Much of this work has been inspired by / done in collab with a few of my clients at new.computer, Metaphor Systems, and Naro, go check them out!</p> <p>Ultimately what you want to deploy is a system that understands how to take the query and rewrite it to improve precision and recall.</p> <p> </p> Query Understanding system routes to multiple search backends. <p>Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#whats-instructor","title":"Whats instructor?","text":"<p>Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling API.</p> <ul> <li>Widespread Adoption: Pydantic is a popular tool among Python developers.</li> <li>Simplicity: Pydantic allows model definition in Python.</li> <li>Framework Compatibility: Many Python frameworks already use Pydantic.</li> </ul>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#case-study-1-metaphor-systems","title":"Case Study 1: Metaphor Systems","text":"<p>Take Metaphor Systems, which turns natural language queries into their custom search-optimized query. If you take a look web UI you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using a language model, and turns it into a fully specified metaphor systems query.</p> <p></p> Metaphor Systems UI <p>If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. It's actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library</p> <pre><code>class DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n\n\nclass MetaphorQuery(BaseModel):\n    rewritten_query: str\n    published_daterange: DateRange\n    domains_allow_list: List[str]\n\n    async def execute():\n        return await metaphor.search(...)\n</code></pre> <p>Note how we model a rewritten query, range of published dates, and a list of domains to search in. This powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables response_model in the openai client\nclient = instructor.patch(OpenAI())\n\nquery = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=MetaphorQuery,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\",\n        },\n        {\"role\": \"user\", \"content\": \"What are some recent developments in AI?\"},\n    ],\n)\n</code></pre> <p>Example Output</p> <pre><code>{\n  \"rewritten_query\": \"novel developments advancements ai artificial intelligence machine learning\",\n  \"published_daterange\": {\n    \"start\": \"2023-09-17\",\n    \"end\": \"2021-06-17\"\n  },\n  \"domains_allow_list\": [\"arxiv.org\"]\n}\n</code></pre> <p>This isn't just about adding some date ranges. It's about nuanced, tailored searches, that are deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features.</p> <pre><code>class DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n    chain_of_thought: str = Field(\n        None,\n        description=\"Think step by step to plan what is the best time range to search in\",\n    )\n</code></pre> <p>Now, let's see how this approach can help model an agent like personal assistant.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#case-study-2-personal-assistant","title":"Case Study 2: Personal Assistant","text":"<p>Another great example of this multiple dispatch pattern is a personal assistant. You might ask, \"What do I have today?\", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts.</p> <pre><code>class ClientSource(enum.Enum):\n    GMAIL = \"gmail\"\n    CALENDAR = \"calendar\"\n\n\nclass SearchClient(BaseModel):\n    query: str\n    keywords: List[str]\n    email: str\n    source: ClientSource\n    start_date: datetime.date\n    end_date: datetime.date\n\n    async def execute(self) -&gt; str:\n        if self.source == ClientSource.GMAIL:\n            ...\n        elif self.source == ClientSource.CALENDAR:\n            ...\n\n\nclass Retrieval(BaseModel):\n    queries: List[SearchClient]\n\n    async def execute(self) -&gt; str:\n        return await asyncio.gather(*[query.execute() for query in self.queries])\n</code></pre> <p>Now we can call this with a simple query like \"What do I have today?\" and it will try to async dispatch to the correct backend. It's still important to prompt the language model well, but we'll leave that for another day.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables response_model in the openai client\nclient = instructor.patch(OpenAI())\n\nretrieval = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=Retrieval,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"},\n        {\"role\": \"user\", \"content\": \"What do I have today?\"},\n    ],\n)\n</code></pre> <p>Example Output</p> <pre><code>{\n    \"queries\": [\n        {\n            \"query\": None,\n            \"keywords\": None,\n            \"email\": \"jason@example.com\",\n            \"source\": \"gmail\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": None\n        },\n        {\n            \"query\": None,\n            \"keywords\": [\"meeting\", \"call\", \"zoom\"]]],\n            \"email\": \"jason@example.com\",\n            \"source\": \"calendar\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": None\n\n        }\n    ]\n}\n</code></pre> <p>Notice that we have a list of queries that route to different search backends (email and calendar). We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well. Perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app.</p> <p>Can I used framework X?</p> <p>I get this question a lot, but it's just code. Within these dispatches you can do whatever you want. You can use <code>input()</code> to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information. The sky is the limit.</p> <p>Both of these examples showcase how both search providers and consumers can use <code>instructor</code> to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#conclusion","title":"Conclusion","text":"<p>This is not about fancy embedding tricks, it's just plain old information retrieval and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#whats-next","title":"What's Next?","text":"<p>Here I want to show that `instructor`` isn\u2019t just about data extraction. It\u2019s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning \u2014 the untapped goldmine is skilled use of tools and APIs.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/","title":"Good LLM Validation is Just Good Validation","text":"<p>What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here.</p> <p>Validation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like <code>Pydantic</code> and <code>Instructor</code>. We validate these outputs using a validation function which conforms to the structure seen below.</p> <pre><code>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#what-is-instructor","title":"What is Instructor?","text":"<p><code>Instructor</code> helps to ensure you get the exact response type you're looking for when using openai's function call api. Once you've defined the <code>Pydantic</code> model for your desired response, <code>Instructor</code> handles all the complicated logic in-between - from the parsing/validation of the response to the automatic retries for invalid responses. This means that we can build in validators 'for free' and have a clear separation of concerns between the prompt and the code that calls openai.</p> <pre><code>from openai import OpenAI\nimport instructor  # pip install instructor\nfrom pydantic import BaseModel\n\n# This enables response_model keyword\n# from client.chat.completions.create\nclient = instructor.patch(OpenAI())  # (1)!\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n    max_retries=3,  # (2)!\n)\n\nassert user.name == \"Jason\"  # (3)!\nassert user.age == 25\n</code></pre> <ol> <li> <p>To simplify your work with OpenAI models and streamline the extraction of Pydantic objects from prompts, we     offer a patching mechanism for the <code>ChatCompletion</code> class.</p> </li> <li> <p>Invalid responses that fail to be validated succesfully will trigger up to as many reattempts as you define.</p> </li> <li> <p>As long as you pass in a <code>response_model</code> parameter to the <code>ChatCompletion</code> api call, the returned object will always     be a validated <code>Pydantic</code> object.</p> </li> </ol> <p>In this post, we'll explore how to evolve from static, rule-based validation methods to dynamic, machine learning-driven ones. You'll learn to use <code>Pydantic</code> and <code>Instructor</code> to leverage language models and dive into advanced topics like content moderation, validating chain of thought reasoning, and contextual validation.</p> <p>Let's examine how these approaches with a example. Imagine that you run a software company who wants to ensure you never serve hateful and racist content. This isn't an easy job since the language around these topics change very quickly and frequently.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#software-10-introduction-to-validations-in-pydantic","title":"Software 1.0: Introduction to Validations in Pydantic","text":"<p>A simple method could be to compile a list of different words that are often associated with hate speech. For simplicity, let's assume that we've found that the words <code>Steal</code> and <code>Rob</code> are good predictors of hateful speech from our database. We can modify our validation structure above to accomodate this.</p> <p>This will throw an error if we pass in a string like <code>Let's rob the bank!</code> or <code>We should steal from the supermarkets</code>.</p> <p>Pydantic offers two approaches for this validation: using the <code>field_validator</code> decorator or the <code>Annotated</code> hints.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#using-field_validator-decorator","title":"Using <code>field_validator</code> decorator","text":"<p>We can use the <code>field_validator</code> decorator to define a validator for a field in Pydantic. Here's a quick example of how we might be able to do so.</p> <pre><code>from pydantic import BaseModel, ValidationError, field_validator\n\n\nclass UserMessage(BaseModel):\n    message: str\n\n    @field_validator('message')\n    def message_cannot_have_blacklisted_words(cls, v: str) -&gt; str:\n        for word in v.split():  # (1)!\n            if word.lower() in {'rob', 'steal'}:\n                raise ValueError(f\"`{word}` was found in the message `{v}`\")\n        return v\n\n\ntry:\n    UserMessage(message=\"This is a lovely day\")\n    UserMessage(message=\"We should go and rob a bank\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserMessage\n    message\n      Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/value_error\n    \"\"\"\n</code></pre> <ol> <li>We split the sentence into its individual words and iterate through each of the words. We then try to see if any of these     words are in our blacklist which in this case is just <code>rob</code> and <code>steal</code></li> </ol> <p>Since the message <code>This is a lovely day</code> does not have any blacklisted words, no errors are thrown. However, in the given example above, the validation fails for the message <code>We should go and rob a bank</code> due to the presence of the word <code>rob</code> and the corresponding error message is displayed.</p> <pre><code>1 validation error for UserMessage\nmessage\n  Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#using-annotated","title":"Using <code>Annotated</code>","text":"<p>Alternatively, you can use the <code>Annotated</code> function to perform the same validation. Here's an example where we utilise the same function we started with.</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n\ndef message_cannot_have_blacklisted_words(value: str):\n    for word in value.split():\n        if word.lower() in {'rob', 'steal'}:\n            raise ValueError(f\"`{word}` was found in the message `{value}`\")\n    return value\n\n\nclass UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]\n\n\ntry:\n    UserMessage(message=\"This is a lovely day\")\n    UserMessage(message=\"We should go and rob a bank\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserMessage\n    message\n      Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/value_error\n    \"\"\"\n</code></pre> <p>This code snippet achieves the same validation result. If the user message contains any of the words in the blacklist, a <code>ValueError</code> is raised and the corresponding error message is displayed.</p> <pre><code>1 validation error for UserMessage\nmessage\n  Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre> <p>Validation is a fundamental concept in software development and remains the same when applied to AI systems. Existing programming concepts should be leveraged when possible instead of introducing new terms and standards. The underlying principles of validation remain unchanged.</p> <p>Suppose now that we've gotten a new message - <code>Violence is always acceptable, as long as we silence the witness</code>. Our original validator wouldn't throw any errors when passed this new message since it uses neither the words <code>rob</code> or <code>steal</code>. However, it's clear that it is not a message which should be published. How can we ensure that our validation logic can adapt to new challenges?</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#software-30-validation-for-llms-or-powered-by-llms","title":"Software 3.0: Validation for LLMs or powered by LLMs","text":"<p>Building upon the understanding of simple field validators, let's delve into probabilistic validation in software 3.0, (prompt engineering). We'll introduce an LLM-powered validator called <code>llm_validator</code> that uses a statement to verify the value.</p> <p>We can get around this by using the inbuilt <code>llm_validator</code> class from <code>Instructor</code>.</p> <pre><code>from instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n\nclass UserMessage(BaseModel):\n    message: Annotated[\n        str, AfterValidator(llm_validator(\"don't say objectionable things\"))\n    ]\n\n\ntry:\n    UserMessage(\n        message=\"Violence is always acceptable, as long as we silence the witness\"\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserMessage\n    message\n      Assertion failed, The statement promotes violence, which is objectionable. [type=assertion_error, input_value='Violence is always accep... we silence the witness', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n    \"\"\"\n</code></pre> <p>This produces the following error message as seen below</p> <pre><code>1 validation error for UserMessage\nmessage\n  Assertion failed, The statement promotes violence, which is objectionable. [type=assertion_error, input_value='Violence is always accep... we silence the witness', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/assertion_error\n</code></pre> <p>The error message is generated by the language model (LLM) rather than the code itself, making it helpful for re-asking the model in a later section. To better understand this approach, let's see how to build an <code>llm_validator</code> from scratch.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#creating-your-own-field-level-llm_validator","title":"Creating Your Own Field Level <code>llm_validator</code>","text":"<p>Building your own <code>llm_validator</code> can be a valuable exercise to get started with <code>Instructor</code> and create custom validators.</p> <p>Before we continue, let's review the anatomy of a validator:</p> <pre><code>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return value\n</code></pre> <p>As we can see, a validator is simply a function that takes in a value and returns a value. If the value is not valid, it raises a <code>ValueError</code>. We can represent this using the following structure:</p> <pre><code>class Validation(BaseModel):\n    is_valid: bool = Field(\n        ..., description=\"Whether the value is valid based on the rules\"\n    )\n    error_message: Optional[str] = Field(\n        ...,\n        description=\"The error message if the value is not valid, to be used for re-asking the model\",\n    )\n</code></pre> <p>Using this structure, we can implement the same logic as before and utilize <code>Instructor</code> to generate the validation.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables `response_model` and `max_retries` parameters\nclient = instructor.patch(OpenAI())\n\n\ndef validator(v):\n    statement = \"don't say objectionable things\"\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Does `{v}` follow the rules: {statement}\",\n            },\n        ],\n        # this comes from client = instructor.patch(OpenAI())\n        response_model=Validation,  # (1)!\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return v\n</code></pre> <ol> <li>The new parameter of <code>response_model</code> comes from <code>client = instructor.patch(OpenAI())</code> and does not exist in the original OpenAI SDK. This    allows us to pass in the <code>Pydantic</code> model that we want as a response.</li> </ol> <p>Now we can use this validator in the same way we used the <code>llm_validator</code> from <code>Instructor</code>.</p> <pre><code>class UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(validator)]\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#writing-more-complex-validations","title":"Writing more complex validations","text":"","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#validating-chain-of-thought","title":"Validating Chain of Thought","text":"<p>A popular way of prompting large language models nowadays is known as chain of thought. This involves getting a model to generate reasons and explanations for an answer to a prompt.</p> <p>We can utilise <code>Pydantic</code> and <code>Instructor</code> to perform a validation to check of the reasoning is reasonable, given both the answer and the chain of thought. To do this we can't build a field validator since we need to access multiple fields in the model. Instead we can use a model validator.</p> <pre><code>def validate_chain_of_thought(values):\n    chain_of_thought = values[\"chain_of_thought\"]\n    answer = values[\"answer\"]\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",\n            },\n        ],\n        # this comes from client = instructor.patch(OpenAI())\n        response_model=Validation,\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return values\n</code></pre> <p>We can then take advantage of the <code>model_validator</code> decorator to perform a validation on a subset of the model's data.</p> <p>We're defining a model validator here which runs before <code>Pydantic</code> parses the input into its respective fields. That's why we have a before keyword used in the <code>model_validator</code> class.</p> <pre><code>from pydantic import BaseModel, model_validator\n\n\nclass AIResponse(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n    @model_validator(mode='before')\n    @classmethod\n    def chain_of_thought_makes_sense(cls, data: Any) -&gt; Any:\n        # here we assume data is the dict representation of the model\n        # since we use 'before' mode.\n        return validate_chain_of_thought(data)\n</code></pre> <p>Now, when you create a <code>AIResponse</code> instance, the <code>chain_of_thought_makes_sense</code> validator will be invoked. Here's an example:</p> <pre><code>try:\n    resp = AIResponse(chain_of_thought=\"1 + 1 = 2\", answer=\"The meaning of life is 42\")\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p>If we create a <code>AIResponse</code> instance with an answer that does not follow the chain of thought, we will get an error.</p> <pre><code>1 validation error for AIResponse\n    Value error, The statement 'The meaning of life is 42' does not follow the chain of thought: 1 + 1 = 2.\n    [type=value_error, input_value={'chain_of_thought': '1 +... meaning of life is 42'}, input_type=dict]\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#validating-citations-from-original-text","title":"Validating Citations From Original Text","text":"<p>Let's see a more concrete example. Let's say that we've asked our model a question about some text source and we want to validate that the generated answer is supported by the source. This would allow us to minimize hallucinations and prevent statements that are not backed by the original text. While we could verify this by looking up the original source manually, a more scalable approach is to use a validator to do this automatically.</p> <p>We can pass in additional context to our validation functions using the <code>model_validate</code> function in <code>Pydantic</code> so that our models have more information to work with when performing validation. This context is a normal python dictionary and can be accessed inside the <code>info</code> argument in our validator functions.</p> <pre><code>from pydantic import ValidationInfo, BaseModel, field_validator\n\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: str\n\n    @field_validator('citation')\n    @classmethod\n    def citation_exists(cls, v: str, info: ValidationInfo):  # (1)!\n        context = info.context\n        if context:\n            context = context.get('text_chunk')\n            if v not in context:\n                raise ValueError(f\"Citation `{v}` not found in text chunks\")\n        return v\n</code></pre> <ol> <li>This <code>info</code> object corresponds to the value of <code>context</code> that we pass into the <code>model_validate</code> function as seen below.</li> </ol> <p>We can then take our original example and test it against our new model</p> <pre><code>try:\n    AnswerWithCitation.model_validate(\n        {\"answer\": \"Jason is a cool guy\", \"citation\": \"Jason is cool\"},\n        context={\"text_chunk\": \"Jason is just a guy\"},  # (1)!\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre> <ol> <li>This <code>context</code> object is just a normal python dictionary and can take in and store any arbitrary values</li> </ol> <p>This in turn generates the following error since <code>Jason is cool</code> does not exist in the text <code>Jason is just a guy</code>.</p> <pre><code>1 validation error for AnswerWithCitation\ncitation\nValue error, Citation `Jason is cool` not found in text chunks [type=value_error, input_value='Jason is cool', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#putting-it-all-together-with-client-instructorpatchopenai","title":"Putting it all together with <code>client = instructor.patch(OpenAI())</code>","text":"<p>To pass this context from the <code>client.chat.completions.create</code> call, <code>client = instructor.patch(OpenAI())</code> also passes the <code>validation_context</code>, which will be accessible from the <code>info</code> argument in the decorated validator functions.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Enables `response_model` and `max_retries` parameters\nclient = instructor.patch(OpenAI())\n\n\ndef answer_question(question: str, text_chunk: str) -&gt; AnswerWithCitation:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Answer the question: {question} with the text chunk: {text_chunk}\",\n            },\n        ],\n        response_model=AnswerWithCitation,\n        validation_context={\"text_chunk\": text_chunk},\n    )\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#error-handling-and-re-asking","title":"Error Handling and Re-Asking","text":"<p>Validators can ensure certain properties of the outputs by throwing errors, in an AI system we can use the errors and allow language model to self correct. The by running <code>client = instructor.patch(OpenAI())</code> not only do we add <code>response_model</code> and <code>validation_context</code> it also allows you to use the <code>max_retries</code> parameter to specify the number of times to try and self correct.</p> <p>This approach provides a layer of defense against two types of bad outputs:</p> <ol> <li>Pydantic Validation Errors (code or LLM-based)</li> <li>JSON Decoding Errors (when the model returns an incorrect response)</li> </ol>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#define-the-response-model-with-validators","title":"Define the Response Model with Validators","text":"<p>To keep things simple let's assume we have a model that returns a <code>UserModel</code> object. We can define the response model using Pydantic and add a field validator to ensure that the name is in uppercase.</p> <pre><code>from pydantic import BaseModel, field_validator\n\n\nclass UserModel(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n</code></pre> <p>This is where the <code>max_retries</code> parameter comes in. It allows the model to self correct and retry the prompt using the error message rather than the prompt.</p> <pre><code>model = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n    # Powered by client = instructor.patch(OpenAI())\n    response_model=UserModel,\n    max_retries=2,\n)\n\nassert model.name == \"JASON\"\n</code></pre> <p>In this example, even though there is no code explicitly transforming the name to uppercase, the model is able to correct the output.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#conclusion","title":"Conclusion","text":"<p>From the simplicity of Pydantic and Instructor to the dynamic validation capabilities of LLMs, the landscape of validation is changing but without needing to introduce new contepts. It's clear that the future of validation is not just about preventing bad data but about allowing llms to understand the data and correcting it.</p> <p>If you enjoy the content or want to try out <code>Instructor</code> please check out the github and give us a star!</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"cli/","title":"Instructor CLI","text":"<p>Welcome to the Instructor Command-Line Interface (CLI), a tool designed to ease your experience with the OpenAI API. Whether it's tracking your API usage or fine-tuning your models, Instructor CLI is your go-to utility.</p>"},{"location":"cli/#quick-start","title":"Quick Start","text":"<p>First things first: make sure your OpenAI API key is set as an environment variable. The CLI will use this for authenticating your requests to OpenAI's services.</p> <p>You can set the API key in your terminal as follows:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"cli/#installation-setup","title":"Installation &amp; Setup","text":"<pre><code>pip install instructor\n</code></pre>"},{"location":"cli/#features","title":"Features","text":"<ul> <li>API Usage Monitoring: Keep tabs on your API usage right from the terminal. Track token counts, total requests, and even calculate the costs. To learn more, consult the Usage Guide.</li> <li>Model Fine-Tuning: Optimize your models to meet your specific requirements using our fine-tuning app. For more details, check out the Fine-Tuning Guide.</li> </ul>"},{"location":"cli/#support-contribution","title":"Support &amp; Contribution","text":"<p>Need help or want to contribute? Visit our GitHub Repository</p>"},{"location":"cli/finetune/","title":"Using the Command Line Interface","text":"<p>The instructor CLI provides functionalities for managing fine-tuning jobs on OpenAI.</p> <p>Incomplete API</p> <p>The CLI is still under development and does not yet support all features of the API. If you would like to use a feature that is not yet supported, please consider using the contributing to our library jxnl/instructor instead.</p> <pre><code>!!! note \"Low hanging fruit\"\n\n    If you want to contribute we're looking for a few things:\n\n    1. Adding filenames on upload\n</code></pre>"},{"location":"cli/finetune/#creating-a-fine-tuning-job","title":"Creating a Fine-Tuning Job","text":""},{"location":"cli/finetune/#view-jobs-options","title":"View Jobs Options","text":"<pre><code>$ instructor jobs --help\n\n Usage: instructor jobs [OPTIONS] COMMAND [ARGS]...\n\n Monitor and create fine tuning jobs\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help                            Display the help message.                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 cancel                    Cancel a fine-tuning job.                                                         \u2502\n\u2502 create-from-file          Create a fine-tuning job from a file.                                             \u2502\n\u2502 create-from-id            Create a fine-tuning job from an existing ID.                                     \u2502\n\u2502 list                      Monitor the status of the most recent fine-tuning jobs.                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/finetune/#create-from-file","title":"Create from File","text":"<p>The create-from-file command uploads and trains a model in a single step.</p> <pre><code>\u276f instructor jobs create-from-file --help\n\nUsage: instructor jobs create-from-file [OPTIONS] FILE\n\n Create a fine-tuning job from a file.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    file      TEXT  Path to the file for fine-tuning [default: None] [required]                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --model                           TEXT     Model to use for fine-tuning [default: gpt-3.5-turbo]  \u2502\n\u2502 --poll                            INTEGER  Polling interval in seconds [default: 2]               \u2502\n\u2502 --n-epochs                        INTEGER  Number of epochs for fine-tuning                       \u2502\n\u2502 --batch-size                      TEXT     Batch size for fine-tuning                             \u2502\n\u2502 --learning-rate-multiplier        TEXT     Learning rate multiplier for fine-tuning               \u2502\n\u2502 --validation-file                 TEXT     Path to the validation file [default: None]            \u2502\n\u2502 --model-suffix                    TEXT     Suffix to identify the model [default: None]           \u2502\n\u2502 --help                                     Show this message and exit.                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre>"},{"location":"cli/finetune/#usage","title":"Usage","text":"<pre><code>$ instructor jobs create-from-file transformed_data.jsonl --validation_file validation_data.jsonl --n_epochs 3 --batch_size 16 --learning_rate_multiplier 0.5\n</code></pre>"},{"location":"cli/finetune/#create-from-id","title":"Create from ID","text":"<p>The create-from-id command uses an uploaded file and trains a model</p> <pre><code>\u276f instructor jobs create-from-id --help\n\n Usage: instructor jobs create-from-id [OPTIONS] ID\n\n Create a fine-tuning job from an existing ID.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    id      TEXT  ID of the existing fine-tuning job [default: None] [required]      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --model                           TEXT     Model to use for fine-tuning               \u2502\n\u2502                                            [default: gpt-3.5-turbo]                   \u2502\n\u2502 --n-epochs                        INTEGER  Number of epochs for fine-tuning           \u2502\n\u2502 --batch-size                      TEXT     Batch size for fine-tuning                 \u2502\n\u2502 --learning-rate-multiplier        TEXT     Learning rate multiplier for fine-tuning   \u2502\n\u2502 --validation-file-id              TEXT     ID of the uploaded validation file         \u2502\n\u2502                                            [default: None]                            \u2502\n\u2502 --help                                     Show this message and exit.                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/finetune/#usage_1","title":"Usage","text":"<pre><code>$ instructor files upload transformed_data.jsonl\n$ instructor files upload validation_data.jsonl\n$ instructor files list\n...\n$ instructor jobs create_from_id &lt;file_id&gt; --validation_file &lt;validation_file_id&gt; --n_epochs 3 --batch_size 16 --learning_rate_multiplier 0.5\n</code></pre>"},{"location":"cli/finetune/#viewing-files-and-jobs","title":"Viewing Files and Jobs","text":""},{"location":"cli/finetune/#viewing-jobs","title":"Viewing Jobs","text":"<pre><code>$ instructor jobs list\n\nOpenAI Fine Tuning Job Monitoring\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                \u2503              \u2503                \u2503     Completion \u2503                 \u2503                \u2503        \u2503                 \u2503\n\u2503 Job ID         \u2503 Status       \u2503  Creation Time \u2503           Time \u2503 Model Name      \u2503 File ID        \u2503 Epochs \u2503 Base Model      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 ftjob-PWo6uwk\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       23:10:54 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-1whjva8\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       22:47:05 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-wGoBDld\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       22:44:12 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-yd5aRTc\u2026 \u2502 \u2705 succeeded \u2502     2023-08-23 \u2502     2023-08-23 \u2502 ft:gpt-3.5-tur\u2026 \u2502 file-IQxAUDqX\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       14:26:03 \u2502       15:02:29 \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    Automatically refreshes every 5 seconds, press Ctrl+C to exit\n</code></pre>"},{"location":"cli/finetune/#viewing-files","title":"Viewing Files","text":"<pre><code>$ instructor files list\n\nOpenAI Files\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 File ID                       \u2503 Size (bytes) \u2503 Creation Time       \u2503 Filename \u2503 Purpose   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 file-0lw2BSNRUlXZXRRu2beCCWjl \u2502       369523 \u2502 2023-08-23 23:31:57 \u2502 file     \u2502 fine-tune \u2502\n\u2502 file-IHaUXcMEykmFUp1kt2puCDEq \u2502       369523 \u2502 2023-08-23 23:09:35 \u2502 file     \u2502 fine-tune \u2502\n\u2502 file-ja9vRBf0FydEOTolaa3BMqES \u2502       369523 \u2502 2023-08-23 22:42:29 \u2502 file     \u2502 fine-tune \u2502\n\u2502 file-F7lJg6Z47CREvmx4kyvyZ6Sn \u2502       369523 \u2502 2023-08-23 22:42:03 \u2502 file     \u2502 fine-tune \u2502\n\u2502 file-YUxqZPyJRl5GJCUTw3cNmA46 \u2502       369523 \u2502 2023-08-23 22:29:10 \u2502 file     \u2502 fine-tune \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/finetune/#contributions","title":"Contributions","text":"<p>We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request.</p>"},{"location":"cli/usage/","title":"Using the OpenAI API Usage CLI","text":"<p>The OpenAI API Usage CLI tool provides functionalities for monitoring your OpenAI API usage, breaking it down by model, date, and cost.</p>"},{"location":"cli/usage/#monitoring-api-usage","title":"Monitoring API Usage","text":""},{"location":"cli/usage/#view-usage-options","title":"View Usage Options","text":"<pre><code>$ instructor usage --help\n\n Usage: instructor usage [OPTIONS] COMMAND [ARGS]...\n\n Check OpenAI API usage data\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 list       Displays OpenAI API usage data for the past N days.  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/usage/#list-usage-for-specific-number-of-days","title":"List Usage for Specific Number of Days","text":"<p>To display API usage for the past 3 days, use the following command:</p> <pre><code>$ instructor usage list -n 3\n</code></pre> <p>This will output a table similar to:</p> <pre><code>                 Usage Summary by Date, Snapshot, and Cost\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Date       \u2503 Snapshot ID               \u2503 Total Requests \u2503 Total Cost ($) \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2023-09-04 \u2502 gpt-4-0613                \u2502             44 \u2502           0.68 \u2502\n\u2502 2023-09-04 \u2502 gpt-3.5-turbo-16k-0613    \u2502            195 \u2502           0.84 \u2502\n\u2502 2023-09-04 \u2502 text-embedding-ada-002-v2 \u2502            276 \u2502           0.00 \u2502\n\u2502 2023-09-04 \u2502 gpt-4-32k-0613            \u2502            328 \u2502          49.45 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/usage/#list-usage-for-today","title":"List Usage for Today","text":"<p>To display the API usage for today, simply run:</p> <pre><code>$ instructor usage list\n</code></pre>"},{"location":"cli/usage/#contributions","title":"Contributions","text":"<p>We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request.</p>"},{"location":"concepts/alias/","title":"Alias","text":"<p>This page is a work in progress</p> <p>This page is a work in progress. Check out Pydantic's documentation</p>"},{"location":"concepts/caching/","title":"Caching","text":"<p>If you want to learn more about concepts in caching and how to use them in your own projects, check out our blog on the topic.</p>"},{"location":"concepts/caching/#1-functoolscache-for-simple-in-memory-caching","title":"1. <code>functools.cache</code> for Simple In-Memory Caching","text":"<p>When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session. or in an application where we don't need to persist the cache between sessions.</p> <pre><code>import time\nimport functools\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.patch(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@functools.cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n\n\nstart = time.perf_counter()  # (1)\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n#&gt; Time taken: 0.8392175831831992\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\")  # (2)\nprint(f\"Time taken: {time.perf_counter() - start}\")\n#&gt; Time taken: 8.33999365568161e-07\n</code></pre> <ol> <li>Using <code>time.perf_counter()</code> to measure the time taken to run the function is better than using <code>time.time()</code> because it's more accurate and less susceptible to system clock changes.</li> <li>The second time we call <code>extract</code>, the result is returned from the cache, and the function is not called.</li> </ol> <p>Changing the Model does not Invalidate the Cache</p> <p>Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result.</p> <p>Now we can call <code>extract</code> multiple times with the same argument, and the result will be cached in memory for faster access.</p> <p>Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries.</p> What is a decorator? <p>A decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it. In Python, decorators are functions that take a function as an argument and return a closure.</p> <pre><code>def decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Do something before\")  # (1)\n        #&gt; Do something before\n        result = func(*args, **kwargs)\n        print(\"Do something after\")  # (2)\n        #&gt; Do something after\n        return result\n\n    return wrapper\n\n\n@decorator\ndef say_hello():\n    #&gt; Hello!\n    print(\"Hello!\")\n    #&gt; Hello!\n\n\nsay_hello()\n#&gt; \"Do something before\"\n#&gt; \"Hello!\"\n#&gt; \"Do something after\"\n</code></pre> <ol> <li>The code is executed before the function is called</li> <li>The code is executed after the function is called</li> </ol>"},{"location":"concepts/caching/#2-diskcache-for-persistent-large-data-caching","title":"2. <code>diskcache</code> for Persistent, Large Data Caching","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport diskcache\n\ncache = diskcache.Cache('./my_cache_directory')  # (1)\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (2)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <ol> <li>We create a new <code>diskcache.Cache</code> instance to store the cached data. This will create a new directory called <code>my_cache_directory</code> in the current working directory.</li> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic in this example code</li> </ol> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data!</p> <pre><code>import functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation  # (4)\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (\n            f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  #  (2)\n        )\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type (3)\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> <li>We use Pydantic's <code>model_validate_json</code> to deserialize the cached result into a Pydantic model.</li> <li>We use <code>inspect.signature</code> to get the function's return type annotation, which we use to validate the cached result.</li> </ol> <p>Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence.</p>"},{"location":"concepts/caching/#2-redis-caching-decorator-for-distributed-systems","title":"2. Redis Caching Decorator for Distributed Systems","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures.</p> <pre><code>import redis\nimport functools\nimport inspect\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  # (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> </ol> <p>Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types.</p> <p>Looking carefully</p> <p>If you look carefully at the code above you'll notice that we're using the same <code>instructor_cache</code> decorator as before. The implementation is the same, but we're using a different caching backend!</p>"},{"location":"concepts/distillation/","title":"Distilling python functions into LLM","text":"<p><code>Instructions</code> from the <code>Instructor</code> library offers a seamless way to make language models backward compatible with existing Python functions. By employing Pydantic type hints, it not only ensures compatibility but also facilitates fine-tuning <code>gpt-3.5-turbo</code> to emulate these functions end-to-end.</p> <p>If you want to see the full example checkout examples/distillation</p>"},{"location":"concepts/distillation/#the-challenges-in-function-level-fine-tuning","title":"The Challenges in Function-Level Fine-Tuning","text":"<p>Replicating the behavior of a Python function in a language model involves intricate data preparation. For instance, teaching a model to execute three-digit multiplication is not as trivial as implementing <code>def f(a, b): return a * b</code>. OpenAI's fine-tuning script coupled with their function calling utility provides a structured output, thereby simplifying the data collection process. Additionally, this eliminates the need for passing the schema to the model, thus conserving tokens.</p>"},{"location":"concepts/distillation/#the-role-of-instructions-in-simplifying-the-fine-tuning-process","title":"The Role of <code>Instructions</code> in Simplifying the Fine-Tuning Process","text":"<p>By using <code>Instructions</code>, you can annotate a Python function that returns a Pydantic object, thereby automating the dataset creation for fine-tuning. A handler for logging is all that's needed to build this dataset.</p>"},{"location":"concepts/distillation/#how-to-implement-instructions-in-your-code","title":"How to Implement <code>Instructions</code> in Your Code","text":""},{"location":"concepts/distillation/#quick-start-how-to-use-instructors-distillation-feature","title":"Quick Start: How to Use Instructor's Distillation Feature","text":"<p>Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file.</p> <pre><code>import logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions  # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")],\n)\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n\n# Generate some data\nfor _ in range(10):\n    random.seed(42)\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n</code></pre>"},{"location":"concepts/distillation/#the-intricacies-of-fine-tuning-language-models","title":"The Intricacies of Fine-tuning Language Models","text":"<p>Fine-tuning isn't just about writing a function like <code>def f(a, b): return a * b</code>. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this.</p>"},{"location":"concepts/distillation/#why-instructor-and-distillation-are-game-changers","title":"Why Instructor and Distillation are Game Changers","text":"<p>The library offers two main benefits:</p> <ol> <li>Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code.</li> <li>Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions.</li> </ol>"},{"location":"concepts/distillation/#role-of-instructor-in-simplifying-fine-tuning","title":"Role of Instructor in Simplifying Fine-Tuning","text":"<p>The <code>from instructor import Instructions</code> feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior.</p>"},{"location":"concepts/distillation/#logging-output-and-running-a-finetune","title":"Logging Output and Running a Finetune","text":"<p>Here's how the logging output would look:</p> <pre><code>{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\n        {\n            \"role\": \"assistant\",\n            \"function_call\": {\n                \"name\": \"Multiply\",\n                \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}',\n            },\n        },\n    ],\n    \"functions\": [\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\n    ],\n}\n</code></pre> <p>Run a finetune like this:</p> <pre><code>instructor jobs create-from-file math_finetunes.jsonl\n</code></pre> <p>Once a model is trained you can simply change <code>mode</code> to <code>dispatch</code> and it will use the model to run the function!</p> <pre><code>from instructor import Instructions\nfrom pydantic import BaseModel\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")\ndef fn(a: int, b: int) -&gt; Multiply:\n    # now this code will be short circuited and the model will be used instead.\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n</code></pre> <p>With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation.</p>"},{"location":"concepts/enums/","title":"Enums","text":"<p>To prevent data misalignment, we can use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>from pydantic import BaseModel, Field\nfrom enum import Enum\n\n\nclass Role(Enum):\n    PRINCIPAL = \"PRINCIPAL\"\n    TEACHER = \"TEACHER\"\n    STUDENT = \"STUDENT\"\n    OTHER = \"OTHER\"\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(\n        description=\"Correctly assign one of the predefined roles to the user.\"\n    )\n</code></pre> <p>If you're having a hard time with <code>Enum</code> and alternative is to use <code>Literal</code> instead.</p> <pre><code>from typing import Literal\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n</code></pre>"},{"location":"concepts/fastapi/","title":"Integrating Pydantic Models with FastAPI","text":"<p>FastAPI is an enjoyable tool for building web applications in Python. It is well known for its integration with <code>Pydantic</code> models, which makes defining and validating data structures straightforward and efficient. In this guide, we explore how simple functions that return <code>Pydantic</code> models can seamlessly integrate with <code>FastAPI</code>.</p>"},{"location":"concepts/fastapi/#why-choose-fastapi-and-pydantic","title":"Why Choose FastAPI and Pydantic?","text":"<ul> <li>FastAPI is a modern, high-performance web framework for building APIs with Python.</li> <li>Supports OpenAPI and JSON Schema for automatic documentation and validation.</li> <li>Supports AsyncIO for asynchronous programming leveraging the AsyncOpenAI() client</li> </ul>"},{"location":"concepts/fastapi/#code-example-starting-a-fastapi-app-with-a-post-request","title":"Code Example: Starting a FastAPI App with a POST Request","text":"<p>The following code snippet demonstrates how to start a <code>FastAPI</code> app with a POST endpoint. This endpoint accepts and returns data defined by a <code>Pydantic</code> model.</p> <pre><code>import instructor\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables response_model\nclient = instructor.patch(AsyncOpenAI())\napp = FastAPI()\n\n\nclass UserData(BaseModel):\n    # This can be the model for the input data\n    query: str\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@app.post(\"/endpoint\", response_model=UserDetail)\nasync def endpoint_function(data: UserData) -&gt; UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\n        ],\n    )\n    return user_detail\n</code></pre>"},{"location":"concepts/fastapi/#streaming-responses-with-fastapi","title":"Streaming Responses with FastAPI","text":"<p><code>FastAPI</code> supports streaming responses, which is useful for returning large amounts of data. This feature is particularly useful when working with large language models (LLMs) that generate a large amount of data.</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass UserData(BaseModel):\n    query: str\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n# Route to handle SSE events and return users\n@app.post(\"/extract\", response_class=StreamingResponse)\nasync def extract(data: UserData):\n    users = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": data.query},\n        ],\n    )\n\n    async def generate():\n        for user in users:\n            resp_json = user.model_dump_json()\n            yield f\"data: {resp_json}\"\n        yield \"data: [DONE]\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n</code></pre>"},{"location":"concepts/fastapi/#automatic-documentation-with-fastapi","title":"Automatic Documentation with FastAPI","text":"<p>FastAPI leverages the OpenAPI specification to automatically generate a dynamic and interactive documentation page, commonly referred to as the <code>/docs</code> page. This feature is incredibly useful for developers, as it offers a live environment to test API endpoints directly through the browser.</p> <p>To explore the capabilities of your API, follow these steps:</p> <ol> <li>Run the API using the Uvicorn command: <code>uvicorn main:app --reload</code>.</li> <li>Open your web browser and navigate to <code>http://127.0.0.1:8000/docs</code>.</li> <li>You will find an interactive UI where you can send different requests to your API and see the responses in real-time.</li> </ol> <p></p>"},{"location":"concepts/fields/","title":"Fields","text":"<p>The <code>pydantic.Field</code> function is used to customize and add metadata to fields of models. To learn more, check out the Pydantic documentation as this is a near replica of that documentation that is relevant to prompting.</p>"},{"location":"concepts/fields/#default-values","title":"Default values","text":"<p>The <code>default</code> parameter is used to define a default value for a field.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str = Field(default='John Doe')\n\n\nuser = User()\nprint(user)\n#&gt; name='John Doe'\n</code></pre> <p>You can also use <code>default_factory</code> to define a callable that will be called to generate a default value.</p> <pre><code>from uuid import uuid4\n\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: str = Field(default_factory=lambda: uuid4().hex)\n</code></pre> <p>Info</p> <p>The <code>default</code> and <code>default_factory</code> parameters are mutually exclusive.</p> <p>Note</p> <p>If you use <code>typing.Optional</code>, it doesn't mean that the field has a default value of <code>None</code> you must use <code>default</code> or <code>default_factory</code> to define a default value. Then it will be considered <code>not required</code> when sent to the language model.</p>"},{"location":"concepts/fields/#using-annotated","title":"Using <code>Annotated</code>","text":"<p>The <code>Field</code> function can also be used together with <code>Annotated</code>.</p> <pre><code>from uuid import uuid4\n\nfrom typing_extensions import Annotated\n\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: Annotated[str, Field(default_factory=lambda: uuid4().hex)]\n</code></pre>"},{"location":"concepts/fields/#exclude","title":"Exclude","text":"<p>The <code>exclude</code> parameter can be used to control which fields should be excluded from the model when exporting the model. This is helpful when you want to exclude fields that are not relevant to the model generation like <code>scratch_pad</code> or <code>chain_of_thought</code></p> <p>See the following example:</p> <pre><code>from pydantic import BaseModel, Field\nfrom datetime import date\n\n\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Reasoning behind the date range.\", exclude=True\n    )\n    start_date: date\n    end_date: date\n\n\ndate_range = DateRange(\n    chain_of_thought=\"\"\"\n        I want to find the date range for the last 30 days.\n        Today is 2021-01-30 therefore the start date\n        should be 2021-01-01 and the end date is 2021-01-30\"\"\",\n    start_date=date(2021, 1, 1),\n    end_date=date(2021, 1, 30),\n)\nprint(date_range.model_dump_json())\n#&gt; {\"start_date\":\"2021-01-01\",\"end_date\":\"2021-01-30\"}\n</code></pre>"},{"location":"concepts/fields/#customizing-json-schema","title":"Customizing JSON Schema","text":"<p>There are some fields that are exclusively used to customise the generated JSON Schema:</p> <ul> <li><code>title</code>: The title of the field.</li> <li><code>description</code>: The description of the field.</li> <li><code>examples</code>: The examples of the field.</li> <li><code>json_schema_extra</code>: Extra JSON Schema properties to be added to the field.</li> </ul> <p>These all work as great opportunities to add more information to the JSON schema as part of your prompt engineering.</p> <p>Here's an example:</p> <pre><code>from pydantic import BaseModel, Field, SecretStr\n\n\nclass User(BaseModel):\n    age: int = Field(description='Age of the user')\n    name: str = Field(title='Username')\n    password: SecretStr = Field(\n        json_schema_extra={\n            'title': 'Password',\n            'description': 'Password of the user',\n            'examples': ['123456'],\n        }\n    )\n\n\nprint(User.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'age': {'description': 'Age of the user', 'title': 'Age', 'type': 'integer'},\n        'name': {'title': 'Username', 'type': 'string'},\n        'password': {\n            'description': 'Password of the user',\n            'examples': ['123456'],\n            'format': 'password',\n            'title': 'Password',\n            'type': 'string',\n            'writeOnly': True,\n        },\n    },\n    'required': ['age', 'name', 'password'],\n    'title': 'User',\n    'type': 'object',\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/fields/#general-notes-on-json-schema-generation","title":"General notes on JSON schema generation","text":"<ul> <li>The JSON schema for Optional fields indicates that the value null is allowed.</li> <li>The Decimal type is exposed in JSON schema (and serialized) as a string.</li> <li>The JSON schema does not preserve namedtuples as namedtuples.</li> <li>When they differ, you can specify whether you want the JSON schema to represent the inputs to validation or the outputs from serialization.</li> <li>Sub-models used are added to the <code>$defs</code> JSON attribute and referenced, as per the spec.</li> <li>Sub-models with modifications (via the Field class) like a custom title, description, or default value, are recursively included instead of referenced.</li> <li>The description for models is taken from either the docstring of the class or the argument description to the Field class.</li> </ul>"},{"location":"concepts/lists/","title":"Multi-task and Streaming","text":"<p>A common use case of structured extraction is defining a single schema class and then making another schema to create a list to do multiple extraction</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclass Users(BaseModel):\n    users: List[User]\n\n\nprint(Users.model_json_schema())\n\"\"\"\n{\n    '$defs': {\n        'User': {\n            'properties': {\n                'name': {'title': 'Name', 'type': 'string'},\n                'age': {'title': 'Age', 'type': 'integer'},\n            },\n            'required': ['name', 'age'],\n            'title': 'User',\n            'type': 'object',\n        }\n    },\n    'properties': {\n        'users': {'items': {'$ref': '#/$defs/User'}, 'title': 'Users', 'type': 'array'}\n    },\n    'required': ['users'],\n    'title': 'Users',\n    'type': 'object',\n}\n\"\"\"\n</code></pre> <p>Defining a task and creating a list of classes is a common enough pattern that we make this convenient by making use of <code>Iterable[T]</code>. This lets us dynamically create a new class that:</p> <ol> <li>Has dynamic docstrings and class name based on the task</li> <li>Support streaming by collecting tokens until a task is received back out.</li> </ol>"},{"location":"concepts/lists/#extracting-tasks-using-iterable","title":"Extracting Tasks using Iterable","text":"<p>By using <code>Iterable</code> you get a very convenient class with prompts and names automatically defined:</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[User],\n    stream=False,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Consider this data: Jason is 10 and John is 30.\\\n                         Correctly segment it into entitites\\\n                        Make sure the JSON is correct\",\n        },\n    ],\n)\nfor user in users:\n    print(user)\n    #&gt; name='Jason' age=10\n    #&gt; name='John' age=30\n</code></pre>"},{"location":"concepts/lists/#streaming-tasks","title":"Streaming Tasks","text":"<p>We can also generate tasks as the tokens are streamed in by defining an <code>Iterable[T]</code> type.</p> <p>Lets look at an example in action with the same class</p> <pre><code>import instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.patch(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Iterable[User],\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (f\"Extract `Jason is 10 and John is 10`\"),\n        },\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    print(user)\n    #&gt; name='Jason' age=10\n    #&gt; name='John' age=10\n</code></pre>"},{"location":"concepts/lists/#asynchronous-streaming","title":"Asynchronous Streaming","text":"<p>I also just want to call out in this example that <code>instructor</code> also supports asynchronous streaming. This is useful when you want to stream a response model and process the results as they come in, but you'll need to use the <code>async for</code> syntax to iterate over the results.</p> <pre><code>import instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.patch(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nasync def print_iterable_results():\n    model = await client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[UserExtract],\n        max_retries=2,\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Make two up people\"},\n        ],\n    )\n    async for m in model:\n        print(m)\n        #&gt; name='John Smith' age=30\n        #&gt; name='Mary Jane' age=28\n\n\nimport asyncio\n\nasyncio.run(print_iterable_results())\n</code></pre>"},{"location":"concepts/logging/","title":"Logging","text":"<p>In order to see the requests made to OpenAI and the responses, you can set logging to DEBUG. This will show the requests and responses made to OpenAI. This can be useful for debugging and understanding the requests and responses made to OpenAI.</p> <pre><code>import instructor\nimport openai\nimport logging\n\nfrom pydantic import BaseModel\n\n# Set logging to DEBUG\nlogging.basicConfig(level=logging.DEBUG)\n\nclient = instructor.patch(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)\n</code></pre>"},{"location":"concepts/maybe/","title":"Handling Missing Data","text":"<p>The <code>Maybe</code> pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning <code>None</code>, you can use a <code>Maybe</code> type to encapsulate both the result and potential errors.</p> <p>This pattern is particularly useful when making LLM calls, as providing language models with an escape hatch can effectively reduce hallucinations.</p>"},{"location":"concepts/maybe/#defining-the-model","title":"Defining the Model","text":"<p>Using Pydantic, we'll first define the <code>UserDetail</code> and <code>MaybeUser</code> classes.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>Notice that <code>MaybeUser</code> has a <code>result</code> field that is an optional <code>UserDetail</code> instance where the extracted data will be stored. The <code>error</code> field is a boolean that indicates whether an error occurred, and the <code>message</code> field is an optional string that contains the error message.</p>"},{"location":"concepts/maybe/#defining-the-function","title":"Defining the function","text":"<p>Once we have the model defined, we can create a function that uses the <code>Maybe</code> pattern to extract the data.</p> <pre><code>import instructor\nimport openai\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\n# This enables the `response_model` keyword\nclient = instructor.patch(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n\n\ndef extract(content: str) -&gt; MaybeUser:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MaybeUser,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\n        ],\n    )\n\n\nuser1 = extract(\"Jason is a 25-year-old scientist\")\nprint(user1.model_dump_json(indent=2))\n\"\"\"\n{\n  \"result\": {\n    \"age\": 25,\n    \"name\": \"Jason\",\n    \"role\": \"scientist\"\n  },\n  \"error\": false,\n  \"message\": null\n}\n\"\"\"\n\nuser2 = extract(\"Unknown user\")\nprint(user2.model_dump_json(indent=2))\n\"\"\"\n{\n  \"result\": null,\n  \"error\": false,\n  \"message\": null\n}\n\"\"\"\n</code></pre> <p>As you can see, when the data is extracted successfully, the <code>result</code> field contains the <code>UserDetail</code> instance. When an error occurs, the <code>error</code> field is set to <code>True</code>, and the <code>message</code> field contains the error message.</p> <p>If you want to learn more about pattern matching, check out Pydantic's docs on Structural Pattern Matching</p>"},{"location":"concepts/models/","title":"Response Model","text":"<p>Defining LLM output schemas in Pydantic is done via <code>pydantic.BaseModel</code>. To learn more about models in Pydantic, check out their documentation.</p> <p>After defining a Pydantic model, we can use it as the <code>response_model</code> in your client <code>create</code> calls to OpenAI or any other supported model. The job of the <code>response_model</code> parameter is to:</p> <ul> <li>Define the schema and prompts for the language model</li> <li>Validate the response from the API</li> <li>Return a Pydantic model instance.</li> </ul>"},{"location":"concepts/models/#prompting","title":"Prompting","text":"<p>When defining a response model, we can use docstrings and field annotations to define the prompt that will be used to generate the response.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    \"\"\"\n    This is the prompt that will be used to generate the response.\n    Any instructions here will be passed to the language model.\n    \"\"\"\n\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n</code></pre> <p>Here all docstrings, types, and field annotations will be used to generate the prompt. The prompt will be generated by the <code>create</code> method of the client and will be used to generate the response.</p>"},{"location":"concepts/models/#optional-values","title":"Optional Values","text":"<p>If we use <code>Optional</code> and <code>default</code>, they will be considered not required when sent to the language model</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n    email: Optional[str] = Field(description=\"The email of the user.\", default=None)\n</code></pre>"},{"location":"concepts/models/#dynamic-model-creation","title":"Dynamic model creation","text":"<p>There are some occasions where it is desirable to create a model using runtime information to specify the fields. For this, Pydantic provides the create_model function to allow models to be created on the fly:</p> <pre><code>from pydantic import BaseModel, create_model\n\n\nclass FooModel(BaseModel):\n    foo: str\n    bar: int = 123\n\n\nBarModel = create_model(\n    'BarModel',\n    apple=(str, 'russet'),\n    banana=(str, 'yellow'),\n    __base__=FooModel,\n)\nprint(BarModel)\n#&gt; &lt;class '__main__.BarModel'&gt;\nprint(BarModel.model_fields.keys())\n#&gt; dict_keys(['foo', 'bar', 'apple', 'banana'])\n</code></pre> When would I use this? <p>Consider a situation where the model is dynamically defined, based on some configuration or database. For example, we could have a database table that stores the properties of a model for some model name or id. We could then query the database for the properties of the model and use that to create the model.</p> <pre><code>SELECT property_name, property_type, description\nFROM prompt\nWHERE model_name = {model_name}\n</code></pre> <p>We can then use this information to create the model.</p> <pre><code>from pydantic import BaseModel, create_model\nfrom typing import List\n\ntypes = {\n    'string': str,\n    'integer': int,\n    'boolean': bool,\n    'number': float,\n    'List[str]': List[str],\n}\n\n# Mocked cursor.fetchall()\ncursor = [\n    ('name', 'string', 'The name of the user.'),\n    ('age', 'integer', 'The age of the user.'),\n    ('email', 'string', 'The email of the user.'),\n]\n\nBarModel = create_model(\n    'User',\n    **{\n        property_name: (types[property_type], description)\n        for property_name, property_type, description in cursor\n    },\n    __base__=BaseModel,\n)\n\nprint(BarModel.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'name': {'default': 'The name of the user.', 'title': 'Name', 'type': 'string'},\n        'age': {'default': 'The age of the user.', 'title': 'Age', 'type': 'integer'},\n        'email': {\n            'default': 'The email of the user.',\n            'title': 'Email',\n            'type': 'string',\n        },\n    },\n    'title': 'User',\n    'type': 'object',\n}\n\"\"\"\n</code></pre> <p>This would be useful when different users have different descriptions for the same model. We can use the same model but have different prompts for each user.</p>"},{"location":"concepts/models/#adding-behavior","title":"Adding Behavior","text":"<p>We can add methods to our Pydantic models, just as any plain Python class. We might want to do this to add some custom logic to our models.</p> <pre><code>from pydantic import BaseModel\nfrom typing import Literal\n\nfrom openai import OpenAI\n\nimport instructor\n\nclient = instructor.patch(OpenAI())\n\n\nclass SearchQuery(BaseModel):\n    query: str\n    query_type: Literal[\"web\", \"image\", \"video\"]\n\n    def execute(self):\n        print(f\"Searching for {self.query} of type {self.query_type}\")\n        #&gt; Searching for cat pictures of type image\n        return \"Results for cat\"\n\n\nquery = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Search for a picture of a cat\"}],\n    response_model=SearchQuery,\n)\n\nresults = query.execute()\nprint(results)\n#&gt; Results for cat\n</code></pre> <p>Now we can call <code>execute</code> on our model instance after extracting it from a language model. If you want to see more examples of this checkout our post on RAG is more than embeddings</p>"},{"location":"concepts/parallel/","title":"Parallel Tools","text":"<p>One of the latest capabilities that OpenAI has recently introduced is parallel function calling. To learn more you can read up on this</p> <p>Experimental Feature</p> <p>This feature is currently in preview and is subject to change. only supported by the <code>gpt-4-turbo-preview</code> model.</p>"},{"location":"concepts/parallel/#understanding-parallel-function-calling","title":"Understanding Parallel Function Calling","text":"<p>By using parallel function callings that allow you to call multiple functions in a single request, you can significantly reduce the latency of your application without having to use tricks with now one builds a schema.</p> <pre><code>import openai\nimport instructor\n\nfrom typing import Iterable, Literal\nfrom pydantic import BaseModel\n\n\nclass Weather(BaseModel):\n    location: str\n    units: Literal[\"imperial\", \"metric\"]\n\n\nclass GoogleSearch(BaseModel):\n    query: str\n\n\nclient = instructor.patch(openai.OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS)  # (1)!\n\nfunction_calls = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You must always use tools\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather in toronto and dallas and who won the super bowl?\",\n        },\n    ],\n    response_model=Iterable[Weather | GoogleSearch],  # (2)!\n)\n\nfor fc in function_calls:\n    print(fc)\n    #&gt; location='Toronto' units='imperial'\n    #&gt; location='Dallas' units='imperial'\n    #&gt; query='who won the super bowl'\n</code></pre> <ol> <li>Set the mode to <code>PARALLEL_TOOLS</code> to enable parallel function calling.</li> <li>Set the response model to <code>Iterable[Weather | GoogleSearch]</code> to indicate that the response will be a list of <code>Weather</code> and <code>GoogleSearch</code> objects. This is necessary because the response will be a list of objects, and we need to specify the types of the objects in the list.</li> </ol> <p>Noticed that the <code>response_model</code> Must be in the form <code>Iterable[Type1 | Type2 | ...]</code> or <code>Iterable[Type1]</code> where <code>Type1</code> and <code>Type2</code> are the types of the objects that will be returned in the response.</p>"},{"location":"concepts/partial/","title":"Streaming Partial Responses","text":"<p>Field level streaming provides incremental snapshots of the current state of the response model that are immediately useable. This approach is particularly relevant in contexts like rendering UI components.</p> <p>Instructor supports this pattern by making use of <code>Partial[T]</code>. This lets us dynamically create a new class that treats all of the original model's fields as <code>Optional</code>.</p>"},{"location":"concepts/partial/#understanding-partial-responses","title":"Understanding Partial Responses","text":"<p>Consider what happens whene we define a response model:</p> <pre><code>from pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n</code></pre> <p>If we streamed json out from OpenAI, we would only be able to parse when the object is completed returned!</p> <pre><code>{\"name\": \"Jo\n{\"name\": \"John\", \"ag\n{\"name\": \"John\", \"age\":\n{\"name\": \"John\", \"age\": 25} # Completed\n</code></pre> <p>When specifying a <code>Partial[T]</code> and setting <code>stream=True</code>, the response from <code>instructor</code> becomes a <code>Generator[T]</code>. As the generator yields results, you can iterate over these incremental updates. The last value yielded by the generator represents the completed extraction!</p> <pre><code>{\"name\": \"Jo                 =&gt; User(name=\"Jo\", age=None)\n{\"name\": \"John\", \"ag         =&gt; User(name=\"John\", age=None)\n{\"name\": \"John\", \"age:       =&gt; User(name=\"John\", age=None)\n{\"name\": \"John\", \"age\": 25}  =&gt; User(name=\"John\", age=25)\n</code></pre> <p>Limited Validator Support</p> <p>Fewer validators are supported by <code>Partial</code> response models as streamed fields will natural raise validation error, as we do not have a strong opinoin on how to handle them.</p> <p>Let's look at an example of streaming an extraction of conference information, that would be used to stream in an react component.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\nfrom rich.console import Console\n\nclient = instructor.patch(OpenAI())\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nextraction_stream = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=instructor.Partial[MeetingInfo],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)\n\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n\nprint(extraction.model_dump_json(indent=2))\n\"\"\"\n{\n  \"users\": [\n    {\n      \"name\": \"John Doe\",\n      \"email\": \"johndoe@email.com\",\n      \"twitter\": \"@TechGuru44\"\n    },\n    {\n      \"name\": \"Jane Smith\",\n      \"email\": \"janesmith@email.com\",\n      \"twitter\": \"@DigitalDiva88\"\n    },\n    {\n      \"name\": \"Alex Johnson\",\n      \"email\": \"alexj@email.com\",\n      \"twitter\": \"@CodeMaster2023\"\n    }\n  ],\n  \"date\": \"2024-03-15\",\n  \"location\": \"Grand Tech Arena located at 4521 Innovation Drive\",\n  \"budget\": 50000,\n  \"deadline\": \"2024-02-20\"\n}\n\"\"\"\n</code></pre> <p>This will output the following:</p> <p></p>"},{"location":"concepts/partial/#asynchronous-streaming","title":"Asynchronous Streaming","text":"<p>I also just want to call out in this example that <code>instructor</code> also supports asynchronous streaming. This is useful when you want to stream a response model and process the results as they come in, but you'll need to use the <code>async for</code> syntax to iterate over the results.</p> <pre><code>import instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.patch(AsyncOpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def print_partial_results():\n    user = await client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        response_model=instructor.Partial[User],\n        max_retries=2,\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Jason is 12 years old\"},\n        ],\n    )\n    async for m in user:\n        print(m)\n        #&gt; name=None age=None\n        #&gt; name='' age=None\n        #&gt; name='Jason' age=None\n        #&gt; name='Jason' age=12\n\n\nimport asyncio\n\nasyncio.run(print_partial_results())\n</code></pre>"},{"location":"concepts/patching/","title":"Patching","text":"<p>Instructor enhances client functionality with three new keywords for backwards compatibility. This allows use of the enhanced client as usual, with structured output benefits.</p> <ul> <li><code>response_model</code>: Defines the response type for <code>chat.completions.create</code>.</li> <li><code>max_retries</code>: Determines retry attempts for failed <code>chat.completions.create</code> validations.</li> <li><code>validation_context</code>: Provides extra context to the validation process.</li> </ul> <p>The default mode is <code>instructor.Mode.TOOLS</code> which is the recommended mode for OpenAI clients. This mode is the most stable and is the most recommended for OpenAI clients. The other modes are for other clients and are not recommended for OpenAI clients.</p>"},{"location":"concepts/patching/#tool-calling","title":"Tool Calling","text":"<p>This is the recommended method for OpenAI clients. It is the most stable as functions is being deprecated soon.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.TOOLS)\n</code></pre>"},{"location":"concepts/patching/#parallel-tool-calling","title":"Parallel Tool Calling","text":"<p>Parallel tool calling is also an option but you must set <code>response_model</code> to be <code>Iterable[Union[...]]</code> types since we expect an array of results. Check out Parallel Tool Calling for more information.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS)\n</code></pre>"},{"location":"concepts/patching/#function-calling","title":"Function Calling","text":"<p>Note that function calling is soon to be deprecated in favor of TOOL mode for OpenAI. But will still be supported for other clients.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.FUNCTIONS)\n</code></pre>"},{"location":"concepts/patching/#json-mode","title":"JSON Mode","text":"<p>JSON mode uses OpenAI's JSON fromat for responses. by setting <code>response_format={\"type\": \"json_object\"}</code> in the <code>chat.completions.create</code> method.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.JSON)\n</code></pre>"},{"location":"concepts/patching/#json-schema-mode","title":"JSON Schema Mode","text":"<p>JSON Schema mode uses OpenAI's JSON fromat for responses. by setting <code>response_format={\"type\": \"json_object\", schema:response_model.model_json_schema()}</code> in the <code>chat.completions.create</code> method. This is only available for select clients (e.g. llama-cpp-python, Anyscale, Together)</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.JSON_SCHEMA)\n</code></pre>"},{"location":"concepts/patching/#markdown-json-mode","title":"Markdown JSON Mode","text":"<p>This just asks for the response in JSON format, but it is not recommended, and may not be supported in the future, this is just left to support vision models and will not give you the full benefits of instructor.</p> <p>Experimental</p> <p>This is not recommended, and may not be supported in the future, this is just left to support vision models.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n</code></pre>"},{"location":"concepts/philosophy/","title":"Philosophy","text":"<p>The instructor values simplicity and flexibility in leveraging language models (LLMs). It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions. Let Pydantic do the heavy lifting.</p> <p>\u201cSimplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.\u201d \u2014 Edsger Dijkstra</p>"},{"location":"concepts/philosophy/#proof-that-its-simple","title":"Proof that its simple","text":"<ol> <li>Most users will only need to learn <code>response_model</code> and <code>patch</code> to get started.</li> <li>No new prompting language to learn, no new abstractions to learn.</li> </ol>"},{"location":"concepts/philosophy/#proof-that-its-transparent","title":"Proof that its transparent","text":"<ol> <li>We write very little prompts, and we don't try to hide the prompts from you.</li> <li>We'll do better in the future to give you config over the 2 prompts we do write, Reasking and JSON_MODE prompts.</li> </ol>"},{"location":"concepts/philosophy/#proof-that-its-flexible","title":"Proof that its flexible","text":"<ol> <li>If you build a system with OpenAI dirrectly, it is easy to incrementally adopt instructor.</li> <li>Add <code>response_model</code> and if you want to revert, just remove it.</li> </ol>"},{"location":"concepts/philosophy/#the-zen-of-instructor","title":"The zen of <code>instructor</code>","text":"<p>Maintain the flexibility and power of Python, without unnecessary constraints.</p> <p>Begin with a function and a return type hint \u2013 simplicity is key. With my experience maintaining a large enterprize framework at my previous job over many years I've learned that the goal of a making a useful framework is minimizing regret, both for the author and hopefully for the user.</p> <ol> <li>Define a Schema <code>class StructuredData(BaseModel):</code></li> <li>Define validators and methods on your schema.</li> <li>Encapsulate all your LLM logic into a function <code>def extract(a) -&gt; StructuredData:</code></li> <li>Define typed computations against your data with <code>def compute(data: StructuredData):</code> or call methods on your schema <code>data.compute()</code></li> </ol> <p>It should be that simple.</p>"},{"location":"concepts/philosophy/#my-goals","title":"My Goals","text":"<p>The goal for the library, documentation, and blog, is to help you be a better python programmer and as a result a better AI engineer.</p> <ul> <li>The library is a result of my desire for simplicity.</li> <li>The library should help maintain simplicity in your codebase.</li> <li>I won't try to write prompts for you,</li> <li>I don't try to create indirections or abstractions that make it hard to debug in the future</li> </ul> <p>Please note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit me up on twitter</p> <p>Cheers!</p>"},{"location":"concepts/prompting/","title":"General Tips for Prompt Engineering","text":"<p>The overarching theme of using Instructor and Pydantic for function calling is to make the models as self-descriptive, modular, and flexible as possible, while maintaining data integrity and ease of use.</p> <ul> <li>Modularity: Design self-contained components for reuse.</li> <li>Self-Description: Use Pydantic's <code>Field</code> for clear field descriptions.</li> <li>Optionality: Use Python's <code>Optional</code> type for nullable fields and set sensible defaults.</li> <li>Standardization: Employ enumerations for fields with a fixed set of values; include a fallback option.</li> <li>Dynamic Data: Use key-value pairs for arbitrary properties and limit list lengths.</li> <li>Entity Relationships: Define explicit identifiers and relationship fields.</li> <li>Contextual Logic: Optionally add a \"chain of thought\" field in reusable components for extra context.</li> </ul>"},{"location":"concepts/prompting/#modular-chain-of-thought","title":"Modular Chain of Thought","text":"<p>This approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass Role(BaseModel):\n    chain_of_thought: str = Field(\n        ..., description=\"Think step by step to determine the correct title\"\n    )\n    title: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>"},{"location":"concepts/prompting/#utilize-optional-attributes","title":"Utilize Optional Attributes","text":"<p>Use Python's Optional type and set a default value to prevent undesired defaults like empty strings.</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel, Field\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"concepts/prompting/#handling-errors-within-function-calls","title":"Handling Errors Within Function Calls","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>With the <code>MaybeUser</code> class, you can either receive a <code>UserDetail</code> object in result or get an error message in message.</p>"},{"location":"concepts/prompting/#simplification-with-the-maybe-pattern","title":"Simplification with the Maybe Pattern","text":"<p>You can further simplify this using instructor to create the <code>Maybe</code> pattern dynamically from any <code>BaseModel</code>.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n\n\nMaybeUser = instructor.Maybe(UserDetail)\n</code></pre> <p>This allows you to quickly create a Maybe type for any class, streamlining the process.</p>"},{"location":"concepts/prompting/#tips-for-enumerations","title":"Tips for Enumerations","text":"<p>To prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>from enum import Enum, auto\nfrom pydantic import BaseModel, Field\n\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(\n        description=\"Correctly assign one of the predefined roles to the user.\"\n    )\n</code></pre> <p>If you're having a hard time with <code>Enum</code> and alternative is to use <code>Literal</code></p> <pre><code>from typing import Literal\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n</code></pre> <p>If you'd like to improve performance more you can reiterate the requirements in the field descriptions or in the docstrings.</p>"},{"location":"concepts/prompting/#reiterate-long-instructions","title":"Reiterate Long Instructions","text":"<p>For complex attributes, it helps to reiterate the instructions in the field's description.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass Role(BaseModel):\n    \"\"\"\n    Extract the role based on the following rules ...\n    \"\"\"\n\n    instructions: str = Field(\n        ...,\n        description=\"Restate the instructions and rules to correctly determine the title.\",\n    )\n    title: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>"},{"location":"concepts/prompting/#handle-arbitrary-properties","title":"Handle Arbitrary Properties","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(\n        ..., description=\"Extract any other properties that might be relevant.\"\n    )\n</code></pre>"},{"location":"concepts/prompting/#limiting-the-length-of-lists","title":"Limiting the Length of Lists","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str\n    value: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be less than 6\",\n    )\n</code></pre> <p>Using Tuples for Simple Types</p> <p>For simple types, tuples can be a more compact alternative to custom classes, especially when the properties don't require additional descriptions.</p> <pre><code>from typing import List, Tuple\nfrom pydantic import BaseModel, Field\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Tuple[int, str]] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be less than 6\",\n    )\n</code></pre>"},{"location":"concepts/prompting/#advanced-arbitrary-properties","title":"Advanced Arbitrary Properties","text":"<p>For multiple users, aim to use consistent key names when extracting properties.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n\n\nclass UserDetails(BaseModel):\n    \"\"\"\n    Extract information for multiple users.\n    Use consistent key names for properties across users.\n    \"\"\"\n\n    users: List[UserDetail]\n</code></pre> <p>This refined guide should offer a cleaner and more organized approach to structure engineering in Python.</p>"},{"location":"concepts/prompting/#defining-relationships-between-entities","title":"Defining Relationships Between Entities","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model. The following example demonstrates how to define relationships between users by incorporating an id and a friends field:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    friends: List[int] = Field(\n        ...,\n        description=\"Correct and complete list of friend IDs, representing relationships between users.\",\n    )\n\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail] = Field(\n        ...,\n        description=\"Collection of users, correctly capturing the relationships among them.\",\n    )\n</code></pre>"},{"location":"concepts/prompting/#reusing-components-with-different-contexts","title":"Reusing Components with Different Contexts","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both work_time and leisure_time.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass TimeRange(BaseModel):\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    work_time: TimeRange = Field(\n        ..., description=\"Time range during which the user is working.\"\n    )\n    leisure_time: TimeRange = Field(\n        ..., description=\"Time range reserved for leisure activities.\"\n    )\n</code></pre> <p>Sometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass TimeRange(BaseModel):\n    chain_of_thought: str = Field(\n        ..., description=\"Step by step reasoning to get the correct time range\"\n    )\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n</code></pre>"},{"location":"concepts/raw_response/","title":"Raw Response","text":"<p>Often times not only do you want the base model but may also want the original response from the API. You can do this by retrieving the <code>raw_response</code>, since the <code>raw_response</code> is also a pydantic model, you can use any of the pydantic model methods on it.</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI())\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(user._raw_response)\n\"\"\"\nChatCompletion(\n    id='chatcmpl-8u9bsrmmf5YjZyfCtQymoZV8LK1qg',\n    choices=[\n        Choice(\n            finish_reason='stop',\n            index=0,\n            logprobs=None,\n            message=ChatCompletionMessage(\n                content=None,\n                role='assistant',\n                function_call=None,\n                tool_calls=[\n                    ChatCompletionMessageToolCall(\n                        id='call_O5rpXf47YgXiYrYWv45yZUeM',\n                        function=Function(\n                            arguments='{\"name\":\"Jason\",\"age\":25}', name='UserExtract'\n                        ),\n                        type='function',\n                    )\n                ],\n            ),\n        )\n    ],\n    created=1708394000,\n    model='gpt-3.5-turbo-0125',\n    object='chat.completion',\n    system_fingerprint='fp_69829325d0',\n    usage=CompletionUsage(completion_tokens=9, prompt_tokens=82, total_tokens=91),\n)\n\"\"\"\n</code></pre> <p>Accessing tokens usage</p> <p>This is the recommended way to access the tokens usage, since it is a pydantic model you can use any of the pydantic model methods on it. For example, you can access the <code>total_tokens</code> by doing <code>user._raw_response.usage.total_tokens</code>. Note that this also includes the tokens used during any previous unsuccessful attempts.</p> <p>In the future, we may add additional hooks to the <code>raw_response</code> to make it easier to access the tokens usage.</p>"},{"location":"concepts/reask_validation/","title":"Validation and Reasking","text":"<p>Instead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the system can use to self-correct.</p>"},{"location":"concepts/reask_validation/#pydantic","title":"Pydantic","text":"<p>Pydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic docs on validators.</p> <p>Good llm validation is just good validation</p> <p>If you want to see some more examples on validators checkout our blog post Good LLM validation is just good validation</p>"},{"location":"concepts/reask_validation/#code-based-validation-example","title":"Code-based Validation Example","text":"<p>First define a Pydantic model with a validator using the <code>Annotation</code> class from <code>typing_extensions</code>.</p> <p>Enforce a naming rule using Pydantic's built-in validation:</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\n\ndef name_must_contain_space(v: str) -&gt; str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/value_error\n    \"\"\"\n</code></pre>"},{"location":"concepts/reask_validation/#output-for-code-based-validation","title":"Output for Code-Based Validation","text":"<pre><code>1 validation error for UserDetail\nname\n   Value error, name must contain a space (type=value_error)\n</code></pre> <p>As we can see, Pydantic raises a validation error when the name attribute does not contain a space. This is a simple example, but it demonstrates how Pydantic can be used to validate attributes of a model.</p>"},{"location":"concepts/reask_validation/#llm-based-validation-example","title":"LLM-Based Validation Example","text":"<p>LLM-based validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error.</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\n\n# Apply the patch to the OpenAI client\nclient = instructor.patch(OpenAI())\n\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\", openai_client=client)\n        ),\n    ]\n\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for QuestionAnswer\n    answer\n      Assertion failed, The statement promotes objectionable behavior by encouraging evil and theft. [type=assertion_error, input_value='The meaning of life is to be evil and steal', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n    \"\"\"\n</code></pre>"},{"location":"concepts/reask_validation/#output-for-llm-based-validation","title":"Output for LLM-Based Validation","text":"<p>It is important to not here that the error message is generated by the LLM, not the code, so it'll be helpful for re asking the model.</p> <pre><code>1 validation error for QuestionAnswer\nanswer\n   Assertion failed, The statement is objectionable. (type=assertion_error)\n</code></pre>"},{"location":"concepts/reask_validation/#using-reasking-logic-to-correct-outputs","title":"Using Reasking Logic to Correct Outputs","text":"<p>Validators are a great tool for ensuring some property of the outputs. When you use the <code>patch()</code> method with the <code>openai</code> client, you can use the <code>max_retries</code> parameter to set the number of times you can reask the model to correct the output.</p> <p>It is a great layer of defense against bad outputs of two forms:</p> <ol> <li>Pydantic Validation Errors (code or llm based)</li> <li>JSON Decoding Errors (when the model returns a bad response)</li> </ol>"},{"location":"concepts/reask_validation/#step-1-define-the-response-model-with-validators","title":"Step 1: Define the Response Model with Validators","text":"<p>Notice that the field validator wants the name in uppercase, but the user input is lowercase. The validator will raise a <code>ValueError</code> if the name is not in uppercase.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.patch(openai.OpenAI())\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n</code></pre>"},{"location":"concepts/reask_validation/#step-2-using-the-client-with-retries","title":"Step 2. Using the Client with Retries","text":"<p>Here, the <code>UserDetails</code> model is passed as the <code>response_model</code>, and <code>max_retries</code> is set to 2.</p> <pre><code>import instructor\nimport openai\nfrom pydantic import BaseModel\n\nclient = instructor.patch(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n\nmodel = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(model.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Jason\",\n  \"age\": 25\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/reask_validation/#what-happens-behind-the-scenes","title":"What happens behind the scenes?","text":"<p>Behind the scenes, the <code>instructor.patch()</code> method adds a <code>max_retries</code> parameter to the <code>openai.ChatCompletion.create()</code> method. The <code>max_retries</code> parameter will trigger up to 2 reattempts if the <code>name</code> attribute fails the uppercase validation in <code>UserDetails</code>.</p> <pre><code>from pydantic import ValidationError\n\n\ntry:\n    ...\nexcept ValidationError as e:\n    kwargs[\"messages\"].append(response.choices[0].message)\n    kwargs[\"messages\"].append(\n        {\n            \"role\": \"user\",\n            \"content\": f\"Please correct the function call; errors encountered:\\n{e}\",\n        }\n    )\n</code></pre>"},{"location":"concepts/reask_validation/#advanced-validation-techniques","title":"Advanced Validation Techniques","text":"<p>The docs are currently incomplete, but we have a few advanced validation techniques that we're working on documenting better such as model level validation, and using a validation context. Check out our example on verifying citations which covers:</p> <ol> <li>Validate the entire object with all attributes rather than one attribute at a time</li> <li>Using some 'context' to validate the object: In this case, we use the <code>context</code> to check if the citation existed in the original text.</li> </ol>"},{"location":"concepts/reask_validation/#takeaways","title":"Takeaways","text":"<p>By integrating these advanced validation techniques, we not only improve the quality and reliability of LLM-generated content, but also pave the way for more autonomous and effective systems.</p>"},{"location":"concepts/retrying/","title":"Retrying","text":"<p>One of the benefits of having Pydantic is the ease with which we can define validators. We cover this topic in many articles, like Reasking Validation and in our blog post Good LLM validation is just good validation.</p> <p>This post will mostly describe how to use simple and more complex retry and logic.</p>"},{"location":"concepts/retrying/#example-of-a-validator","title":"Example of a Validator","text":"<p>Before we begin, we'll use a simple example of a validator. One that checks that the name is in all caps. While we could obviously prompt that we want the name in all caps, this serves as an example of how we can build in additional logic without changing our prompts.</p> <p>To use simple retry, we just need to set `max_retries`` as an integer. In this example.</p> <pre><code>from typing import Annotated\nfrom pydantic import AfterValidator, BaseModel\n\n\ndef uppercase_validator(v):\n    if v.islower():\n        raise ValueError(\"Name must be ALL CAPS\")\n    return v\n\n\nclass UserDetail(BaseModel):\n    name: Annotated[str, AfterValidator(uppercase_validator)]\n    age: int\n\n\ntry:\n    UserDetail(name=\"jason\", age=12)\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must be ALL CAPS [type=value_error, input_value='jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/value_error\n    \"\"\"\n</code></pre>"},{"location":"concepts/retrying/#simple-max-retries","title":"Simple: Max Retries","text":"<p>The simplest way of defining a retry is just defining the maximum number of retries.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.patch(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    max_retries=3,  # (1)!\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n# (2)!\n</code></pre> <ol> <li>We set the maximum number of retries to 3. This means that if the model returns an error, we'll reask the model up to 3 times.</li> <li>We assert that the name is in all caps.</li> </ol>"},{"location":"concepts/retrying/#advanced-retry-logic","title":"Advanced: Retry Logic","text":"<p>If you want more control over how we define retries such as back-offs and additional retry logic we can use a library called Tenacity. To learn more, check out the documentation on the Tenacity website.</p> <p>Rather than using the decorator <code>@retry</code>, we can use the <code>Retrying</code> and <code>AsyncRetrying</code> classes to define our own retry logic.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import Retrying, stop_after_attempt, wait_fixed\n\nclient = instructor.patch(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    max_retries=Retrying(\n        stop=stop_after_attempt(2),  # (1)!\n        wait=wait_fixed(1),  # (2)!\n    ),  # (3)!\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n</code></pre> <ol> <li>We stop after 2 attempts</li> <li>We wait 1 second between each attempt</li> <li>We can now define our own retry logic</li> </ol>"},{"location":"concepts/retrying/#asynchronous-retries","title":"asynchronous retries","text":"<p>If you're using asynchronous code, you can use <code>AsyncRetrying</code> instead.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import AsyncRetrying, stop_after_attempt, wait_fixed\n\nclient = instructor.patch(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\ntask = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    max_retries=AsyncRetrying(\n        stop=stop_after_attempt(2),\n        wait=wait_fixed(1),\n    ),\n)\n\nimport asyncio\n\nresponse = asyncio.run(task)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/retrying/#other-features-of-tenacity","title":"Other Features of Tenacity","text":"<p>Tenacity features a huge number of different retrying capabilities. A few of them are listed below.</p> <ul> <li><code>Retrying(stop=stop_after_attempt(2))</code>: Stop after 2 attempts</li> <li><code>Retrying(stop=stop_after_delay(10))</code>: Stop after 10 seconds</li> <li><code>Retrying(wait=wait_fixed(1))</code>: Wait 1 second between each attempt</li> <li><code>Retrying(wait=wait_random(0, 1))</code>: Wait a random amount of time between 0 and 1 seconds</li> <li><code>Retrying(wait=wait_exponential(multiplier=1, min=4, max=10))</code>: Wait an exponential amount of time between 4 and 10 seconds</li> <li><code>Retrying(wait=(stop_after_attempt(2) | stop_after_delay(10)))</code>: Stop after 2 attempts or 10 seconds</li> <li><code>Retrying(wait=(wait_fixed(1) + wait_random(0.2)))</code>: Wait at least 1 second and add up to 0.2 seconds</li> </ul> <p>Remember that for async clients you need to use <code>AsyncRetrying</code> instead of <code>Retrying</code>!</p>"},{"location":"concepts/typeadapter/","title":"Type Adapter","text":"<p>This page is a work in progress</p> <p>This page is a work in progress. Check out Pydantic's documentation</p>"},{"location":"concepts/types/","title":"Support for Simple Types","text":"<p>Aside from the recommended <code>pydantic.BaseModel</code>, and Iterable, and Partial,</p> <p>Instructor supports simple types like <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, <code>Union</code>, <code>Literal</code>, out of the box. You can use these types directly in your response models.</p> <p>To add more descriptions you can also use <code>typing.Annotated</code> to include more information about the type.</p>"},{"location":"concepts/types/#what-happens-behind-the-scenes","title":"What happens behind the scenes?","text":"<p>We will actually wrap the response model with a <code>pydantic.BaseModel</code> of the following form:</p> <pre><code>from typing import Annotated\nfrom pydantic import create_model, Field, BaseModel\n\ntypehint = Annotated[bool, Field(description=\"Sample Description\")]\n\nmodel = create_model(\"Response\", content=(typehint, ...), __base__=BaseModel)\n\nprint(model.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'content': {\n            'description': 'Sample Description',\n            'title': 'Content',\n            'type': 'boolean',\n        }\n    },\n    'title': 'Response',\n    'type': 'object',\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/types/#primitive-types-str-int-float-bool","title":"Primitive Types (str, int, float, bool)","text":"<pre><code>import instructor\nimport openai\n\nclient = instructor.patch(openai.OpenAI())\n\n# Response model with simple types like str, int, float, bool\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=bool,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Is it true that Paris is the capital of France?\",\n        },\n    ],\n)\nassert resp is True, \"Paris is the capital of France\"\nprint(resp)\n#&gt; True\n</code></pre>"},{"location":"concepts/types/#annotated","title":"Annotated","text":"<p>Annotations can be used to add more information about the type. This can be useful for adding descriptions to the type, along with more complex information like field names, and more.</p> <pre><code>import instructor\nimport openai\nfrom typing import Annotated\nfrom pydantic import Field\n\nclient = instructor.patch(openai.OpenAI())\n\nUpperCaseStr = Annotated[str, Field(description=\"string must be upper case\")]\n\n# Response model with simple types like str, int, float, bool\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UpperCaseStr,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the capital of france?\",\n        },\n    ],\n)\nassert resp == \"PARIS\", \"Paris is the capital of France\"\nprint(resp)\n#&gt; PARIS\n</code></pre>"},{"location":"concepts/types/#literal","title":"Literal","text":"<p>When doing simple classification Literals go quite well, they support literal of string, int, bool.</p> <pre><code>import instructor\nimport openai\nfrom typing import Literal\n\nclient = instructor.patch(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Literal[\"BILLING\", \"SHIPPING\"],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Classify the following messages: 'I am having trouble with my billing'\",\n        },\n    ],\n)\nassert resp == \"BILLING\"\nprint(resp)\n#&gt; BILLING\n</code></pre>"},{"location":"concepts/types/#enum","title":"Enum","text":"<p>Enums are harder to get right without some addition promping but are useful if these are values that are shared across the application.</p> <pre><code>import instructor\nimport openai\nfrom enum import Enum\n\n\nclass Label(str, Enum):\n    BILLING = \"BILLING\"\n    SHIPPING = \"SHIPPING\"\n\n\nclient = instructor.patch(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Label,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Classify the following messages: 'I am having trouble with my billing'\",\n        },\n    ],\n)\nassert resp == Label.BILLING\nprint(resp)\n#&gt; BILLING\n</code></pre>"},{"location":"concepts/types/#list","title":"List","text":"<pre><code>import instructor\nimport openai\nfrom typing import List\n\nclient = instructor.patch(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=List[int],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Give me the first 5 prime numbers\",\n        },\n    ],\n)\n\nassert resp == [2, 3, 5, 7, 11]\nprint(resp)\n#&gt; [2, 3, 5, 7, 11]\n</code></pre>"},{"location":"concepts/types/#union","title":"Union","text":"<p>Union is a great way to handle multiple types of responses, similar to multiple function calls but not limited to the function calling api, like in JSON_SCHEMA modes.</p> <pre><code>import instructor\nimport openai\nfrom pydantic import BaseModel\nfrom typing import Union\n\nclient = instructor.patch(openai.OpenAI())\n\n\nclass Add(BaseModel):\n    a: int\n    b: int\n\n\nclass Weather(BaseModel):\n    location: str\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Union[Add, Weather],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is 5 + 5?\",\n        },\n    ],\n)\n\nassert resp == Add(a=5, b=5)\nprint(resp)\n#&gt; a=5 b=5\n</code></pre>"},{"location":"concepts/types/#complex-types","title":"Complex Types","text":""},{"location":"concepts/types/#pandas-dataframe","title":"Pandas DataFrame","text":"<p>This is a more complex example, where we use a custom type to convert markdown to a pandas DataFrame.</p> <pre><code>from io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema\nimport pandas as pd\nimport instructor\nimport openai\n\n\ndef md_to_df(data: Any) -&gt; Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    # Validates final type\n    InstanceOf[pd.DataFrame],\n    # Converts markdown to DataFrame\n    BeforeValidator(md_to_df),\n    # Converts DataFrame to markdown on model_dump_json\n    PlainSerializer(lambda df: df.to_markdown()),\n    # Adds a description to the type\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"\"\"\n            The markdown representation of the table,\n            each one should be tidy, do not try to join\n            tables that should be seperate\"\"\",\n        }\n    ),\n]\n\n\nclient = instructor.patch(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=MarkdownDataFrame,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Jason is 20, Sarah is 30, and John is 40\",\n        },\n    ],\n)\n\nassert isinstance(resp, pd.DataFrame)\nprint(resp)\n\"\"\"\n        Age\n Name\nJason     20\nSarah     30\nJohn      40\n\"\"\"\n</code></pre>"},{"location":"concepts/types/#lists-of-unions","title":"Lists of Unions","text":"<p>Just like Unions we can use List of Unions to represent multiple types of responses. This will feel similar to the parallel function calls but not limited to the function calling api, like in JSON_SCHEMA modes.</p> <pre><code>import instructor\nimport openai\nfrom pydantic import BaseModel\nfrom typing import Union, List\n\nclient = instructor.patch(openai.OpenAI())\n\n\nclass Weather(BaseModel, frozen=True):\n    location: str\n\n\nclass Add(BaseModel, frozen=True):\n    a: int\n    b: int\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=List[Union[Add, Weather]],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Add 5 and 5, and also whats the weather in Toronto?\",\n        },\n    ],\n)\n\nassert resp == [Add(a=5, b=5), Weather(location=\"Toronto\")]\nprint(resp)\n#&gt; [Add(a=5, b=5), Weather(location='Toronto')]\n</code></pre>"},{"location":"concepts/union/","title":"Union","text":"<p>Pydantic models also support <code>Union</code> types, which are used to represent a value that can be one of several types.</p> <p>While many libraries support multiple function calls, and tool calls support multiple returns, the goal is to provide only one way to do things.</p>"},{"location":"concepts/union/#unions-for-multiple-types","title":"Unions for Multiple Types","text":"<p>You can use <code>Union</code> types to write agents that can dynamically choose actions - by choosing an output class. For example, in a search and lookup function, the LLM can determine whether to execute another search, lookup or other action.</p> <pre><code>from pydantic import BaseModel\nfrom typing import Union\n\n\nclass Search(BaseModel):\n    query: str\n\n    def execute(self):\n        return ...\n\n\nclass Lookup(BaseModel):\n    key: str\n\n    def execute(self):\n        return ...\n\n\nclass Action(BaseModel):\n    action: Union[Search, Lookup]\n\n    def execute(self):\n        return self.action.execute()\n</code></pre> <p>See 'examples/union/run.py' for a working example.</p>"},{"location":"concepts/usage/","title":"Usage Tokens","text":"<p>The easiest way to get usage for non streaming requests is to access the raw response.</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI())\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(user._raw_response.usage)\n#&gt; CompletionUsage(completion_tokens=9, prompt_tokens=82, total_tokens=91)\n</code></pre>"},{"location":"examples/","title":"Function Calls by Example","text":""},{"location":"examples/#quick-links","title":"Quick Links","text":"<ol> <li>How are single and multi-label classifications done using enums?</li> <li>How is AI self-assessment implemented with <code>llm_validator</code>?</li> <li>How to do classification in batch from user provided classes.</li> <li>How are exact citations retrieved using regular expressions and smart prompting?</li> <li>How are search queries segmented through function calling and multi-task definitions?</li> <li>How are knowledge graphs generated from questions?</li> <li>How are complex queries decomposed into subqueries in a single request?</li> <li>How are entities extracted and resolved from documents?</li> <li>How is Personally Identifiable Information sanitized from documents?</li> <li>How are action items and dependencies generated from transcripts?</li> <li>How to enable OpenAI's moderation</li> <li>How to extract tables using GPT-Vision?</li> <li>How to generate advertising copy from image inputs</li> <li>How to use local models from Ollama</li> <li>How to store responses in a database with SQLModel</li> </ol> <p>Explore more!</p>"},{"location":"examples/batch_classification/","title":"Bulk Classification from User-Provided Tags.","text":"<p>This tutorial shows how to do classification from user provided tags. This is valuable when you want to provide services that allow users to do some kind of classification.</p> <p>Motivation</p> <p>Imagine allowing the user to upload documents as part of a RAG application. Oftentimes, we might want to allow the user to specify an existing set of tags, give descriptions, and do the classification for them.</p>"},{"location":"examples/batch_classification/#defining-the-structures","title":"Defining the Structures","text":"<p>One of the easy things to do is to allow users to define a set of tags in some kind of schema and save that in a database. Here's an example of a schema that we might use:</p> tag_id name instructions 0 personal Personal information 1 phone Phone number 2 email Email address 3 address Address 4 Other Other information <ol> <li>tag_id \u2014 The unique identifier for the tag.</li> <li>name \u2014 The name of the tag.</li> <li>instructions \u2014 A description of the tag, which can be used as a prompt to describe the tag.</li> </ol>"},{"location":"examples/batch_classification/#implementing-the-classification","title":"Implementing the Classification","text":"<p>In order to do this we'll do a couple of things:</p> <ol> <li>We'll use the <code>instructor</code> library to patch the <code>openai</code> library to use the <code>AsyncOpenAI</code> client.</li> <li>Implement a <code>Tag</code> model that will be used to validate the tags from the context. (This will allow us to avoid hallucinating tags that are not in the context.)</li> <li>Helper models for the request and response.</li> <li>An async function to do the classification.</li> <li>A main function to run the classification using the <code>asyncio.gather</code> function to run the classification in parallel.</li> </ol> <p>If you want to learn more about how to do bad computations, check out our post on AsyncIO here.</p> <pre><code>import openai\nimport instructor\n\nclient = instructor.patch(\n    openai.AsyncOpenAI(),\n)\n</code></pre> <p>First, we'll need to import all of our Pydantic and instructor code and use the AsyncOpenAI client. Then, we'll define the tag model along with the tag instructions to provide input and output.</p> <p>This is very helpful because once we use something like FastAPI to create endpoints, the Pydantic functions will serve as multiple tools:</p> <ol> <li>A description for the developer</li> <li>Type hints for the IDE</li> <li>OpenAPI documentation for the FastAPI endpoint</li> <li>Schema and Response Model for the language model.</li> </ol> <pre><code>from typing import List\nfrom pydantic import BaseModel, ValidationInfo, model_validator\n\nclass Tag(BaseModel):\n    id: int\n    name: str\n\n    @model_validator(mode=\"after\")\n    def validate_ids(self, info: ValidationInfo):\n        context = info.context\n        if context:\n            tags: List[Tag] = context.get(\"tags\")\n            assert self.id in {\n                tag.id for tag in tags\n            }, f\"Tag ID {self.id} not found in context\"\n            assert self.name in {\n                tag.name for tag in tags\n            }, f\"Tag name {self.name} not found in context\"\n        return self\n\n\nclass TagWithInstructions(Tag):\n    instructions: str\n\n\nclass TagRequest(BaseModel):\n    texts: List[str]\n    tags: List[TagWithInstructions]\n\n\nclass TagResponse(BaseModel):\n    texts: List[str]\n    predictions: List[Tag]\n</code></pre> <p>Let's delve deeper into what the <code>validate_ids</code> function does. Notice that its purpose is to extract tags from the context and ensure that each ID and name exists in the set of tags. This approach helps minimize hallucinations. If we mistakenly identify either the ID or the tag, an error will be thrown, and the instructor will prompt the language model to retry until the correct item is successfully extracted.</p> <pre><code>@model_validator(mode=\"after\")\ndef validate_ids(self, info: ValidationInfo):\n    context = info.context\n    if context:\n        tags: List[Tag] = context.get(\"tags\")\n        assert self.id in {\n            tag.id for tag in tags\n        }, f\"Tag ID {self.id} not found in context\"\n        assert self.name in {\n            tag.name for tag in tags\n        }, f\"Tag name {self.name} not found in context\"\n    return self\n</code></pre> <p>Now, let's implement the function to do the classification. This function will take a single text and a list of tags and return the predicted tag.</p> <pre><code>async def tag_single_request(text: str, tags: List[Tag]) -&gt; Tag:\n    allowed_tags = [(tag.id, tag.name) for tag in tags]\n    allowed_tags_str = \", \".join([f\"`{tag}`\" for tag in allowed_tags])\n\n    return await client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world-class text tagging system.\",\n            },\n            {\"role\": \"user\", \"content\": f\"Describe the following text: `{text}`\"},\n            {\n                \"role\": \"user\",\n                \"content\": f\"Here are the allowed tags: {allowed_tags_str}\",\n            },\n        ],\n        response_model=Tag,  # Minimizes the hallucination of tags that are not in the allowed tags.\n        validation_context={\"tags\": tags},\n    )\n\n\nasync def tag_request(request: TagRequest) -&gt; TagResponse:\n    predictions = await asyncio.gather(\n        *[tag_single_request(text, request.tags) for text in request.texts]\n    )\n    return TagResponse(\n        texts=request.texts,\n        predictions=predictions,\n    )\n</code></pre> <p>Notice that we first define a single async function that makes a prediction of a tag, and we pass it into the validation context in order to minimize hallucinations.</p> <p>Finally, we'll implement the main function to run the classification using the <code>asyncio.gather</code> function to run the classification in parallel.</p> <pre><code>tags = [\n    TagWithInstructions(id=0, name=\"personal\", instructions=\"Personal information\"),\n    TagWithInstructions(id=1, name=\"phone\", instructions=\"Phone number\"),\n    TagWithInstructions(id=2, name=\"email\", instructions=\"Email address\"),\n    TagWithInstructions(id=3, name=\"address\", instructions=\"Address\"),\n    TagWithInstructions(id=4, name=\"Other\", instructions=\"Other information\"),\n]\n\n# Texts will be a range of different questions.\n# Such as \"How much does it cost?\", \"What is your privacy policy?\", etc.\ntexts = [\n    \"What is your phone number?\",\n    \"What is your email address?\",\n    \"What is your address?\",\n    \"What is your privacy policy?\",\n]\n\n# The request will contain the texts and the tags.\nrequest = TagRequest(texts=texts, tags=tags)\n\n# The response will contain the texts, the predicted tags, and the confidence.\nresponse = asyncio.run(tag_request(request))\nprint(response.model_dump_json(indent=2))\n</code></pre> <p>Which would result in:</p> <pre><code>{\n  \"texts\": [\n    \"What is your phone number?\",\n    \"What is your email address?\",\n    \"What is your address?\",\n    \"What is your privacy policy?\"\n  ],\n  \"predictions\": [\n    {\n      \"id\": 1,\n      \"name\": \"phone\"\n    },\n    {\n      \"id\": 2,\n      \"name\": \"email\"\n    },\n    {\n      \"id\": 3,\n      \"name\": \"address\"\n    },\n    {\n      \"id\": 4,\n      \"name\": \"Other\"\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/batch_classification/#what-happens-in-production","title":"What happens in production?","text":"<p>If we were to use this in production, we might expect to have some kind of fast API endpoint.</p> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.post(\"/tag\", response_model=TagResponse)\nasync def tag(request: TagRequest) -&gt; TagResponse:\n    return await tag_request(request)\n</code></pre> <p>Since everything is already annotated with Pydantic, this code is very simple to write!</p> <p>Where do tags come from?</p> <p>I just want to call out that here you can also imagine the tag spec IDs and names and instructions for example could come from a database or somewhere else. I'll leave this as an exercise to the reader, but I hope this gives us a clear understanding of how we can do something like user-defined classification.</p>"},{"location":"examples/batch_classification/#improving-the-model","title":"Improving the Model","text":"<p>There's a couple things we could do to make this system a little bit more robust.</p> <ol> <li>Use confidence score:</li> </ol> <pre><code>class TagWithConfidence(Tag):\n    confidence: float = Field(\n        ...,\n        ge=0,\n        le=1,\n        description=\"The confidence of the prediction, 0 is low, 1 is high\",\n    )\n</code></pre> <ol> <li>Use multiclass classification:</li> </ol> <p>Notice in the example we use Iterable[Tag] vs Tag. This is because we might want to use a multiclass classification model that returns multiple tag!</p> <pre><code>await client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world-class text tagging system.\",\n        },\n        {\"role\": \"user\", \"content\": f\"Describe the following text: `{text}`\"},\n        {\n            \"role\": \"user\",\n            \"content\": f\"Here are the allowed tags: {allowed_tags_str}\",\n        },\n    ],\n    response_model=Iterable[Tag],\n    validation_context={\"tags\": tags},\n)\n</code></pre>"},{"location":"examples/classification/","title":"Example: Text Classification using OpenAI and Pydantic","text":"<p>This tutorial showcases how to implement text classification tasks\u2014specifically, single-label and multi-label classifications\u2014using the OpenAI API, Python's <code>enum</code> module, and Pydantic models.</p> <p>Motivation</p> <p>Text classification is a common problem in many NLP applications, such as spam detection or support ticket categorization. The goal is to provide a systematic way to handle these cases using OpenAI's GPT models in combination with Python data structures.</p>"},{"location":"examples/classification/#single-label-classification","title":"Single-Label Classification","text":""},{"location":"examples/classification/#defining-the-structures","title":"Defining the Structures","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a Pydantic model for the output.</p> <pre><code>import enum\nfrom pydantic import BaseModel\n\n\nclass Labels(str, enum.Enum):\n    \"\"\"Enumeration for single-label text classification.\"\"\"\n\n    SPAM = \"spam\"\n    NOT_SPAM = \"not_spam\"\n\n\nclass SinglePrediction(BaseModel):\n    \"\"\"\n    Class for a single class label prediction.\n    \"\"\"\n\n    class_label: Labels\n</code></pre>"},{"location":"examples/classification/#classifying-text","title":"Classifying Text","text":"<p>The function <code>classify</code> will perform the single-label classification.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\n\ndef classify(data: str) -&gt; SinglePrediction:\n    \"\"\"Perform single-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=SinglePrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following text: {data}\",\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code># Test single-label classification\nprediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\")\nassert prediction.class_label == Labels.SPAM\n</code></pre>"},{"location":"examples/classification/#multi-label-classification","title":"Multi-Label Classification","text":""},{"location":"examples/classification/#defining-the-structures_1","title":"Defining the Structures","text":"<p>For multi-label classification, we introduce a new enum class and a different Pydantic model to handle multiple labels.</p> <pre><code>from typing import List\nimport enum\n\n# Define Enum class for multiple labels\nclass MultiLabels(str, enum.Enum):\n    TECH_ISSUE = \"tech_issue\"\n    BILLING = \"billing\"\n    GENERAL_QUERY = \"general_query\"\n\n\n# Define the multi-class prediction model\nclass MultiClassPrediction(BaseModel):\n    \"\"\"\n    Class for a multi-class label prediction.\n    \"\"\"\n\n    class_labels: List[MultiLabels]\n</code></pre>"},{"location":"examples/classification/#classifying-text_1","title":"Classifying Text","text":"<p>The function <code>multi_classify</code> is responsible for multi-label classification.</p> <pre><code>def multi_classify(data: str) -&gt; MultiClassPrediction:\n    \"\"\"Perform multi-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=MultiClassPrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following support ticket: {data}\",\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/classification/#testing-and-evaluation_1","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code># Test multi-label classification\nticket = \"My account is locked and I can't access my billing info.\"\nprediction = multi_classify(ticket)\nassert MultiLabels.TECH_ISSUE in prediction.class_labels\nassert MultiLabels.BILLING in prediction.class_labels\n</code></pre>"},{"location":"examples/entity_resolution/","title":"Entity Resolution and Visualization for Legal Documents","text":"<p>In this guide, we demonstrate how to extract and resolve entities from a sample legal contract. Then, we visualize these entities and their dependencies as an entity graph. This approach can be invaluable for legal tech applications, aiding in the understanding of complex documents.</p> <p>Motivation</p> <p>Legal contracts are full of intricate details and interconnected clauses. Automatically extracting and visualizing these elements can make it easier to understand the document's overall structure and terms.</p>"},{"location":"examples/entity_resolution/#defining-the-data-structures","title":"Defining the Data Structures","text":"<p>The <code>Entity</code> and <code>Property</code> classes model extracted entities and their attributes. <code>DocumentExtraction</code> encapsulates a list of these entities.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n    resolved_absolute_value: str\n\n\nclass Entity(BaseModel):\n    id: int = Field(\n        ...,\n        description=\"Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities\",\n    )\n    subquote_string: List[str] = Field(\n        ...,\n        description=\"Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution\",\n    )\n    entity_title: str\n    properties: List[Property] = Field(\n        ..., description=\"List of properties of the entity\"\n    )\n    dependencies: List[int] = Field(\n        ...,\n        description=\"List of entity ids that this entity depends  or relies on to resolve it\",\n    )\n\n\nclass DocumentExtraction(BaseModel):\n    entities: List[Entity] = Field(\n        ...,\n        description=\"Body of the answer, each fact should be a separate object with a body and a list of sources\",\n    )\n</code></pre>"},{"location":"examples/entity_resolution/#entity-extraction-and-resolution","title":"Entity Extraction and Resolution","text":"<p>The <code>ask_ai</code> function utilizes OpenAI's API to extract and resolve entities from the input content.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\n\ndef ask_ai(content) -&gt; DocumentExtraction:\n    return client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=DocumentExtraction,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Extract and resolve a list of entities from the following document:\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": content,\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/entity_resolution/#graph-visualization","title":"Graph Visualization","text":"<p><code>generate_graph</code> takes the extracted entities and visualizes them using Graphviz. It creates nodes for each entity and edges for their dependencies.</p> <pre><code>from graphviz import Digraph\n\n\ndef generate_html_label(entity: Entity) -&gt; str:\n    rows = [\n        f\"&lt;tr&gt;&lt;td&gt;{prop.key}&lt;/td&gt;&lt;td&gt;{prop.resolved_absolute_value}&lt;/td&gt;&lt;/tr&gt;\"\n        for prop in entity.properties\n    ]\n    table_rows = \"\".join(rows)\n    return f\"&lt;&lt;table border='0' cellborder='1' cellspacing='0'&gt;&lt;tr&gt;&lt;td colspan='2'&gt;&lt;b&gt;{entity.entity_title}&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;{table_rows}&lt;/table&gt;&gt;\"\n\n\ndef generate_graph(data: DocumentExtraction):\n    dot = Digraph(comment=\"Entity Graph\", node_attr={\"shape\": \"plaintext\"})\n\n    for entity in data.entities:\n        label = generate_html_label(entity)\n        dot.node(str(entity.id), label)\n\n    for entity in data.entities:\n        for dep_id in entity.dependencies:\n            dot.edge(str(entity.id), str(dep_id))\n\n    dot.render(\"entity.gv\", view=True)\n</code></pre>"},{"location":"examples/entity_resolution/#execution","title":"Execution","text":"<p>Finally, execute the code to visualize the entity graph for the sample legal contract.</p> <pre><code>content = \"\"\"\nSample Legal Contract\nAgreement Contract\n\nThis Agreement is made and entered into on 2020-01-01 by and between Company A (\"the Client\") and Company B (\"the Service Provider\").\n\nArticle 1: Scope of Work\n\nThe Service Provider will deliver the software product to the Client 30 days after the agreement date.\n\nArticle 2: Payment Terms\n\nThe total payment for the service is $50,000.\nAn initial payment of $10,000 will be made within 7 days of the the signed date.\nThe final payment will be due 45 days after [SignDate].\n\nArticle 3: Confidentiality\n\nThe parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date.\n\nArticle 4: Termination\n\nThe contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the [DeliveryDate].\n\"\"\"  # Your legal contract here\nmodel = ask_ai(content)\ngenerate_graph(model)\n</code></pre> <p>This will produce a graphical representation of the entities and their dependencies, stored as \"entity.gv\".</p> <p></p>"},{"location":"examples/exact_citations/","title":"Example: Answering Questions with Validated Citations","text":"<p>For the full code example check out examples/citation_fuzzy_match.py</p>"},{"location":"examples/exact_citations/#overview","title":"Overview","text":"<p>This example shows how to use Instructor with validators to not only add citations to answers generated but also prevent hallucinations by ensuring that every statement made by the LLM is backed up by a direct quote from the context provided, and that those quotes exist!.Two Python classes, <code>Fact</code> and <code>QuestionAnswer</code>, are defined to encapsulate the information of individual facts and the entire answer, respectively.</p>"},{"location":"examples/exact_citations/#data-structures","title":"Data Structures","text":""},{"location":"examples/exact_citations/#the-fact-class","title":"The <code>Fact</code> Class","text":"<p>The <code>Fact</code> class encapsulates a single statement or fact. It contains two fields:</p> <ul> <li><code>fact</code>: A string representing the body of the fact or statement.</li> <li><code>substring_quote</code>: A list of strings. Each string is a direct quote from the context that supports the <code>fact</code>.</li> </ul>"},{"location":"examples/exact_citations/#validation-method-validate_sources","title":"Validation Method: <code>validate_sources</code>","text":"<p>This method validates the sources (<code>substring_quote</code>) in the context. It utilizes regex to find the span of each substring quote in the given context. If the span is not found, the quote is removed from the list.</p> <pre><code>from pydantic import Field, BaseModel, model_validator, FieldValidationInfo\nfrom typing import List\n\n\nclass Fact(BaseModel):\n    fact: str = Field(...)\n    substring_quote: List[str] = Field(...)\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self, info: FieldValidationInfo) -&gt; \"Fact\":\n        text_chunks = info.context.get(\"text_chunk\", None)\n        spans = list(self.get_spans(text_chunks))\n        self.substring_quote = [text_chunks[span[0] : span[1]] for span in spans]\n        return self\n\n    def get_spans(self, context):\n        for quote in self.substring_quote:\n            yield from self._get_span(quote, context)\n\n    def _get_span(self, quote, context):\n        for match in re.finditer(re.escape(quote), context):\n            yield match.span()\n</code></pre>"},{"location":"examples/exact_citations/#the-questionanswer-class","title":"The <code>QuestionAnswer</code> Class","text":"<p>This class encapsulates the question and its corresponding answer. It contains two fields:</p> <ul> <li><code>question</code>: The question asked.</li> <li><code>answer</code>: A list of <code>Fact</code> objects that make up the answer.</li> </ul>"},{"location":"examples/exact_citations/#validation-method-validate_sources_1","title":"Validation Method: <code>validate_sources</code>","text":"<p>This method checks that each <code>Fact</code> object in the <code>answer</code> list has at least one valid source. If a <code>Fact</code> object has no valid sources, it is removed from the <code>answer</code> list.</p> <pre><code>class QuestionAnswer(BaseModel):\n    question: str = Field(...)\n    answer: List[Fact] = Field(...)\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self) -&gt; \"QuestionAnswer\":\n        self.answer = [fact for fact in self.answer if len(fact.substring_quote) &gt; 0]\n        return self\n</code></pre>"},{"location":"examples/exact_citations/#function-to-ask-ai-a-question","title":"Function to Ask AI a Question","text":""},{"location":"examples/exact_citations/#the-ask_ai-function","title":"The <code>ask_ai</code> Function","text":"<p>This function takes a string <code>question</code> and a string <code>context</code> and returns a <code>QuestionAnswer</code> object. It uses the OpenAI API to fetch the answer and then validates the sources using the defined classes.</p> <p>To understand the validation context work from pydantic check out pydantic's docs</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model, validation_context keyword\nclient = instructor.patch(OpenAI())\n\n\ndef ask_ai(question: str, context: str) -&gt; QuestionAnswer:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        temperature=0,\n        response_model=QuestionAnswer,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class algorithm to answer questions with correct and exact citations.\",\n            },\n            {\"role\": \"user\", \"content\": f\"{context}\"},\n            {\"role\": \"user\", \"content\": f\"Question: {question}\"},\n        ],\n        validation_context={\"text_chunk\": context},\n    )\n</code></pre>"},{"location":"examples/exact_citations/#example","title":"Example","text":"<p>dd Here's an example of using these classes and functions to ask a question and validate the answer.</p> <pre><code>question = \"What did the author do during college?\"\ncontext = \"\"\"\nMy name is Jason Liu, and I grew up in Toronto Canada but I was born in China.\nI went to an arts high school but in university I studied Computational Mathematics and physics.\nAs part of coop I worked at many companies including Stitchfix, Facebook.\nI also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\n\"\"\"\n</code></pre> <p>The output would be a <code>QuestionAnswer</code> object containing validated facts and their sources.</p> <pre><code>{\n    \"question\": \"where did he go to school?\",\n    \"answer\": [\n        {\n            \"statement\": \"Jason Liu went to an arts highschool.\",\n            \"substring_phrase\": [\"arts highschool\"],\n        },\n        {\n            \"statement\": \"Jason Liu studied Computational Mathematics and physics in university.\",\n            \"substring_phrase\": [\"university\"],\n        },\n    ],\n}\n</code></pre> <p>This ensures that every piece of information in the answer has been validated against the context.</p>"},{"location":"examples/extract_slides/","title":"Data extraction from slides","text":"<p>In this guide, we demonstrate how to extract data from slides.</p> <p>Motivation</p> <p>When we want to translate key information from slides into structured data, simply isolating the text and running extraction might not be enough. Sometimes the important data is in the images on the slides, so we should consider including them in our extraction pipeline.</p>"},{"location":"examples/extract_slides/#defining-the-necessary-data-structures","title":"Defining the necessary Data Structures","text":"<p>Let's say we want to extract the competitors from various presentations and categorize them according to their respective industries.</p> <p>Our data model will have <code>Industry</code> which will be a list of <code>Competitor</code>'s for a specific industry, and <code>Competition</code> which will aggregate the competitors for all the industries.</p> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\n\nclass Competitor(BaseModel):\n    name: str\n    features: Optional[List[str]]\n\n\n# Define models\nclass Industry(BaseModel):\n    \"\"\"\n    Represents competitors from a specific industry extracted from an image using AI.\n    \"\"\"\n\n    name: str = Field(\n        description=\"The name of the industry\"\n    )\n    competitor_list: List[Competitor] = Field(\n        description=\"A list of competitors for this industry\"\n    )\n\nclass Competition(BaseModel):\n    \"\"\"\n    This class serves as a structured representation of \n    competitors and their qualities.\n    \"\"\"\n\n    industry_list: List[IndustryCompetition] = Field(\n        description=\"A list of industries and their competitors\"\n    )\n</code></pre>"},{"location":"examples/extract_slides/#competitors-extraction","title":"Competitors extraction","text":"<p>To extract competitors from slides we will define a function which will read images from urls and extract the relevant information from them.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(\n    OpenAI(), mode=instructor.Mode.MD_JSON\n)\n\n# Define functions\ndef read_images(image_urls: List[str]) -&gt; Competition:\n    \"\"\"\n    Given a list of image URLs, identify the competitors in the images.\n    \"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        response_model=Competition,\n        max_tokens=2048,\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Identify competitors and generate key features for each competitor.\",\n                    },\n                    *[\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\n                        for url in image_urls\n                    ],\n                ],\n            }\n        ],\n    )\n</code></pre>"},{"location":"examples/extract_slides/#execution","title":"Execution","text":"<p>Finally, we will run the previous function with a few sample slides to see the data extractor in action.</p> <p>As we can see, our model extracted the relevant information for each competitor regardless of how this information was formatted in the original presentations.</p> <p><pre><code>url = [\n    'https://miro.medium.com/v2/resize:fit:1276/0*h1Rsv-fZWzQUyOkt', \n    'https://earlygame.vc/wp-content/uploads/2020/06/startup-pitch-deck-5.jpg'\n    ]\nmodel = read_images(url)\nprint(model.model_json_dump(indent=2))\n</code></pre>     industry_list=[</p> <pre><code>Industry(name='Accommodation and Hospitality', competitor_list=[Competitor(name='CouchSurfing', features=['Affordable', 'Online Transaction']), Competitor(name='Craigslist', features=['Affordable', 'Offline Transaction']), Competitor(name='BedandBreakfast.com', features=['Affordable', 'Offline Transaction']), Competitor(name='AirBed&amp;Breakfast', features=['Affordable', 'Online Transaction']), Competitor(name='Hostels.com', features=['Affordable', 'Online Transaction']), Competitor(name='VRBO', features=['Expensive', 'Offline Transaction']), Competitor(name='Rentahome', features=['Expensive', 'Online Transaction']), Competitor(name='Orbitz', features=['Expensive', 'Online Transaction']), Competitor(name='Hotels.com', features=['Expensive', 'Online Transaction'])]),\n\nIndustry(name='Wine E-commerce', competitor_list=[Competitor(name='WineSimple', features=['Ecommerce Retailers', 'True Personalized Selections', 'Brand Name Wine', 'No Inventory Cost', 'Target Mass Market']), Competitor(name='NakedWines', features=['Ecommerce Retailers', 'Target Mass Market']), Competitor(name='Club W', features=['Ecommerce Retailers', 'Brand Name Wine', 'Target Mass Market']), Competitor(name='Tasting Room', features=['Ecommerce Retailers', 'True Personalized Selections', 'Brand Name Wine']), Competitor(name='Drync', features=['Ecommerce Retailers', 'True Personalized Selections', 'No Inventory Cost']), Competitor(name='Hello Vino', features=['Ecommerce Retailers', 'Brand Name Wine', 'Target Mass Market'])])\n\n]\n</code></pre> <p><code></code></p>"},{"location":"examples/extracting_tables/","title":"Extracting Tables using GPT-Vision","text":"<p>This post demonstrates how to use Python's type annotations and OpenAI's new vision model to extract tables from images and convert them into markdown format. This method is particularly useful for data analysis and automation tasks.</p> <p>The full code is available on GitHub</p>"},{"location":"examples/extracting_tables/#building-the-custom-type-for-markdown-tables","title":"Building the Custom Type for Markdown Tables","text":"<p>First, we define a custom type, <code>MarkdownDataFrame</code>, to handle pandas DataFrames formatted in markdown. This type uses Python's <code>Annotated</code> and <code>InstanceOf</code> types, along with decorators <code>BeforeValidator</code> and <code>PlainSerializer</code>, to process and serialize the data.</p> <pre><code>from io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import BaseModel, Field, BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema\nfrom typing import Iterable\nimport pandas as pd\n\n\ndef md_to_df(data: Any) -&gt; Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    InstanceOf[pd.DataFrame],\n    BeforeValidator(md_to_df),\n    PlainSerializer(lambda df: df.to_markdown()),\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"The markdown representation of the table, each one should be tidy, do not try to join tables that should be seperate\",\n        }\n    ),\n]\n</code></pre>"},{"location":"examples/extracting_tables/#defining-the-table-class","title":"Defining the Table Class","text":"<p>The <code>Table</code> class is essential for organizing the extracted data. It includes a caption and a dataframe, processed as a markdown table. Since most of the complexity is handled by the <code>MarkdownDataFrame</code> type, the <code>Table</code> class is straightforward!</p> <pre><code>class Table(BaseModel):\n    caption: str\n    dataframe: MarkdownDataFrame\n</code></pre>"},{"location":"examples/extracting_tables/#extracting-tables-from-images","title":"Extracting Tables from Images","text":"<p>The <code>extract_table</code> function uses OpenAI's vision model to process an image URL and extract tables in markdown format. We utilize the <code>instructor</code> library to patch the OpenAI client for this purpose.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client to support response_model\n# Also use MD_JSON mode since the visino model does not support any special structured output mode\nclient = instructor.patch(OpenAI(), mode=instructor.function_calls.Mode.MD_JSON)\n\n\ndef extract_table(url: str) -&gt; Iterable[Table]:\n    return client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        response_model=Iterable[Table],\n        max_tokens=1800,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"Extract table from image.\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": url}},\n                ],\n            }\n        ],\n    )\n</code></pre>"},{"location":"examples/extracting_tables/#practical-example","title":"Practical Example","text":"<p>In this example, we apply the method to extract data from an image showing the top grossing apps in Ireland for October 2023.</p> <pre><code>url = \"https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png\"\ntables = extract_table(url)\nfor table in tables:\n    print(table.caption, end=\"\\n\")\n    print(table.dataframe)\n</code></pre> Expand to see the output <p></p>"},{"location":"examples/extracting_tables/#top-10-grossing-apps-in-october-2023-ireland-for-android-platforms","title":"Top 10 Grossing Apps in October 2023 (Ireland) for Android Platforms","text":"Rank App Name Category 1 Google One Productivity 2 Disney+ Entertainment 3 TikTok - Videos, Music &amp; LIVE Entertainment 4 Candy Crush Saga Games 5 Tinder: Dating, Chat &amp; Friends Social networking 6 Coin Master Games 7 Roblox Games 8 Bumble - Dating &amp; Make Friends Dating 9 Royal Match Games 10 Spotify: Music and Podcasts Music &amp; Audio"},{"location":"examples/extracting_tables/#top-10-grossing-apps-in-october-2023-ireland-for-ios-platforms","title":"Top 10 Grossing Apps in October 2023 (Ireland) for iOS Platforms","text":"Rank App Name Category 1 Tinder: Dating, Chat &amp; Friends Social networking 2 Disney+ Entertainment 3 YouTube: Watch, Listen, Stream Entertainment 4 Audible: Audio Entertainment Entertainment 5 Candy Crush Saga Games 6 TikTok - Videos, Music &amp; LIVE Entertainment 7 Bumble - Dating &amp; Make Friends Dating 8 Roblox Games 9 LinkedIn: Job Search &amp; News Business 10 Duolingo - Language Lessons Education"},{"location":"examples/image_to_ad_copy/","title":"Use Vision API to detect products and generate advertising copy","text":"<p>This post demonstrates how to use GPT-4 Vision API and the Chat API to automatically generate advertising copy from product images. This method can be useful for marketing and advertising teams, as well as for e-commerce platforms.</p> <p>The full code is available on GitHub.</p>"},{"location":"examples/image_to_ad_copy/#building-the-models","title":"Building the models","text":""},{"location":"examples/image_to_ad_copy/#product","title":"Product","text":"<p>For the <code>Product</code> model, we define a class that represents a product extracted from an image and store the name, key features, and description. The product attributes are dynamically determined based on the content of the image.</p> <p>Note that it is easy to add Validators and other Pydantic features to the model to ensure that the data is valid and consistent.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass Product(BaseModel):\n    \"\"\"\n    Represents a product extracted from an image using AI.\n\n    The product attributes are dynamically determined based on the content\n    of the image and the AI's interpretation. This class serves as a structured\n    representation of the identified product characteristics.\n    \"\"\"\n\n    name: str = Field(\n        description=\"A generic name for the product.\", example=\"Headphones\"\n    )\n    key_features: Optional[List[str]] = Field(\n        description=\"A list of key features of the product that stand out.\",\n        default=None,\n    )\n\n    description: Optional[str] = Field(\n        description=\"A description of the product.\",\n        default=None,\n    )\n\n    # Can be customized and automatically generated\n    def generate_prompt(self):\n        prompt = f\"Product: {self.name}\\n\"\n        if self.description:\n            prompt += f\"Description: {self.description}\\n\"\n        if self.key_features:\n            prompt += f\"Key Features: {', '.join(self.key_features)}\\n\"\n        return prompt\n</code></pre>"},{"location":"examples/image_to_ad_copy/#identified-product","title":"Identified Product","text":"<p>We also define a class that represents a list of products identified in the images. We also add an error flag and message to indicate if there was an error in the processing of the image.</p> <pre><code>class IdentifiedProduct(BaseModel):\n    \"\"\"\n    Represents a list of products identified in the images.\n    \"\"\"\n\n    products: Optional[List[Product]] = Field(\n        description=\"A list of products identified by the AI.\",\n        example=[\n            Product(\n                name=\"Headphones\",\n                description=\"Wireless headphones with noise cancellation.\",\n                key_features=[\"Wireless\", \"Noise Cancellation\"],\n            )\n        ],\n        default=None,\n    )\n\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"examples/image_to_ad_copy/#advertising-copy","title":"Advertising Copy","text":"<p>Finally, the <code>AdCopy</code> models stores the output in a structured format with a headline and the text.</p> <pre><code>class AdCopy(BaseModel):\n    \"\"\"\n    Represents a generated ad copy.\n    \"\"\"\n\n    headline: str = Field(\n        description=\"A short, catchy, and memorable headline for the given product. The headline should invoke curiosity and interest in the product.\",\n    )\n    ad_copy: str = Field(\n        description=\"A long-form advertisement copy for the given product. This will be used in campaigns to promote the product with a persuasive message and a call-to-action with the objective of driving sales.\",\n    )\n    name: str = Field(description=\"The name of the product being advertised.\")\n</code></pre>"},{"location":"examples/image_to_ad_copy/#calling-the-api","title":"Calling the API","text":""},{"location":"examples/image_to_ad_copy/#product-detection","title":"Product Detection","text":"<p>The <code>read_images</code> function uses OpenAI's vision model to process a list of image URLs and identify products in each of them. We utilize the <code>instructor</code> library to patch the OpenAI client for this purpose.</p> <pre><code>def read_images(image_urls: List[str]) -&gt; IdentifiedProduct:\n    \"\"\"\n    Given a list of image URLs, identify the products in the images.\n    \"\"\"\n\n    logger.info(f\"Identifying products in images... {len(image_urls)} images\")\n\n    return client_image.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        response_model=IdentifiedProduct,\n        max_tokens=1024,  # can be changed\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Identify products using the given images and generate key features for each product.\",\n                    },\n                    *[\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\n                        for url in image_urls\n                    ],\n                ],\n            }\n        ],\n    )\n</code></pre> <p>This gives us a list of products identified in all the images.</p>"},{"location":"examples/image_to_ad_copy/#generate-advertising-copy","title":"Generate advertising copy","text":"<p>Then, we can use the <code>generate_ad_copy</code> function to generate advertising copy for each of the products identified in the images.</p> <p>Two clients are defined for the two different models. This is because the <code>gpt-4-vision-preview</code> model is not compatible with the <code>gpt-4-1106-preview</code> model in terms of their response format.</p> <pre><code>def generate_ad_copy(product: Product) -&gt; AdCopy:\n    \"\"\"\n    Given a product, generate an ad copy for the product.\n    \"\"\"\n\n    logger.info(f\"Generating ad copy for product: {product.name}\")\n\n    return client_copy.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        response_model=AdCopy,\n        temperature=0.3,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert marketing assistant for all products. Your task is to generate an advertisement copy for a product using the name, description, and key features.\",\n            },\n            {\"role\": \"user\", \"content\": product.generate_prompt()},\n        ],\n    )\n</code></pre>"},{"location":"examples/image_to_ad_copy/#putting-it-all-together","title":"Putting it all together","text":"<p>Finally, we can put it all together in a single function that takes a list of image URLs and generates advertising copy for the products identified in the images. Please refer to the full code for the complete implementation.</p>"},{"location":"examples/image_to_ad_copy/#input-file","title":"Input file","text":"<p>The input file is currently a list of image URLs, but this trivial to change to any required format.</p> <pre><code>https://contents.mediadecathlon.com/p1279823/9a1c59ad97a4084a346c014740ae4d3ff860ea70b485ee65f34017ff5e9ae5f7/recreational-ice-skates-fit-50-black.jpg?format=auto\nhttps://contents.mediadecathlon.com/p1279822/a730505231dbd6747c14ee93e8f89e824d3fa2a5b885ec26de8d7feb5626638a/recreational-ice-skates-fit-50-black.jpg?format=auto\nhttps://contents.mediadecathlon.com/p2329893/1ed75517602a5e00245b89ab6a1c6be6d8968a5a227c932b10599f857f3ed4cd/mens-hiking-leather-boots-sh-100-x-warm.jpg?format=auto\nhttps://contents.mediadecathlon.com/p2047870/8712c55568dd9928c83b19c6a4067bf161811a469433dc89244f0ff96a50e3e9/men-s-winter-hiking-boots-sh-100-x-warm-grey.jpg?format=auto\n</code></pre> Expand to see the output <p> </p> <pre><code>{\n    \"products\":\n    [\n        {\n            \"name\": \"Ice Skates\",\n            \"key_features\": [\n                \"Lace-up closure\",\n                \"Durable blade\",\n                \"Ankle support\"\n            ],\n            \"description\": \"A pair of ice skates with lace-up closure for secure fit, durable blade for ice skating, and reinforced ankle support.\"\n        },\n        {\n            \"name\": \"Hiking Boots\",\n            \"key_features\": [\n                \"High-top design\",\n                \"Rugged outsole\",\n                \"Water-resistant\"\n            ],\n            \"description\": \"Sturdy hiking boots featuring a high-top design for ankle support, rugged outsole for grip on uneven terrain, and water-resistant construction.\"\n        },\n        {\n            \"name\": \"Winter Boots\",\n            \"key_features\": [\n                \"Insulated lining\",\n                \"Waterproof lower\",\n                \"Slip-resistant sole\"\n            ],\n            \"description\": \"Warm winter boots with insulated lining for cold weather, waterproof lower section to keep feet dry, and a slip-resistant sole for stability.\"\n        }\n    ],\n    \"ad_copies\": [\n        {\n            \"headline\": \"Glide with Confidence - Discover the Perfect Ice Skates!\",\n            \"ad_copy\": \"Step onto the ice with poise and precision with our premium Ice Skates. Designed for both beginners and seasoned skaters, these skates offer a perfect blend of comfort and performance. The lace-up closure ensures a snug fit that keeps you stable as you carve through the ice. With a durable blade that withstands the test of time, you can focus on perfecting your moves rather than worrying about your equipment. The reinforced ankle support provides the necessary protection and aids in preventing injuries, allowing you to skate with peace of mind. Whether you're practicing your spins, jumps, or simply enjoying a leisurely glide across the rink, our Ice Skates are the ideal companion for your ice adventures. Lace up and get ready to experience the thrill of ice skating like never before!\",\n            \"name\": \"Ice Skates\"\n        },\n        {\n            \"headline\": \"Conquer Every Trail with Confidence!\",\n            \"ad_copy\": \"Embark on your next adventure with our top-of-the-line Hiking Boots! Designed for the trail-blazing spirits, these boots boast a high-top design that provides unparalleled ankle support to keep you steady on any path. The rugged outsole ensures a firm grip on the most uneven terrains, while the water-resistant construction keeps your feet dry as you traverse through streams and muddy trails. Whether you're a seasoned hiker or just starting out, our Hiking Boots are the perfect companion for your outdoor escapades. Lace up and step into the wild with confidence - your journey awaits!\",\n            \"name\": \"Hiking Boots\"\n        },\n        {\n            \"headline\": \"Conquer the Cold with Comfort!\",\n            \"ad_copy\": \"Step into the season with confidence in our Winter Boots, the ultimate ally against the chill. Designed for those who don't let the cold dictate their moves, these boots feature an insulated lining that wraps your feet in a warm embrace, ensuring that the biting cold is a worry of the past. But warmth isn't their only virtue. With a waterproof lower section, your feet will remain dry and cozy, come rain, snow, or slush. And let's not forget the slip-resistant sole that stands between you and the treacherous ice, offering stability and peace of mind with every step you take. Whether you're braving a blizzard or just nipping out for a coffee, our Winter Boots are your trusty companions, keeping you warm, dry, and upright. Don't let winter slow you down. Lace up and embrace the elements!\",\n            \"name\": \"Winter Boots\"\n        }\n    ]\n}\n</code></pre>"},{"location":"examples/knowledge_graph/","title":"Visualizing Knowledge Graphs for Complex Topics","text":"<p>In this guide, you'll discover how to visualise a detailed knowledge graph when dealing with complex topics. We'll then move on to iteratively updating our knowledge graph with new information through a series of sequential api calls using only the Instructor library, Pydantic and Graphviz to visualise our graph.</p> <p>Motivation</p> <p>Knowledge graphs offer a visually appealing and coherent way to understand complicated topics like quantum mechanics. By generating these graphs automatically, you can accelerate the learning process and make it easier to digest complex information.</p>"},{"location":"examples/knowledge_graph/#defining-the-structures","title":"Defining the Structures","text":"<p>Let's model a knowledge graph with <code>Node</code> and <code>Edge</code> objects. <code>Node</code> objects represent key concepts or entities, while <code>Edge</code> objects indicate the relationships between them.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(..., default_factory=list)\n    edges: List[Edge] = Field(..., default_factory=list)\n</code></pre>"},{"location":"examples/knowledge_graph/#generating-knowledge-graphs","title":"Generating Knowledge Graphs","text":"<p>The <code>generate_graph</code> function leverages OpenAI's API to generate a knowledge graph based on the input query.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Adds response_model to ChatCompletion\n# Allows the return of Pydantic model rather than raw JSON\nclient = instructor.patch(OpenAI())\n\n\ndef generate_graph(input) -&gt; KnowledgeGraph:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Help me understand the following by describing it as a detailed knowledge graph: {input}\",\n            }\n        ],\n        response_model=KnowledgeGraph,\n    )  # type: ignore\n</code></pre>"},{"location":"examples/knowledge_graph/#visualizing-the-graph","title":"Visualizing the Graph","text":"<p>The <code>visualize_knowledge_graph</code> function uses the Graphviz library to render the generated knowledge graph.</p> <pre><code>from graphviz import Digraph\n\n\ndef visualize_knowledge_graph(kg: KnowledgeGraph):\n    dot = Digraph(comment=\"Knowledge Graph\")\n\n    # Add nodes\n    for node in kg.nodes:\n        dot.node(str(node.id), node.label, color=node.color)\n\n    # Add edges\n    for edge in kg.edges:\n        dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n\n    # Render the graph\n    dot.render(\"knowledge_graph.gv\", view=True)\n\ngraph = generate_graph(\"Teach me about quantum mechanics\")\nvisualize_knowledge_graph(graph)\n</code></pre> <p></p> <p>This will produce a visual representation of the knowledge graph, stored as \"knowledge_graph.gv\". You can open this file to explore the key concepts and their relationships in quantum mechanics.</p>"},{"location":"examples/knowledge_graph/#iterative-updates","title":"Iterative Updates","text":"<p>Now that we've seen how to generate a knowledge graph from a single input, let's see how we can iteratively update our knowledge graph with new information, or when informatino does not fit into a single prompt.</p> <p>Let's take an easy example where we want to visualise the combined knowledge graph that the following sentences represent.</p> <pre><code>text_chunks = [\n    \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\",\n    \"Professors are smart.\",\n    \"Sarah knows Jason and is a student of his.\",\n    \"Sarah is a student at the University of Toronto. and UofT is in Canada\",\n]\n</code></pre>"},{"location":"examples/knowledge_graph/#updating-our-data-model","title":"Updating Our Data Model","text":"<p>To support our new iterative approach, we need to update our data model. We can do this by adding helper methods <code>update</code> and <code>draw</code> to our Pydantic models. These methods will simplify our code and allow us to easily visualize the knowledge graph.</p> <p>In the <code>KnowledgeGraph</code> class, we have migrated the code from the <code>visualize_knowledge_graph</code> method and added new lists for nodes and edges.</p> <pre><code>class KnowledgeGraph(BaseModel):\n    nodes: Optional[List[Node]] = Field(..., default_factory=list)\n    edges: Optional[List[Edge]] = Field(..., default_factory=list)\n\n    def update(self, other: \"KnowledgeGraph\") -&gt; \"KnowledgeGraph\":\n        \"\"\"Updates the current graph with the other graph, deduplicating nodes and edges.\"\"\"\n        return KnowledgeGraph(\n            nodes=list(set(self.nodes + other.nodes)),\n            edges=list(set(self.edges + other.edges)),\n        )\n\n    def draw(self, prefix: str = None):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:  # (1)!\n            dot.node(str(node.id), node.label, color=node.color)\n\n        for edge in self.edges:  # (2)!\n            dot.edge(\n                str(edge.source), str(edge.target), label=edge.label, color=edge.color\n            )\n        dot.render(prefix, format=\"png\", view=True)\n</code></pre> <ol> <li>We iterate through all the nodes in our graph and add them to the graph</li> <li>We iterate through all the edges in our graph and add them to the graph</li> </ol> <p>We can modify our <code>generate_graph</code> function to now take in a list of strings. At each step, it'll extract out the key insights from the sentences in the form of edges and nodes like we've seen before. We can then combine these new edges and nodes with our existing knowledge graph through iterative updates to our graph before arriving at our final result.</p> <pre><code>def generate_graph(input: List[str]) -&gt; KnowledgeGraph:\n    cur_state = KnowledgeGraph()  # (1)!\n    num_iterations = len(input)\n    for i, inp in enumerate(input):\n        new_updates = client.chat.completions.create(\n            model=\"gpt-3.5-turbo-16k\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an iterative knowledge graph builder.\n                    You are given the current state of the graph, and you must append the nodes and edges\n                    to it Do not procide any duplcates and try to reuse nodes as much as possible.\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Extract any new nodes and edges from the following:\n                    # Part {i}/{num_iterations} of the input:\n\n                    {inp}\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Here is the current state of the graph:\n                    {cur_state.model_dump_json(indent=2)}\"\"\",\n                },  # (2)!\n            ],\n            response_model=KnowledgeGraph,\n        )  # type: ignore\n\n        # Update the current state\n        cur_state = cur_state.update(new_updates)  # (3)!\n        cur_state.draw(prefix=f\"iteration_{i}\")\n    return cur_state\n</code></pre> <ol> <li> <p>We first initialise an empty <code>KnowledgeGraph</code>. In this state, it has zero nodes and edges</p> </li> <li> <p>We then add in the current state of the graph into the prompt so that the model knows what new information needs to be added</p> </li> <li> <p>We then update the nodes and edges of our graph with the information that our model has returned before visualizing the new changes</p> </li> </ol> <p>Once we've done this, we can now run this new <code>generate_graph</code> function with the following two lines.</p> <pre><code>graph: KnowledgeGraph = generate_graph(text_chunks)\ngraph.draw(prefix=\"final\")\n</code></pre>"},{"location":"examples/knowledge_graph/#conclusion","title":"Conclusion","text":"<p>We've seen how we can use <code>Instructor</code> to obtain structured outputs from the OpenAI LLM API but you could use that for any of the other open-source models that the library is compatible with. If you enjoy the content or want to try out <code>Instructor</code> check out the github and don't forget to give us a star!</p>"},{"location":"examples/moderation/","title":"OpenAI Moderation","text":"<p>This example uses OpenAI's moderation endpoint to check content compliance with OpenAI's usage policies. It can identify and filter harmful content that violates the policies.</p> <p>The model flags content and classifies it into categories including hate, harassment, self-harm, sexual content, and violence. Each category has subcategories for detailed classification.</p> <p>This validator is to be used for monitoring OpenAI API inputs and outputs, other use cases are currently not allowed.</p>"},{"location":"examples/moderation/#incorporating-openai-moderation-validator","title":"Incorporating OpenAI moderation validator","text":"<p>The following code defines a function to validate content using OpenAI's Moderation endpoint. The <code>AfterValidator</code> is used to apply OpenAI's moderation after the compute. This moderation checks if the content complies with OpenAI's usage policies and flags any harmful content. Here's how it works:</p> <ol> <li> <p>Generate the OpenAI client and patch it with the <code>instructor</code>. Patching is not strictly necessary for this example but its a good idea to always patch the client to leverage the full <code>instructor</code> functionality.</p> </li> <li> <p>Annotate our <code>message</code> field with <code>AfterValidator(openai_moderation(client=client))</code>. This means that after the <code>message</code> is computed, it will be passed to the <code>openai_moderation</code> function for validation.</p> </li> </ol> <pre><code>import instructor\n\nfrom instructor import openai_moderation\n\nfrom typing_extensions import Annotated\nfrom pydantic import BaseModel, AfterValidator\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(openai_moderation(client=client))]\n\ntry:\n    Response(message=\"I want to make them suffer the consequences\")\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for Response\n    message\n      Value error, `I want to make them suffer the consequences` was flagged for violence, violence/threat [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n    \"\"\"\n\ntry:\n    Response(message=\"I want to hurt myself.\")\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for Response\n    message\n      Value error, `I want to hurt myself` was flagged for self_harm, self_harm_intent, violence, self-harm, self-harm/intent [type=value_error, input_value='I want to hurt myself', input_type=str]\n    \"\"\"\n</code></pre>"},{"location":"examples/ollama/","title":"Structured Outputs with Ollama","text":"<p>Open-source LLMS are gaining popularity, and with the release of Ollama's OpenAI compatibility layer, it has become possible to obtain structured outputs using JSON schema.</p> <p>By the end of this blog post, you will learn how to effectively utilize instructor with Ollama. But before we proceed, let's first explore the concept of patching.</p>"},{"location":"examples/ollama/#patching","title":"Patching","text":"<p>Instructor's patch enhances an openai api with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>"},{"location":"examples/ollama/#ollama","title":"Ollama","text":"<p>Start by downloading Ollama, and then pull a model such as Llama 2 or Mistral.</p> <p>Make sure you update your <code>ollama</code> to the latest version!</p> <pre><code>ollama pull llama2\n</code></pre> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nimport instructor\n\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    fact: List[str] = Field(..., description=\"A list of facts about the character\")\n\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nresp = client.chat.completions.create(\n    model=\"llama2\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me about the Harry Potter\",\n        }\n    ],\n    response_model=Character,\n)\nprint(resp.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Harry James Potter\",\n  \"age\": 37,\n  \"fact\": [\n    \"He is the chosen one.\",\n    \"He has a lightning-shaped scar on his forehead.\",\n    \"He is the son of James and Lily Potter.\",\n    \"He attended Hogwarts School of Witchcraft and Wizardry.\",\n    \"He is a skilled wizard and sorcerer.\",\n    \"He fought against Lord Voldemort and his followers.\",\n    \"He has a pet owl named Snowy.\"\n  ]\n}\n\"\"\"\n</code></pre>"},{"location":"examples/open_source/","title":"Instructor with open source models","text":"<p>Instructor works with Open source model providers that support the OpenAI API chat endpoint</p> <p>See examples README here</p>"},{"location":"examples/open_source/#currently-tested-open-source-model-providers","title":"Currently tested open source model providers","text":"<ul> <li>OpenRouter</li> <li>Perplexity</li> <li>RunPod TheBloke LLMs **</li> </ul> <p>** This utilizes text-generation-webui w/ Openai plugin under the hood. </p>"},{"location":"examples/pii/","title":"PII Data Extraction and Scrubbing","text":""},{"location":"examples/pii/#overview","title":"Overview","text":"<p>This example demonstrates the usage of OpenAI's ChatCompletion model for the extraction and scrubbing of Personally Identifiable Information (PII) from a document. The code defines Pydantic models to manage the PII data and offers methods for both extraction and sanitation.</p>"},{"location":"examples/pii/#defining-the-structures","title":"Defining the Structures","text":"<p>First, Pydantic models are defined to represent the PII data and the overall structure for PII data extraction.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\n\n# Define Schemas for PII data\nclass Data(BaseModel):\n    index: int\n    data_type: str\n    pii_value: str\n\n\nclass PIIDataExtraction(BaseModel):\n    \"\"\"\n    Extracted PII data from a document, all data_types should try to have consistent property names\n    \"\"\"\n\n    private_data: List[Data]\n\n    def scrub_data(self, content: str) -&gt; str:\n        \"\"\"\n        Iterates over the private data and replaces the value with a placeholder in the form of\n        &lt;{data_type}_{i}&gt;\n        \"\"\"\n        for i, data in enumerate(self.private_data):\n            content = content.replace(data.pii_value, f\"&lt;{data.data_type}_{i}&gt;\")\n        return content\n</code></pre>"},{"location":"examples/pii/#extracting-pii-data","title":"Extracting PII Data","text":"<p>The OpenAI API is utilized to extract PII information from a given document.</p> <pre><code>from openai import OpenAI\nimport instructor\n\nclient = instructor.patch(OpenAI())\n\nEXAMPLE_DOCUMENT = \"\"\"\n# Fake Document with PII for Testing PII Scrubbing Model\n# (The content here)\n\"\"\"\n\npii_data = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=PIIDataExtraction,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world class PII scrubbing model, Extract the PII data from the following document\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": EXAMPLE_DOCUMENT,\n        },\n    ],\n)  # type: ignore\n\nprint(\"Extracted PII Data:\")\nprint(pii_data.model_dump_json())\n</code></pre>"},{"location":"examples/pii/#output-of-extracted-pii-data","title":"Output of Extracted PII Data","text":"<pre><code>{\n  \"private_data\": [\n    {\n      \"index\": 0,\n      \"data_type\": \"date\",\n      \"pii_value\": \"01/02/1980\"\n    },\n    {\n      \"index\": 1,\n      \"data_type\": \"ssn\",\n      \"pii_value\": \"123-45-6789\"\n    },\n    {\n      \"index\": 2,\n      \"data_type\": \"email\",\n      \"pii_value\": \"john.doe@email.com\"\n    },\n    {\n      \"index\": 3,\n      \"data_type\": \"phone\",\n      \"pii_value\": \"555-123-4567\"\n    },\n    {\n      \"index\": 4,\n      \"data_type\": \"address\",\n      \"pii_value\": \"123 Main St, Springfield, IL, 62704\"\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/pii/#scrubbing-pii-data","title":"Scrubbing PII Data","text":"<p>After extracting the PII data, the <code>scrub_data</code> method is used to sanitize the document.</p> <pre><code>print(\"Scrubbed Document:\")\nprint(pii_data.scrub_data(EXAMPLE_DOCUMENT))\n</code></pre>"},{"location":"examples/pii/#output-of-scrubbed-document","title":"Output of Scrubbed Document","text":"<pre><code># Fake Document with PII for Testing PII Scrubbing Model\n\n## Personal Story\n\nJohn Doe was born on &lt;date_0&gt;. His social security number is &lt;ssn_1&gt;. He has been using the email address &lt;email_2&gt; for years, and he can always be reached at &lt;phone_3&gt;.\n\n## Residence\n\nJohn currently resides at &lt;address_4&gt;. He's been living there for about 5 years now.\n</code></pre>"},{"location":"examples/planning-tasks/","title":"Example: Planning and Executing a Query Plan","text":"<p>This example demonstrates how to use the OpenAI Function Call ChatCompletion model to plan and execute a query plan in a question-answering system. By breaking down a complex question into smaller sub-questions with defined dependencies, the system can systematically gather the necessary information to answer the main question.</p> <p>Motivation</p> <p>The goal of this example is to showcase how query planning can be used to handle complex questions, facilitate iterative information gathering, automate workflows, and optimize processes. By leveraging the OpenAI Function Call model, you can design and execute a structured plan to find answers effectively.</p> <p>Use Cases:</p> <ul> <li>Complex question answering</li> <li>Iterative information gathering</li> <li>Workflow automation</li> <li>Process optimization</li> </ul> <p>With the OpenAI Function Call model, you can customize the planning process and integrate it into your specific application to meet your unique requirements.</p>"},{"location":"examples/planning-tasks/#defining-the-structures","title":"Defining the Structures","text":"<p>Let's define the necessary Pydantic models to represent the query plan and the queries.</p> <pre><code>import enum\nfrom typing import List\nfrom pydantic import Field, BaseModel\n\n\nclass QueryType(str, enum.Enum):\n    \"\"\"Enumeration representing the types of queries that can be asked to a question answer system.\"\"\"\n\n    SINGLE_QUESTION = \"SINGLE\"\n    MERGE_MULTIPLE_RESPONSES = \"MERGE_MULTIPLE_RESPONSES\"\n\n\nclass Query(BaseModel):\n    \"\"\"Class representing a single question in a query plan.\"\"\"\n\n    id: int = Field(..., description=\"Unique id of the query\")\n    question: str = Field(\n        ...,\n        description=\"Question asked using a question answering system\",\n    )\n    dependencies: List[int] = Field(\n        default_factory=list,\n        description=\"List of sub questions that need to be answered before asking this question\",\n    )\n    node_type: QueryType = Field(\n        default=QueryType.SINGLE_QUESTION,\n        description=\"Type of question, either a single question or a multi-question merge\",\n    )\n\n\nclass QueryPlan(BaseModel):\n    \"\"\"Container class representing a tree of questions to ask a question answering system.\"\"\"\n\n    query_graph: List[Query] = Field(\n        ..., description=\"The query graph representing the plan\"\n    )\n\n    def _dependencies(self, ids: List[int]) -&gt; List[Query]:\n        \"\"\"Returns the dependencies of a query given their ids.\"\"\"\n        return [q for q in self.query_graph if q.id in ids]\n</code></pre> <p>Graph Generation</p> <p>Notice that this example produces a flat list of items with dependencies that resemble a graph, while pydantic allows for recursive definitions, it's much easier and less confusing for the model to generate flat schemas rather than recursive schemas. If you want to see a recursive example, see recursive schemas</p>"},{"location":"examples/planning-tasks/#planning-a-query-plan","title":"Planning a Query Plan","text":"<p>Now, let's demonstrate how to plan and execute a query plan using the defined models and the OpenAI API.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\n\ndef query_planner(question: str) -&gt; QueryPlan:\n    PLANNING_MODEL = \"gpt-4-0613\"\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world class query planning algorithm capable ofbreaking apart questions into its dependency queries such that the answers can be used to inform the parent question. Do not answer the questions, simply provide a correct compute graph with good specific questions to ask and relevant dependencies. Before you call the function, think step-by-step to get a better understanding of the problem.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Consider: {question}\\nGenerate the correct query plan.\",\n        },\n    ]\n\n    root = client.chat.completions.create(\n        model=PLANNING_MODEL,\n        temperature=0,\n        response_model=QueryPlan,\n        messages=messages,\n        max_tokens=1000,\n    )\n    return root\n</code></pre> <pre><code>plan = query_planner(\n    \"What is the difference in populations of Canada and the Jason's home country?\"\n)\nplan.model_dump()\n</code></pre> <p>No RAG</p> <p>While we build the query plan in this example, we do not propose a method to actually answer the question. You can implement your own answer function that perhaps makes a retrieval and calls openai for retrieval augmented generation. That step would also make use of function calls but goes beyond the scope of this example.</p> <pre><code>{\n    \"query_graph\": [\n        {\n            \"dependencies\": [],\n            \"id\": 1,\n            \"node_type\": \"SINGLE\",\n            \"question\": \"Identify Jason's home country\",\n        },\n        {\n            \"dependencies\": [],\n            \"id\": 2,\n            \"node_type\": \"SINGLE\",\n            \"question\": \"Find the population of Canada\",\n        },\n        {\n            \"dependencies\": [1],\n            \"id\": 3,\n            \"node_type\": \"SINGLE\",\n            \"question\": \"Find the population of Jason's home country\",\n        },\n        {\n            \"dependencies\": [2, 3],\n            \"id\": 4,\n            \"node_type\": \"SINGLE\",\n            \"question\": \"Calculate the difference in populations between Canada and Jasons home country\",\n        },\n    ]\n}\n</code></pre> <p>In the above code, we define a <code>query_planner</code> function that takes a question as input and generates a query plan using the OpenAI API.</p>"},{"location":"examples/planning-tasks/#conclusion","title":"Conclusion","text":"<p>In this example, we demonstrated how to use the OpenAI Function Call <code>ChatCompletion</code> model to plan and execute a query plan using a question-answering system. We defined the necessary structures using Pydantic, created a query planner function.</p> <p>If you want to see multiple versions of this style of code, please visit:</p> <ol> <li>query planning example</li> <li>task planning with topo sort</li> </ol> <p>Feel free to modify the code to fit your specific use case and explore other possibilities of using the OpenAI Function Call model to plan and execute complex workflows.</p>"},{"location":"examples/search/","title":"Example: Segmenting Search Queries","text":"<p>In this example, we will demonstrate how to leverage the <code>MultiTask</code> and <code>enum.Enum</code> features of OpenAI Function Call to segment search queries. We will define the necessary structures using Pydantic and demonstrate how segment queries into multiple sub queries and execute them in parallel with <code>asyncio</code>.</p> <p>Motivation</p> <p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use OpenAI Function Call to segment search queries and execute them in parallel.</p>"},{"location":"examples/search/#structure-of-the-data","title":"Structure of the Data","text":"<p>The <code>Search</code> class is a Pydantic model that defines the structure of the search query. It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel, Field\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\nclass Search(BaseModel):\n    query: str = Field(..., description=\"Query to search for relevant content\")\n    type: Literal[\"web\", \"image\", \"video\"] = Field(..., description=\"Type of search\")\n\n    async def execute(self):\n        print(\n            f\"Searching for `{self.title}` with query `{self.query}` using `{self.type}`\"\n        )\n\n\ndef segment(data: str) -&gt; MultiSearch:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=Iterable[Search],\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Consider the data below: '\\n{data}' and segment it into multiple search queries\",\n            },\n        ],\n        max_tokens=1000,\n    )\n\nfor search in segment(\"Search for a picture of a cat and a video of a dog\"):\n    print(search.model_dump_json())\n    \"\"\"\n    {\n        \"query\": \"a picture of a cat\",\n        \"type\": \"image\"\n    }\n    {\n        \"query\": \"a video of a dog\",\n        \"type\": \"video\"\n    }\n    \"\"\"\n    }\n</code></pre>"},{"location":"examples/self_critique/","title":"Self-Correction with <code>llm_validator</code>","text":""},{"location":"examples/self_critique/#introduction","title":"Introduction","text":"<p>This guide demonstrates how to use <code>llm_validator</code> for implementing self-healing. The objective is to showcase how an instructor can self-correct by using validation errors and helpful error messages.</p> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\nquestion = \"What is the meaning of life?\"\ncontext = \"The according to the devil the meaning of live is to live a life of sin and debauchery.\"\n\nqa: QuestionAnswer = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n</code></pre>"},{"location":"examples/self_critique/#output-before-validation","title":"Output Before Validation","text":"<p>While it calls out the objectionable content, it doesn't provide any details on how to correct it.</p> <pre><code>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life, according to the context, is to live a life of sin and debauchery.\"\n}\n</code></pre>"},{"location":"examples/self_critique/#adding-custom-validation","title":"Adding Custom Validation","text":"<p>By adding a validator to the <code>answer</code> field, we can try to catch the issue and correct it. Lets integrate <code>llm_validator</code> into the model and see the error message. Its important to note that you can use all of pydantic's validators as you would normally as long as you raise a <code>ValidationError</code> with a helpful error message as it will be used as part of the self correction prompt.</p> <pre><code>from pydantic import BaseModel, BeforeValidator\nfrom typing_extensions import Annotated\nfrom instructor import llm_validator\n\nclass QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\", allow_override=True)\n        ),\n    ]\n\n\ntry:\n    qa: QuestionAnswerNoEvil = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=QuestionAnswerNoEvil,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n            },\n        ],\n    )\nexcept Exception as e:\n    print(e)\n</code></pre>"},{"location":"examples/self_critique/#output-after-validation","title":"Output After Validation","text":"<p>Now, we throw validation error that its objectionable and provide a helpful error message.</p> <pre><code>1 validation error for QuestionAnswerNoEvil\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which is objectionable.\n</code></pre>"},{"location":"examples/self_critique/#retrying-with-corrections","title":"Retrying with Corrections","text":"<p>By adding the <code>max_retries</code> parameter, we can retry the request with corrections. and use the error message to correct the output.</p> <pre><code>qa: QuestionAnswerNoEvil = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswerNoEvil,\n    max_retries=2,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n</code></pre>"},{"location":"examples/self_critique/#final-output","title":"Final Output","text":"<p>Now, we get a valid response that is not objectionable!</p> <pre><code>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life is subjective and can vary depending on individual beliefs and philosophies.\"\n}\n</code></pre>"},{"location":"examples/sqlmodel/","title":"Integrating Instructor with SQLModel","text":"<p>SQLModel is a library designed for interacting with SQL databases from Python code using Python objects. <code>SQLModel</code> is based on <code>Pydantic</code> and <code>SQLAlchemy</code> and was created by tiangolo who also developed <code>FastAPI</code>. So you can expect seamless integration across all these libraries, reducing code duplicating and improving your developer experience. </p>"},{"location":"examples/sqlmodel/#example-adding-responses-from-instructor-directly-to-your-db","title":"Example: Adding responses from Instructor directly to your DB","text":""},{"location":"examples/sqlmodel/#defining-the-models","title":"Defining the Models","text":"<p>First we'll define a model that will serve as a table for our database and the structure of our outputs from <code>Instructor</code></p> <p>Model Definition</p> <p>You'll need to subclass your models with both <code>SQLModel</code> and <code>instructor.OpenAISchema</code> for them to work with SQLModel</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Optional\nfrom sqlmodel import Field, SQLModel, create_engine\n\n\nclass Hero(SQLModel, instructor.OpenAISchema, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n    secret_name: str\n    age: Optional[int] = None\n</code></pre>"},{"location":"examples/sqlmodel/#generating-a-record","title":"Generating a record","text":"<p>The <code>create_hero</code> function will query <code>OpenAI</code> for a <code>Hero</code> record</p> <pre><code>client = instructor.patch(OpenAI())\n\ndef create_hero() -&gt; Hero:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Hero,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Make a new superhero\"},\n        ],\n    )\n</code></pre>"},{"location":"examples/sqlmodel/#inserting-the-response-into-the-db","title":"Inserting the response into the DB","text":"<pre><code>engine = create_engine(\"sqlite:///database.db\")\nSQLModel.metadata.create_all(engine)\n\nhero = create_hero()\nprint(hero.model_dump())\n    \"\"\"\n    {'name': 'SuperNova', 'secret_name': 'Mia Thompson', 'age': 28, 'id': None}\n    \"\"\"\n\nwith Session(engine) as session:\n    session.add(hero)\n    session.commit()\n</code></pre> <p>And there you have it! You can now use the same models for your database and <code>Instructor</code> enabling them work seamlessly! Also checkout the FastAPI guide to see how you can use these models in an API as well. </p>"},{"location":"hub/","title":"Instructor Hub","text":"<p>Welcome to instructor hub, the goal of this project is to provide a set of tutorials and examples to help you get started, and allow you to pull in the code you need to get started with <code>instructor</code></p> <p>Make sure you're using the latest version of <code>instructor</code> by running:</p> <pre><code>pip install -U instructor\n</code></pre>"},{"location":"hub/#contributing","title":"Contributing","text":"<p>We welcome contributions to the instructor hub, if you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ol> <li>The code must be in a single file</li> <li>Make sure that its referenced in the <code>mkdocs.yml</code></li> <li>Make sure that the code is unit tested.</li> </ol>"},{"location":"hub/#using-pytest_examples","title":"Using pytest_examples","text":"<p>By running the following command you can run the tests and update the examples. This ensures that the examples are always up to date. Linted correctly and that the examples are working, make sure to include a <code>if __name__ == \"__main__\":</code> block in your code and add some asserts to ensure that the code is working.</p> <pre><code>poetry run pytest tests/openai/docs/test_hub.py --update-examples\n</code></pre>"},{"location":"hub/#cli-usage","title":"CLI Usage","text":"<p>Instructor hub comes with a command line interface (CLI) that allows you to view and interact with the tutorials and examples and allows you to pull in the code you need to get started with the API.</p>"},{"location":"hub/#list-cookbooks","title":"List Cookbooks","text":"<p>By running <code>instructor hub list</code> you can see all the available tutorials and examples. By clickony (doc) you can see the full tutorial back on this website.</p> <pre><code>$ instructor hub list --sort\n</code></pre> hub_id slug title n_downloads 2 multiple_classification (doc) Multiple Classification Model 24 1 single_classification (doc) Single Classification Model 2"},{"location":"hub/#searching-for-cookbooks","title":"Searching for Cookbooks","text":"<p>You can search for a tutorial by running <code>instructor hub list -q &lt;QUERY&gt;</code>. This will return a list of tutorials that match the query.</p> <pre><code>$ instructor hub list -q multi\n</code></pre> hub_id slug title n_downloads 2 multiple_classification (doc) Multiple Classification Model 24"},{"location":"hub/#reading-a-cookbook","title":"Reading a Cookbook","text":"<p>To read a tutorial, you can run <code>instructor hub pull --id &lt;hub_id&gt; --page</code> to see the full tutorial in the terminal. You can use <code>j,k</code> to scroll up and down, and <code>q</code> to quit. You can also run it without <code>--page</code> to print the tutorial to the terminal.</p> <pre><code>$ instructor hub pull --id 2 --page\n</code></pre>"},{"location":"hub/#pulling-in-code","title":"Pulling in Code","text":"<p>You can pull in the code with <code>--py --output=&lt;filename&gt;</code> to save the code to a file, or you cal also run it without <code>--output</code> to print the code to the terminal.</p> <pre><code>$ instructor hub pull --id 2 --py --output=run.py\n$ instructor hub pull --id 2 --py &gt; run.py\n</code></pre> <p>You can run the code instantly if you <code>|</code> it to <code>python</code>:</p> <pre><code>$ instructor hub pull --id 2 --py | python\n</code></pre>"},{"location":"hub/#call-for-contributions","title":"Call for Contributions","text":"<p>We're looking for a bunch more hub examples, if you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ul> <li> Converting the cookbooks to the new format</li> <li> Validator examples</li> <li> Data extraction examples</li> <li> Streaming examples (Iterable and Partial)</li> <li> Batch Parsing examples</li> <li> Query Expansion examples</li> <li> Batch Data Processing examples</li> <li> Batch Data Processing examples with Cache</li> </ul>"},{"location":"hub/action_items/","title":"Extracting Action Items from Meeting Transcripts","text":"<p>In this guide, we'll walk through how to extract action items from meeting transcripts using OpenAI's API and Pydantic. This use case is essential for automating project management tasks, such as task assignment and priority setting.</p> <p>If you want to try outs via <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug action_items --py &gt; action_items.py\n</code></pre> <p>For multi-label classification, we introduce a new enum class and a different Pydantic model to handle multiple labels.</p> <p>Motivation</p> <p>Significant amount of time is dedicated to meetings, where action items are generated as the actionable outcomes of these discussions. Automating the extraction of action items can save time and guarantee that no critical tasks are overlooked.</p>"},{"location":"hub/action_items/#defining-the-structures","title":"Defining the Structures","text":"<p>We'll model a meeting transcript as a collection of <code>Ticket</code> objects, each representing an action item. Every <code>Ticket</code> can have multiple <code>Subtask</code> objects, representing smaller, manageable pieces of the main task.</p>"},{"location":"hub/action_items/#extracting-action-items","title":"Extracting Action Items","text":"<p>To extract action items from a meeting transcript, we use the <code>generate</code> function. It calls OpenAI's API, processes the text, and returns a set of action items modeled as <code>ActionItems</code>.</p>"},{"location":"hub/action_items/#evaluation-and-testing","title":"Evaluation and Testing","text":"<p>To test the <code>generate</code> function, we provide it with a sample transcript, and then print the JSON representation of the extracted action items.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable, List, Optional\nfrom enum import Enum\nfrom pydantic import BaseModel\n\n\nclass PriorityEnum(str, Enum):\n    high = \"High\"\n    medium = \"Medium\"\n    low = \"Low\"\n\n\nclass Subtask(BaseModel):\n    \"\"\"Correctly resolved subtask from the given transcript\"\"\"\n\n    id: int\n    name: str\n\n\nclass Ticket(BaseModel):\n    \"\"\"Correctly resolved ticket from the given transcript\"\"\"\n\n    id: int\n    name: str\n    description: str\n    priority: PriorityEnum\n    assignees: List[str]\n    subtasks: Optional[List[Subtask]]\n    dependencies: Optional[List[int]]\n\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\n\ndef generate(data: str) -&gt; Iterable[Ticket]:\n    return client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[Ticket],\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"The following is a transcript of a meeting...\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Create the action items for the following transcript: {data}\",\n            },\n        ],\n    )\n\n\nprediction = generate(\n    \"\"\"\nAlice: Hey team, we have several critical tasks we need to tackle for the upcoming release. First, we need to work on improving the authentication system. It's a top priority.\n\nBob: Got it, Alice. I can take the lead on the authentication improvements. Are there any specific areas you want me to focus on?\n\nAlice: Good question, Bob. We need both a front-end revamp and back-end optimization. So basically, two sub-tasks.\n\nCarol: I can help with the front-end part of the authentication system.\n\nBob: Great, Carol. I'll handle the back-end optimization then.\n\nAlice: Perfect. Now, after the authentication system is improved, we have to integrate it with our new billing system. That's a medium priority task.\n\nCarol: Is the new billing system already in place?\n\nAlice: No, it's actually another task. So it's a dependency for the integration task. Bob, can you also handle the billing system?\n\nBob: Sure, but I'll need to complete the back-end optimization of the authentication system first, so it's dependent on that.\n\nAlice: Understood. Lastly, we also need to update our user documentation to reflect all these changes. It's a low-priority task but still important.\n\nCarol: I can take that on once the front-end changes for the authentication system are done. So, it would be dependent on that.\n\nAlice: Sounds like a plan. Let's get these tasks modeled out and get started.\"\"\"\n)\n</code></pre>"},{"location":"hub/action_items/#visualizing-the-tasks","title":"Visualizing the tasks","text":"<p>In order to quickly visualize the data we used code interpreter to create a graphviz export of the json version of the ActionItems array.</p> <p></p> <pre><code>[\n  {\n    \"id\": 1,\n    \"name\": \"Improve Authentication System\",\n    \"description\": \"Revamp the front-end and optimize the back-end of the authentication system\",\n    \"priority\": \"High\",\n    \"assignees\": [\"Bob\", \"Carol\"],\n    \"subtasks\": [\n      {\n        \"id\": 2,\n        \"name\": \"Front-end Revamp\"\n      },\n      {\n        \"id\": 3,\n        \"name\": \"Back-end Optimization\"\n      }\n    ],\n    \"dependencies\": []\n  },\n  {\n    \"id\": 4,\n    \"name\": \"Integrate Authentication System with Billing System\",\n    \"description\": \"Integrate the improved authentication system with the new billing system\",\n    \"priority\": \"Medium\",\n    \"assignees\": [\"Bob\"],\n    \"subtasks\": [],\n    \"dependencies\": [1]\n  },\n  {\n    \"id\": 5,\n    \"name\": \"Update User Documentation\",\n    \"description\": \"Update the user documentation to reflect the changes in the authentication system\",\n    \"priority\": \"Low\",\n    \"assignees\": [\"Carol\"],\n    \"subtasks\": [],\n    \"dependencies\": [2]\n  }\n]\n</code></pre> <p>In this example, the <code>generate</code> function successfully identifies and segments the action items, assigning them priorities, assignees, subtasks, and dependencies as discussed in the meeting.</p> <p>By automating this process, you can ensure that important tasks and details are not lost in the sea of meeting minutes, making project management more efficient and effective.</p>"},{"location":"hub/anyscale/","title":"Structured Outputs with Anyscale","text":"<p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug anyscale --py &gt; anyscale_example.py\n</code></pre> <p>Open-source LLMS are gaining popularity, and the release of Anyscale's Mistral model has made it possible to obtain structured outputs using JSON schema at any scale. Instead of relying on a model's default output mode, you can utilize JSON schema to obtain structured outputs. This approach is a time-saving alternative to extensive prompt engineering.</p> <p>By the end of this blog post, you will learn how to effectively utilize the instructor at any scale. But before we proceed, let's first explore the concept of patching.</p>","tags":["patching","open source"]},{"location":"hub/anyscale/#patching","title":"Patching","text":"<p>Instructor's patch enhances a openai api it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>","tags":["patching","open source"]},{"location":"hub/anyscale/#anyscale","title":"Anyscale","text":"<p>The good news is that Anyscale employs the same OpenAI client, and its models support some of these output modes too!</p> <p>Getting access</p> <p>If you want to try this out for yourself check out the Anyscale website. You can get started here.</p> <p>Let's explore one of the models available in Anyscale's extensive collection!</p> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel\nimport os\nimport instructor\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"https://api.endpoints.anyscale.com/v1\",\n        api_key=os.environ[\"ANYSCALE_API_KEY\"],\n    ),\n    # This uses Anyscale's json schema output mode\n    mode=instructor.Mode.JSON_SCHEMA,\n)\n\nresp = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a world class extractor\"},\n        {\"role\": \"user\", \"content\": 'Extract the following entities: \"Jason is 20\"'},\n    ],\n    response_model=UserDetails,\n)\nprint(resp)\n#&gt; name='Jason' age=20\n# # &gt; name='Jason' age=20\n</code></pre> <p>You can find more information about Anyscale's output mode support here.</p>","tags":["patching","open source"]},{"location":"hub/batch_classification_langsmith/","title":"Seamless Support with Langsmith","text":"<p>Its a common misconception that LangChain's LangSmith is only compatible with LangChain's models. In reality, LangSmith is a unified DevOps platform for developing, collaborating, testing, deploying, and monitoring LLM applications. In this blog we will explore how LangSmith can be used to enhance the OpenAI client alongside <code>instructor</code>.</p> <p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>pip install -U langsmith\ninstructor hub pull --slug batch_classification_langsmith --py &gt; langsmith_example.py\n</code></pre>"},{"location":"hub/batch_classification_langsmith/#langsmith","title":"LangSmith","text":"<p>In order to use langsmith, you first need to set your LangSmith API key.</p> <pre><code>export LANGCHAIN_API_KEY=&lt;your-api-key&gt;\n</code></pre> <p>Next, you will need to install the LangSmith SDK:</p> <pre><code>pip install -U langsmith\npip install -U instructor\n</code></pre> <p>In this example we'll use the <code>wrap_openai</code> function to wrap the OpenAI client with LangSmith. This will allow us to use LangSmith's observability and monitoring features with the OpenAI client. Then we'll use <code>instructor</code> to patch the client with the <code>TOOLS</code> mode. This will allow us to use <code>instructor</code> to add additional functionality to the client.</p> <pre><code>import instructor\nimport asyncio\n\nfrom langsmith import traceable\nfrom langsmith.wrappers import wrap_openai\n\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import List\nfrom enum import Enum\n\n# Wrap the OpenAI client with LangSmith\nclient = wrap_openai(AsyncOpenAI())\n\n# Patch the client with instructor\nclient = instructor.patch(client, mode=instructor.Mode.TOOLS)\n\n# Rate limit the number of requests\nsem = asyncio.Semaphore(5)\n\n# Use an Enum to define the types of questions\nclass QuestionType(Enum):\n    CONTACT = \"CONTACT\"\n    TIMELINE_QUERY = \"TIMELINE_QUERY\"\n    DOCUMENT_SEARCH = \"DOCUMENT_SEARCH\"\n    COMPARE_CONTRAST = \"COMPARE_CONTRAST\"\n    EMAIL = \"EMAIL\"\n    PHOTOS = \"PHOTOS\"\n    SUMMARY = \"SUMMARY\"\n\n\n# You can add more instructions and examples in the description\n# or you can put it in the prompt in `messages=[...]`\nclass QuestionClassification(BaseModel):\n    \"\"\"\n    Predict the type of question that is being asked.\n    Here are some tips on how to predict the question type:\n    CONTACT: Searches for some contact information.\n    TIMELINE_QUERY: \"When did something happen?\n    DOCUMENT_SEARCH: \"Find me a document\"\n    COMPARE_CONTRAST: \"Compare and contrast two things\"\n    EMAIL: \"Find me an email, search for an email\"\n    PHOTOS: \"Find me a photo, search for a photo\"\n    SUMMARY: \"Summarize a large amount of data\"\n    \"\"\"\n\n    # If you want only one classification, just change it to\n    #   `classification: QuestionType` rather than `classifications: List[QuestionType]``\n    chain_of_thought: str = Field(\n        ..., description=\"The chain of thought that led to the classification\"\n    )\n    classification: List[QuestionType] = Field(\n        description=f\"An accuracy and correct prediction predicted class of question. Only allowed types: {[t.value for t in QuestionType]}, should be used\",\n    )\n\n    @field_validator(\"classification\", mode=\"before\")\n    def validate_classification(cls, v):\n        # sometimes the API returns a single value, just make sure it's a list\n        if not isinstance(v, list):\n            v = [v]\n        return v\n\n\n@traceable(name=\"classify-question\")\nasync def classify(data: str) -&gt; QuestionClassification:\n    \"\"\"\n    Perform multi-label classification on the input text.\n    Change the prompt to fit your use case.\n\n    Args:\n        data (str): The input text to classify.\n    \"\"\"\n    async with sem:  # some simple rate limiting\n        return data, await client.chat.completions.create(\n            model=\"gpt-4-turbo-preview\",\n            response_model=QuestionClassification,\n            max_retries=2,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Classify the following question: {data}\",\n                },\n            ],\n        )\n\n\nasync def main(questions: List[str]):\n    tasks = [classify(question) for question in questions]\n\n    for task in asyncio.as_completed(tasks):\n        question, label = await task\n        resp = {\n            \"question\": question,\n            \"classification\": [c.value for c in label.classification],\n            \"chain_of_thought\": label.chain_of_thought,\n        }\n        resps.append(resp)\n    return resps\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    questions = [\n        \"What was that ai app that i saw on the news the other day?\",\n        \"Can you find the trainline booking email?\",\n        \"what did I do on Monday?\",\n        \"Tell me about todays meeting and how it relates to the email on Monday\",\n    ]\n\n    resp = asyncio.run(main(questions))\n\n    for r in resp:\n        print(\"q:\", r[\"question\"])\n        #&gt; q: what did I do on Monday?\n        print(\"c:\", r[\"classification\"])\n        #&gt; c: ['SUMMARY']\n</code></pre> <p>If you follow what we've done is wrapped the client and proceeded to quickly use asyncio to classify a list of questions. This is a simple example of how you can use LangSmith to enhance the OpenAI client. You can use LangSmith to monitor and observe the client, and use <code>instructor</code> to add additional functionality to the client.</p> <p>To take a look at trace of this run check out this shareable link.</p>"},{"location":"hub/llama-cpp-python/","title":"Structured Outputs with llama-cpp-python","text":"<p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug llama-cpp-python --py &gt; llama_cpp_python_example.py\n</code></pre> <p>Open-source LLMS are gaining popularity, and llama-cpp-python has made the <code>llama-cpp</code> model available to obtain structured outputs using JSON schema via a mixture of constrained sampling and speculative decoding. They also support a OpenAI compatible client, which can be used to obtain structured output as a in process mechanism to avoid any network dependency.</p>","tags":["patching"]},{"location":"hub/llama-cpp-python/#patching","title":"Patching","text":"<p>Instructor's patch enhances an create call it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page. If you want to check out examples of using Pydantic with Instructor, visit the examples page.</p>","tags":["patching"]},{"location":"hub/llama-cpp-python/#llama-cpp-python","title":"llama-cpp-python","text":"<p>Recently llama-cpp-python added support for structured outputs via JSON schema mode. This is a time-saving alternative to extensive prompt engineering and can be used to obtain structured outputs.</p> <p>In this example we'll cover a more advanced use case of JSON_SCHEMA mode to stream out partial models. To learn more partial streaming check out partial streaming.</p> <pre><code>import llama_cpp\nfrom llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n\nimport instructor\n\nfrom pydantic import BaseModel\nfrom typing import List\nfrom rich.console import Console\n\n\nllama = llama_cpp.Llama(\n    model_path=\"../../models/OpenHermes-2.5-Mistral-7B-GGUF/openhermes-2.5-mistral-7b.Q4_K_M.gguf\",\n    n_gpu_layers=-1,\n    chat_format=\"chatml\",\n    n_ctx=2048,\n    draft_model=LlamaPromptLookupDecoding(num_pred_tokens=2),  # (1)!\n    logits_all=True,\n    verbose=False,\n)\n\n\ncreate = instructor.patch(\n    create=llama.create_chat_completion_openai_v1,\n    mode=instructor.Mode.JSON_SCHEMA,  # (2)!\n)\n\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss\nthe upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024,\nat the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher,\nwill be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities.\nEach participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nextraction_stream = create(\n    response_model=instructor.Partial[MeetingInfo],  # (3)!\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)\n\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()  # (4)!\n    console.print(obj)\n</code></pre> <p>We use LlamaPromptLookupDecoding to speed up structured output generation using speculative decoding. The draft model generates candidate tokens during generation 10 is good for GPU, 2 is good for CPU. 2. We use <code>instructor.Mode.JSON_SCHEMA</code> return a JSON schema response. 3. We use <code>instructor.Partial</code> to stream out partial models. 4. This is just a simple example of how to stream out partial models and clear the console.</p> <p></p>","tags":["patching"]},{"location":"hub/multiple_classification/","title":"Multiple Classification Model","text":"<p>If you want to try outs via <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug multiple_classification --py &gt; multiple_classification.py\n</code></pre> <p>For multi-label classification, we introduce a new enum class and a different Pydantic model to handle multiple labels.</p> <pre><code>import openai\nimport instructor\n\nfrom typing import List, Literal\nfrom pydantic import BaseModel, Field\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(openai.OpenAI())\n\nLABELS = Literal[\"ACCOUNT\", \"BILLING\", \"GENERAL_QUERY\"]\n\n\nclass MultiClassPrediction(BaseModel):\n    labels: List[LABELS] = Field(\n        ...,\n        description=\"Only select the labels that apply to the support ticket.\",\n    )\n\n\ndef multi_classify(data: str) -&gt; MultiClassPrediction:\n    return client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",  # gpt-3.5-turbo fails\n        response_model=MultiClassPrediction,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You are a support agent at a tech company. Only select the labels that apply to the support ticket.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following support ticket: {data}\",\n            },\n        ],\n    )  # type: ignore\n\n\nif __name__ == \"__main__\":\n    ticket = \"My account is locked and I can't access my billing info.\"\n    prediction = multi_classify(ticket)\n    assert {\"ACCOUNT\", \"BILLING\"} == {label for label in prediction.labels}\n    print(\"input:\", ticket)\n    #&gt; input: My account is locked and I can't access my billing info.\n    print(\"labels:\", LABELS)\n    #&gt; labels: typing.Literal['ACCOUNT', 'BILLING', 'GENERAL_QUERY']\n    print(\"prediction:\", prediction)\n    #&gt; prediction: labels=['ACCOUNT', 'BILLING']\n</code></pre>"},{"location":"hub/ollama/","title":"Structured Outputs with Ollama","text":"<p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug ollama --py &gt; ollama_example.py\n</code></pre> <p>Open-source LLMS are gaining popularity, and the release of Ollama's OpenAI compatibility later it has made it possible to obtain structured outputs using JSON schema.</p> <p>By the end of this blog post, you will learn how to effectively utilize instructor with ollama. But before we proceed, let's first explore the concept of patching.</p>","tags":["patching","open source"]},{"location":"hub/ollama/#patching","title":"Patching","text":"<p>Instructor's patch enhances a openai api it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>","tags":["patching","open source"]},{"location":"hub/ollama/#ollama","title":"Ollama","text":"<p>Start by downloading Ollama, and then pull a model such as Llama 2 or Mistral.</p> <p>Make sure you update your <code>ollama</code> to the latest version!</p> <pre><code>ollama pull llama2\n</code></pre> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nimport instructor\n\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    fact: List[str] = Field(..., description=\"A list of facts about the character\")\n\n\n# enables `response_model` in create call\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nresp = client.chat.completions.create(\n    model=\"llama2\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me about the Harry Potter\",\n        }\n    ],\n    response_model=Character,\n)\nprint(resp.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Harry James Potter\",\n  \"age\": 37,\n  \"fact\": [\n    \"He is the chosen one.\",\n    \"He has a lightning-shaped scar on his forehead.\",\n    \"He is the son of James and Lily Potter.\",\n    \"He attended Hogwarts School of Witchcraft and Wizardry.\",\n    \"He is a skilled wizard and sorcerer.\",\n    \"He fought against Lord Voldemort and his followers.\",\n    \"He has a pet owl named Snowy.\"\n  ]\n}\n\"\"\"\n</code></pre>","tags":["patching","open source"]},{"location":"hub/pandas_df/","title":"Extracting directly to a DataFrame","text":"<p>You can pull this example into your IDE by running the following command:</p> <pre><code>instructor hub pull --slug pandas_df --py &gt; pandas_df.py\n</code></pre> <p>In this example we'll show you how to extract directly to a <code>pandas.DataFrame</code></p> <pre><code>from io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import (\n    BaseModel,\n    BeforeValidator,\n    PlainSerializer,\n    InstanceOf,\n    WithJsonSchema,\n)\nimport pandas as pd\nimport instructor\nimport openai\n\n\ndef md_to_df(data: Any) -&gt; Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    # Validates final type\n    InstanceOf[pd.DataFrame],\n    # Converts markdown to DataFrame\n    BeforeValidator(md_to_df),\n    # Converts DataFrame to markdown on model_dump_json\n    PlainSerializer(lambda df: df.to_markdown()),\n    # Adds a description to the type\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"\"\"\n            The markdown representation of the table,\n            each one should be tidy, do not try to join\n            tables that should be seperate\"\"\",\n        }\n    ),\n]\n\nclient = instructor.patch(openai.OpenAI())\n\n\ndef extract_df(data: str) -&gt; pd.DataFrame:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MarkdownDataFrame,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a data extraction system, table of writing perfectly formatted markdown tables.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Extract the data into a table: {data}\",\n            },\n        ],\n    )\n\n\nclass Table(BaseModel):\n    title: str\n    data: MarkdownDataFrame\n\n\ndef extract_table(data: str) -&gt; Table:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Table,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a data extraction system, table of writing perfectly formatted markdown tables.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Extract the data into a table: {data}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    df = extract_df(\n        \"\"\"Create a table of the last 5 presidents of the United States,\n        including their party and the years they served.\"\"\"\n    )\n    assert isinstance(df, pd.DataFrame)\n    print(df)\n    \"\"\"\n                        Party           Years Served\n     President\n    Joe Biden                Democratic          2021-\n    Donald Trump             Republican      2017-2021\n    Barack Obama             Democratic      2009-2017\n    George W. Bush           Republican      2001-2009\n    Bill Clinton             Democratic      1993-2001\n    \"\"\"\n\n    table = extract_table(\n        \"\"\"Create a table of the last 5 presidents of the United States,\n        including their party and the years they served.\"\"\"\n    )\n    assert isinstance(table, Table)\n    assert isinstance(table.data, pd.DataFrame)\n    print(table.title)\n    #&gt; Last 5 Presidents of the United States\n    print(table.data)\n    \"\"\"\n                         Party    Years Served\n     President\n    Joe Biden          Democrat  2021 - Present\n    Donald Trump     Republican     2017 - 2021\n    Barack Obama       Democrat     2009 - 2017\n    George W. Bush   Republican     2001 - 2009\n    Bill Clinton       Democrat     1993 - 2001\n    \"\"\"\n</code></pre> <p>Notice that you can extract both the raw <code>MarkdownDataFrame</code> or a more complex structure like <code>Table</code> which includes a title and the data as a DataFrame. You can even request <code>Iterable[Table]</code> to get multiple tables in a single response!</p>"},{"location":"hub/partial_streaming/","title":"Streaming Partial Responses","text":"<p>Field level streaming provides incremental snapshots of the current state of the response model that are immediately useable. This approach is particularly relevant in contexts like rendering UI components.</p> <p>Instructor supports this pattern by making use of <code>Partial[T]</code>. This lets us dynamically create a new class that treats all of the original model's fields as <code>Optional</code>.</p> <p>If you want to try outs via <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug partial_streaming --py &gt; partial_streaming.py\n</code></pre> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\n\nclient = instructor.patch(OpenAI())\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nPartialMeetingInfo = instructor.Partial[MeetingInfo]\n\n\nextraction_stream = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=PartialMeetingInfo,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)  # type: ignore\n\n\nfrom rich.console import Console\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n</code></pre>"},{"location":"hub/single_classification/","title":"Single-Label Classification","text":"<p>IF you want to try this code with <code>instructor hub</code> you can pull it by running</p> <pre><code>instructor hub pull --slug single_classification --py &gt; single_classification.py\n</code></pre> <p>This example demonstrates how to perform single-label classification using the OpenAI API. The example uses the <code>gpt-3.5-turbo</code> model to classify text as either <code>SPAM</code> or <code>NOT_SPAM</code>.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Literal\nfrom openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.patch(OpenAI())\n\n\nclass ClassificationResponse(BaseModel):\n    label: Literal[\"SPAM\", \"NOT_SPAM\"] = Field(\n        ...,\n        description=\"The predicted class label.\",\n    )\n\n\ndef classify(data: str) -&gt; ClassificationResponse:\n    \"\"\"Perform single-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=ClassificationResponse,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following text: {data}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    for text, label in [\n        (\"Hey Jason! You're awesome\", \"NOT_SPAM\"),\n        (\"I am a nigerian prince and I need your help.\", \"SPAM\"),\n    ]:\n        prediction = classify(text)\n        assert prediction.label == label\n        print(f\"Text: {text}, Predicted Label: {prediction.label}\")\n        #&gt; Text: Hey Jason! You're awesome, Predicted Label: NOT_SPAM\n        #&gt; Text: I am a nigerian prince and I need your help., Predicted Label: SPAM\n</code></pre>"},{"location":"hub/tables_from_vision/","title":"Extracting Tables from Images with OpenAI's GPT-4 Vision Model","text":"<p>First, we define a custom type, <code>MarkdownDataFrame</code>, to handle pandas DataFrames formatted in markdown. This type uses Python's <code>Annotated</code> and <code>InstanceOf</code> types, along with decorators <code>BeforeValidator</code> and <code>PlainSerializer</code>, to process and serialize the data.</p>"},{"location":"hub/tables_from_vision/#defining-the-table-class","title":"Defining the Table Class","text":"<p>The <code>Table</code> class is essential for organizing the extracted data. It includes a caption and a dataframe, processed as a markdown table. Since most of the complexity is handled by the <code>MarkdownDataFrame</code> type, the <code>Table</code> class is straightforward!</p> <p>This requires additional dependencies <code>pip install pandas tabulate</code>.</p> <pre><code>from openai import OpenAI\nfrom io import StringIO\nfrom typing import Annotated, Any, List\nfrom pydantic import (\n    BaseModel,\n    BeforeValidator,\n    PlainSerializer,\n    InstanceOf,\n    WithJsonSchema,\n)\nimport instructor\nimport pandas as pd\n\n\nclient = instructor.patch(OpenAI(), mode=instructor.function_calls.Mode.MD_JSON)\n\n\ndef md_to_df(data: Any) -&gt; Any:\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Get rid of whitespaces\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .map(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    InstanceOf[pd.DataFrame],\n    BeforeValidator(md_to_df),\n    PlainSerializer(lambda x: x.to_markdown()),\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"\"\"\n                The markdown representation of the table,\n                each one should be tidy, do not try to join tables\n                that should be seperate\"\"\",\n        }\n    ),\n]\n\n\nclass Table(BaseModel):\n    caption: str\n    dataframe: MarkdownDataFrame\n\n\nclass MultipleTables(BaseModel):\n    tables: List[Table]\n\n\nexample = MultipleTables(\n    tables=[\n        Table(\n            caption=\"This is a caption\",\n            dataframe=pd.DataFrame(\n                {\n                    \"Chart A\": [10, 40],\n                    \"Chart B\": [20, 50],\n                    \"Chart C\": [30, 60],\n                }\n            ),\n        )\n    ]\n)\n\n\ndef extract(url: str) -&gt; MultipleTables:\n    tables = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        max_tokens=4000,\n        response_model=MultipleTables,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": f\"Describe this data accurately as a table in markdown format. {example.model_dump_json(indent=2)}\",\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": url},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"\"\"\n                            First take a moment to reason about the best set of headers for the tables.\n                            Write a good h1 for the image above. Then follow up with a short description of the what the data is about.\n                            Then for each table you identified, write a h2 tag that is a descriptive title of the table.\n                            Then follow up with a short description of the what the data is about.\n                            Lastly, produce the markdown table for each table you identified.\n                            Make sure to escape the markdown table properly, and make sure to include the caption and the dataframe.\n                            including escaping all the newlines and quotes. Only return a markdown table in dataframe, nothing else.\n                        \"\"\",\n                    },\n                ],\n            }\n        ],\n    )\n    return tables\n\n\nif __name__ == \"__main__\":\n    urls = [\n        \"https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png/m/2880x0\",\n    ]\n    for url in urls:\n        tables = extract(url)\n        for table in tables.tables:\n            print(table.caption)\n            #&gt; Top 10 Grossing Android Apps\n            \"\"\"\n                        App Name                    Category\n             Rank\n            1                           Google One       Productivity\n            2                              Disney+      Entertainment\n            3        TikTok - Videos, Music &amp; LIVE      Entertainment\n            4                     Candy Crush Saga              Games\n            5       Tinder: Dating, Chat &amp; Friends  Social networking\n            6                          Coin Master              Games\n            7                               Roblox              Games\n            8       Bumble - Dating &amp; Make Friends             Dating\n            9                          Royal Match              Games\n            10         Spotify: Music and Podcasts      Music &amp; Audio\n            \"\"\"\n            print(table.dataframe)\n            \"\"\"\n                        App Name                    Category\n             Rank\n            1       Tinder: Dating, Chat &amp; Friends  Social networking\n            2                              Disney+      Entertainment\n            3       YouTube: Watch, Listen, Stream      Entertainment\n            4         Audible: Audio Entertainment      Entertainment\n            5                     Candy Crush Saga              Games\n            6        TikTok - Videos, Music &amp; LIVE      Entertainment\n            7       Bumble - Dating &amp; Make Friends             Dating\n            8                               Roblox              Games\n            9          LinkedIn: Job Search &amp; News           Business\n            10         Duolingo - Language Lessons          Education\n            \"\"\"\n</code></pre>"},{"location":"hub/together/","title":"Structured Outputs with Together AI","text":"<p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug together --py &gt; together_example.py\n</code></pre> <p>Open-source LLMS are gaining popularity, and with the release of Together's Function calling models, its been easier than ever to get structured outputs.</p> <p>By the end of this blog post, you will learn how to effectively utilize instructor with Together AI. But before we proceed, let's first explore the concept of patching.</p> <p>Other Languages</p> <p>This blog post is written in Python, but the concepts are applicable to other languages as well, as we currently have support for Javascript and Elixir</p>","tags":["patching","open source"]},{"location":"hub/together/#patching","title":"Patching","text":"<p>Instructor's patch enhances the openai api it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>","tags":["patching","open source"]},{"location":"hub/together/#together-ai","title":"Together AI","text":"<p>The good news is that Together employs the same OpenAI client, and its models support some of these output modes too!</p> <p>Getting access</p> <p>If you want to try this out for yourself check out the Together AI website. You can get started here.</p> <pre><code>import os\nimport openai\nfrom pydantic import BaseModel\nimport instructor\n\nclient = openai.OpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n)\n\n\n# By default, the patch function will patch the ChatCompletion.create and ChatCompletion.create methods to support the response_model parameter\nclient = instructor.patch(client, mode=instructor.Mode.TOOLS)\n\n\n# Now, we can use the response_model parameter using only a base model\n# rather than having to use the OpenAISchema class\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert isinstance(user, UserExtract), \"Should be instance of UserExtract\"\nassert user.name.lower() == \"jason\"\nassert user.age == 25\n\nprint(user.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 25\n}\n\"\"\"\n{\n    \"name\": \"Jason\",\n    \"age\": 25,\n}\n</code></pre> <p>You can find more information about Together's function calling support here.</p>","tags":["patching","open source"]},{"location":"tutorials/1-introduction/","title":"Tutorials (Notebooks)","text":"In\u00a0[1]: Copied! <pre>data = [{\"first_name\": \"Jason\", \"age\": 10}, {\"firstName\": \"Jason\", \"age\": \"10\"}]\n</pre> data = [{\"first_name\": \"Jason\", \"age\": 10}, {\"firstName\": \"Jason\", \"age\": \"10\"}] <p>We have a <code>name</code> field, which is a string, and an <code>age</code> field, which is an integer. However, if we were to load this into a dictionary, we would have no way of knowing if the data is valid. For example, we could have a string for the age, or we could have a float for the age. We could also have a string for the name, or we could have a list for the name.</p> In\u00a0[2]: Copied! <pre>for obj in data:\n    name = obj.get(\"first_name\")\n    age = obj.get(\"age\")\n    print(f\"{name} is {age}\")\n\nfor obj in data:\n    name = obj.get(\"first_name\")\n    age = obj.get(\"age\")\n    print(f\"Next year {name} will be {age+1} years old\")\n</pre> for obj in data:     name = obj.get(\"first_name\")     age = obj.get(\"age\")     print(f\"{name} is {age}\")  for obj in data:     name = obj.get(\"first_name\")     age = obj.get(\"age\")     print(f\"Next year {name} will be {age+1} years old\") <pre>Jason is 10\nNone is 10\nNext year Jason will be 11 years old\n</pre> <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/docs/tutorials/1-introduction.ipynb Cell 5 line 9\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/1-introduction.ipynb#W4sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt; name = obj.get(\"first_name\")\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/1-introduction.ipynb#W4sZmlsZQ%3D%3D?line=7'&gt;8&lt;/a&gt; age = obj.get(\"age\")\n----&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/1-introduction.ipynb#W4sZmlsZQ%3D%3D?line=8'&gt;9&lt;/a&gt; print(f\"Next year {name} will be {age+1} years old\")\n\nTypeError: can only concatenate str (not \"int\") to str</pre> <p>You see that while we were able to program with a dictionary, we had issues with the data being valid. We would have had to manually check the types of the data, and we had to manually check if the data was valid. This is a pain, and we can do better.</p> In\u00a0[3]: Copied! <pre>from pydantic import BaseModel, Field\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nperson = Person(name=\"Sam\", age=30)\nperson\n</pre> from pydantic import BaseModel, Field   class Person(BaseModel):     name: str     age: int   person = Person(name=\"Sam\", age=30) person Out[3]: <pre>Person(name='Sam', age=30)</pre> In\u00a0[4]: Copied! <pre># Data is correctly casted to the right type\nperson = Person.model_validate({\"name\": \"Sam\", \"age\": \"30\"})\nperson\n</pre> # Data is correctly casted to the right type person = Person.model_validate({\"name\": \"Sam\", \"age\": \"30\"}) person Out[4]: <pre>Person(name='Sam', age=30)</pre> In\u00a0[5]: Copied! <pre>assert person.name == \"Sam\"\nassert person.age == 20\n</pre> assert person.name == \"Sam\" assert person.age == 20 <pre>\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/docs/tutorials/1-introduction.ipynb Cell 10 line 2\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/1-introduction.ipynb#X12sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; assert person.name == \"Sam\"\n----&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/1-introduction.ipynb#X12sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt; assert person.age == 20\n\nAssertionError: </pre> In\u00a0[6]: Copied! <pre># Data is validated to get better error messages\nperson = Person.model_validate({\"first_name\": \"Sam\", \"age\": \"30.2\"})\nperson\n</pre> # Data is validated to get better error messages person = Person.model_validate({\"first_name\": \"Sam\", \"age\": \"30.2\"}) person <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/docs/tutorials/1-introduction.ipynb Cell 11 line 2\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/1-introduction.ipynb#X13sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; # Data is validated to get better error messages\n----&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/1-introduction.ipynb#X13sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt; person = Person.model_validate({\"first_name\": \"Sam\", \"age\": \"30.2\"})\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/1-introduction.ipynb#X13sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt; person\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:509, in BaseModel.model_validate(cls, obj, strict, from_attributes, context)\n    507 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    508 __tracebackhide__ = True\n--&gt; 509 return cls.__pydantic_validator__.validate_python(\n    510     obj, strict=strict, from_attributes=from_attributes, context=context\n    511 )\n\nValidationError: 2 validation errors for Person\nname\n  Field required [type=missing, input_value={'first_name': 'Sam', 'age': '30.2'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/missing\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='30.2', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/int_parsing</pre> <p>By introducing pydantic into any python codebase you can get a lot of benefits. You can get type checking, you can get validation, and you can get autocomplete. This is a huge win, because it means you can catch errors before they happen. This is even more useful when we rely on language models to generate data for us.</p> <p>You can also define validators that are run on the data. This is useful because it means you can catch errors before they happen. For example, you can define a validator that checks if the age is greater than 0. This is useful because it means you can catch errors before they happen.</p> In\u00a0[7]: Copied! <pre>from openai import OpenAI\n\nclient = OpenAI()\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Please give me jason is 10 as a json object ```json\\n\"},\n    ],\n    n=10,\n    temperature=1,\n)\n\nfor choice in resp.choices:\n    json = choice.message.content\n    try:\n        person = Person.model_validate_json(json)\n        print(f\"correctly parsed {person=}\")\n    except Exception as e:\n        print(\"error!!\")\n        print(json)\n</pre> from openai import OpenAI  client = OpenAI()  resp = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     messages=[         {\"role\": \"user\", \"content\": \"Please give me jason is 10 as a json object ```json\\n\"},     ],     n=10,     temperature=1, )  for choice in resp.choices:     json = choice.message.content     try:         person = Person.model_validate_json(json)         print(f\"correctly parsed {person=}\")     except Exception as e:         print(\"error!!\")         print(json) <pre>correctly parsed person=Person(name='Jason', age=10)\ncorrectly parsed person=Person(name='Jason', age=10)\ncorrectly parsed person=Person(name='Jason', age=10)\ncorrectly parsed person=Person(name='Jason', age=10)\ncorrectly parsed person=Person(name='Jason', age=10)\ncorrectly parsed person=Person(name='Jason', age=10)\nerror!!\n{\"jason\": 10}\ncorrectly parsed person=Person(name='Jason', age=10)\ncorrectly parsed person=Person(name='Jason', age=10)\ncorrectly parsed person=Person(name='Jason', age=10)\n</pre> In\u00a0[8]: Copied! <pre>import datetime\n\n\nclass PersonBirthday(BaseModel):\n    name: str\n    age: int\n    birthday: datetime.date\n\n\nschema = {\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"age\": {\"type\": \"integer\"},\n        \"birthday\": {\"type\": \"string\", \"format\": \"YYYY-MM-DD\"},\n    },\n    \"required\": [\"name\", \"age\"],\n    \"type\": \"object\",\n}\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Extract `Jason Liu is thirty years old his birthday is yesturday` into json today is {datetime.date.today()}\",\n        },\n    ],\n    functions=[{\"name\": \"Person\", \"parameters\": schema}],\n    function_call=\"auto\",\n)\n\nPersonBirthday.model_validate_json(resp.choices[0].message.function_call.arguments)\n</pre> import datetime   class PersonBirthday(BaseModel):     name: str     age: int     birthday: datetime.date   schema = {     \"properties\": {         \"name\": {\"type\": \"string\"},         \"age\": {\"type\": \"integer\"},         \"birthday\": {\"type\": \"string\", \"format\": \"YYYY-MM-DD\"},     },     \"required\": [\"name\", \"age\"],     \"type\": \"object\", }  resp = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     messages=[         {             \"role\": \"user\",             \"content\": f\"Extract `Jason Liu is thirty years old his birthday is yesturday` into json today is {datetime.date.today()}\",         },     ],     functions=[{\"name\": \"Person\", \"parameters\": schema}],     function_call=\"auto\", )  PersonBirthday.model_validate_json(resp.choices[0].message.function_call.arguments) Out[8]: <pre>PersonBirthday(name='Jason Liu', age=30, birthday=datetime.date(2024, 2, 8))</pre> <p>But it turns out, pydantic actually not only does our serialization, we can define the schema as well as add additional documentation!</p> In\u00a0[9]: Copied! <pre>PersonBirthday.model_json_schema()\n</pre> PersonBirthday.model_json_schema() Out[9]: <pre>{'properties': {'name': {'title': 'Name', 'type': 'string'},\n  'age': {'title': 'Age', 'type': 'integer'},\n  'birthday': {'format': 'date', 'title': 'Birthday', 'type': 'string'}},\n 'required': ['name', 'age', 'birthday'],\n 'title': 'PersonBirthday',\n 'type': 'object'}</pre> <p>We can even define nested complex schemas, and documentation with ease.</p> In\u00a0[10]: Copied! <pre>class Address(BaseModel):\n    address: str = Field(description=\"Full street address\")\n    city: str\n    state: str\n\n\nclass PersonAddress(Person):\n    \"\"\"A Person with an address\"\"\"\n\n    address: Address\n\n\nPersonAddress.model_json_schema()\n</pre> class Address(BaseModel):     address: str = Field(description=\"Full street address\")     city: str     state: str   class PersonAddress(Person):     \"\"\"A Person with an address\"\"\"      address: Address   PersonAddress.model_json_schema() Out[10]: <pre>{'$defs': {'Address': {'properties': {'address': {'description': 'Full street address',\n     'title': 'Address',\n     'type': 'string'},\n    'city': {'title': 'City', 'type': 'string'},\n    'state': {'title': 'State', 'type': 'string'}},\n   'required': ['address', 'city', 'state'],\n   'title': 'Address',\n   'type': 'object'}},\n 'description': 'A Person with an address',\n 'properties': {'name': {'title': 'Name', 'type': 'string'},\n  'age': {'title': 'Age', 'type': 'integer'},\n  'address': {'$ref': '#/$defs/Address'}},\n 'required': ['name', 'age', 'address'],\n 'title': 'PersonAddress',\n 'type': 'object'}</pre> <p>These simple concepts become what we built into <code>instructor</code> and most of the work has been around documenting how we can leverage schema engineering. Except now we use <code>instructor.patch()</code> to add a bunch more capabilities to the OpenAI SDK.</p> In\u00a0[11]: Copied! <pre>import instructor\nimport datetime\n\n# patch the client to add `response_model` to the `create` method\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"\n            Today is {datetime.date.today()} \n\n            Extract `Jason Liu is thirty years old his birthday is yesturday` \n            he lives at 123 Main St, San Francisco, CA\"\"\",\n        },\n    ],\n    response_model=PersonAddress,\n)\nresp\n</pre> import instructor import datetime  # patch the client to add `response_model` to the `create` method client = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)  resp = client.chat.completions.create(     model=\"gpt-3.5-turbo-1106\",     messages=[         {             \"role\": \"user\",             \"content\": f\"\"\"             Today is {datetime.date.today()}               Extract `Jason Liu is thirty years old his birthday is yesturday`              he lives at 123 Main St, San Francisco, CA\"\"\",         },     ],     response_model=PersonAddress, ) resp Out[11]: <pre>PersonAddress(name='Jason Liu', age=30, address=Address(address='123 Main St', city='San Francisco', state='CA'))</pre> <p>By defining <code>response_model</code> we can leverage pydantic to do all the heavy lifting. Later we'll introduce the other features that <code>instructor.patch()</code> adds to the OpenAI SDK. but for now, this small change allows us to do a lot more with the API.</p>"},{"location":"tutorials/1-introduction/#working-with-structured-outputs","title":"Working with structured outputs\u00b6","text":"<p>If you've seen my talk on this topic, you can skip this chapter.</p> <p>tl;dr</p> <p>When we work with LLMs you find that many times we are not building chatbots, instead we're working with structured outputs in order to solve a problem by returning machine readable data. However the way we think about the problem is still very much influenced by the way we think about chatbots. This is a problem because it leads to a lot of confusion and frustration. In this chapter we'll try to understand why this happens and how we can fix it.</p>"},{"location":"tutorials/1-introduction/#the-fundamental-problem-with-json-and-dictionaries","title":"The fundamental problem with JSON and Dictionaries\u00b6","text":"<p>Lets say we have a simple JSON object, and we want to work with it. We can use the <code>json</code> module to load it into a dictionary, and then work with it. However, this is a bit of a pain, because we have to manually check the types of the data, and we have to manually check if the data is valid. For example, lets say we have a JSON object that looks like this:</p>"},{"location":"tutorials/1-introduction/#pydantic-to-the-rescue","title":"Pydantic to the rescue\u00b6","text":"<p>Pydantic is a library that allows us to define data structures, and then validate them.</p>"},{"location":"tutorials/1-introduction/#fundamental-problem-with-asking-for-json-from-openai","title":"Fundamental problem with asking for JSON from OpenAI\u00b6","text":""},{"location":"tutorials/1-introduction/#introduction-to-function-calling","title":"Introduction to Function Calling\u00b6","text":"<p>The json could be anything! We could add more and more into a prompt and hope it works, or we can use something called function calling to directly specify the schema we want.</p> <p>Function Calling</p> <p>In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call one or many functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code.</p>"},{"location":"tutorials/1-introduction/#the-core-idea-around-instructor","title":"The core idea around Instructor\u00b6","text":"<ol> <li>Using function calling allows us use a llm that is finetuned to use json_schema and output json.</li> <li>Pydantic can be used to define the object, schema, and validation in one single class, allow us to encapsulate everything neatly</li> <li>As a library with 100M downloads, we can leverage pydantic to do all the heavy lifting for us and fit nicely with the python ecosystem</li> </ol>"},{"location":"tutorials/1-introduction/#is-instructor-the-only-way-to-do-this","title":"Is instructor the only way to do this?\u00b6","text":"<p>No. Libraries like Marvin, Langchain, and Llamaindex all now leverage the Pydantic object in similar ways. The goal is to be as light weight as possible, get you as close as possible to the openai api, and then get out of your way.</p> <p>More importantly, we've also added straight forward validation and reasking to the mix.</p> <p>The goal of instructor is to show you how to think about structured prompting and provide examples and documentation that you can take with you to any framework.</p> <p>For further exploration:</p> <ul> <li>Marvin</li> <li>Langchain</li> <li>LlamaIndex</li> </ul>"},{"location":"tutorials/2-tips/","title":"Tips and Tricks","text":"In\u00a0[1]: Copied! <pre>import instructor\nfrom openai import OpenAI\n\nfrom enum import Enum\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import Literal\n\n\nclient = instructor.patch(OpenAI())\n\n\n# Tip: Do not use auto() as they cast to 1,2,3,4\nclass House(Enum):\n    Gryffindor = \"gryffindor\"\n    Hufflepuff = \"hufflepuff\"\n    Ravenclaw = \"ravenclaw\"\n    Slytherin = \"slytherin\"\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: House\n\n    def say_hello(self):\n        print(\n            f\"Hello, I'm {self.name}, I'm {self.age} years old and I'm from {self.house.value.title()}\"\n        )\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n</pre> import instructor from openai import OpenAI  from enum import Enum from pydantic import BaseModel, Field from typing_extensions import Literal   client = instructor.patch(OpenAI())   # Tip: Do not use auto() as they cast to 1,2,3,4 class House(Enum):     Gryffindor = \"gryffindor\"     Hufflepuff = \"hufflepuff\"     Ravenclaw = \"ravenclaw\"     Slytherin = \"slytherin\"   class Character(BaseModel):     age: int     name: str     house: House      def say_hello(self):         print(             f\"Hello, I'm {self.name}, I'm {self.age} years old and I'm from {self.house.value.title()}\"         )   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],     response_model=Character, ) resp.model_dump() Out[1]: <pre>{'age': 17, 'name': 'Harry Potter', 'house': &lt;House.Gryffindor: 'gryffindor'&gt;}</pre> In\u00a0[2]: Copied! <pre>resp.say_hello()\n</pre> resp.say_hello() <pre>Hello, I'm Harry Potter, I'm 17 years old and I'm from Gryffindor\n</pre> In\u00a0[3]: Copied! <pre>class Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n</pre> class Character(BaseModel):     age: int     name: str     house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],     response_model=Character, ) resp.model_dump() Out[3]: <pre>{'age': 11, 'name': 'Harry Potter', 'house': 'Gryffindor'}</pre> In\u00a0[4]: Copied! <pre>from typing import List\n\n\nclass Property(BaseModel):\n    key: str = Field(description=\"Must be snake case\")\n    value: str\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n    properties: List[Property]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n</pre> from typing import List   class Property(BaseModel):     key: str = Field(description=\"Must be snake case\")     value: str   class Character(BaseModel):     age: int     name: str     house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]     properties: List[Property]   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],     response_model=Character, ) resp.model_dump() Out[4]: <pre>{'age': 38,\n 'name': 'Severus Snape',\n 'house': 'Slytherin',\n 'properties': [{'key': 'role', 'value': 'Potions Master'},\n  {'key': 'patronus', 'value': 'Doe'},\n  {'key': 'loyalty', 'value': 'Dumbledore'},\n  {'key': 'played_by', 'value': 'Alan Rickman'}]}</pre> In\u00a0[5]: Copied! <pre>class Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str = Field(description=\"Must be snake case\")\n    value: str\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n    properties: List[Property] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be exactly 5\",\n    )\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n</pre> class Property(BaseModel):     index: str = Field(..., description=\"Monotonically increasing ID\")     key: str = Field(description=\"Must be snake case\")     value: str   class Character(BaseModel):     age: int     name: str     house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]     properties: List[Property] = Field(         ...,         description=\"Numbered list of arbitrary extracted properties, should be exactly 5\",     )   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],     response_model=Character, ) resp.model_dump() Out[5]: <pre>{'age': 38,\n 'name': 'Severus Snape',\n 'house': 'Slytherin',\n 'properties': [{'index': '1',\n   'key': 'position_at_hogwarts',\n   'value': 'Potions Master'},\n  {'index': '2', 'key': 'patronus_form', 'value': 'Doe'},\n  {'index': '3', 'key': 'loyalty', 'value': 'Albus Dumbledore'},\n  {'index': '4', 'key': 'played_by', 'value': 'Alan Rickman'},\n  {'index': '5', 'key': 'final_act', 'value': 'Protecting Harry Potter'}]}</pre> In\u00a0[6]: Copied! <pre>from typing import Iterable\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n</pre> from typing import Iterable   class Character(BaseModel):     age: int     name: str     house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],     response_model=Iterable[Character], )  for character in resp:     print(character) <pre>age=11 name='Harry Potter' house='Gryffindor'\nage=11 name='Hermione Granger' house='Gryffindor'\nage=11 name='Ron Weasley' house='Gryffindor'\nage=11 name='Draco Malfoy' house='Slytherin'\nage=11 name='Neville Longbottom' house='Gryffindor'\n</pre> In\u00a0[7]: Copied! <pre>from typing import Iterable\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],\n    stream=True,\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n</pre> from typing import Iterable   class Character(BaseModel):     age: int     name: str     house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],     stream=True,     response_model=Iterable[Character], )  for character in resp:     print(character) <pre>age=11 name='Harry Potter' house='Gryffindor'\nage=11 name='Hermione Granger' house='Gryffindor'\nage=11 name='Ron Weasley' house='Gryffindor'\nage=17 name='Draco Malfoy' house='Slytherin'\nage=11 name='Luna Lovegood' house='Ravenclaw'\n</pre> In\u00a0[8]: Copied! <pre>class Character(BaseModel):\n    id: int\n    name: str\n    friends_array: List[int] = Field(description=\"Relationships to their friends using the id\")\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"5 kids from Harry Potter\"}],\n    stream=True,\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n</pre> class Character(BaseModel):     id: int     name: str     friends_array: List[int] = Field(description=\"Relationships to their friends using the id\")   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"5 kids from Harry Potter\"}],     stream=True,     response_model=Iterable[Character], )  for character in resp:     print(character) <pre>id=1 name='Harry Potter' friends_array=[2, 3, 4, 5, 6]\nid=2 name='Hermione Granger' friends_array=[1, 3, 4, 5]\nid=3 name='Ron Weasley' friends_array=[1, 2, 4, 6]\nid=4 name='Neville Longbottom' friends_array=[1, 2, 3, 5]\nid=5 name='Luna Lovegood' friends_array=[1, 2, 4, 6]\nid=6 name='Draco Malfoy' friends_array=[1, 3, 5]\n</pre> <p>With the tools we've discussed, we can find numerous real-world applications in production settings. These include extracting action items from transcripts, generating fake data, filling out forms, and creating objects that correspond to generative UI. These simple tricks will be highly useful.</p> In\u00a0[9]: Copied! <pre>from typing import Optional\n\nclass Character(BaseModel):\n    age: int\n    name: str\n\nclass MaybeCharacter(BaseModel):\n    result: Optional[Character] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n</pre> from typing import Optional  class Character(BaseModel):     age: int     name: str  class MaybeCharacter(BaseModel):     result: Optional[Character] = Field(default=None)     error: bool = Field(default=False)     message: Optional[str] In\u00a0[10]: Copied! <pre>def extract(content: str) -&gt; MaybeCharacter:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MaybeCharacter,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\n        ],\n    )\n</pre> def extract(content: str) -&gt; MaybeCharacter:     return client.chat.completions.create(         model=\"gpt-3.5-turbo\",         response_model=MaybeCharacter,         messages=[             {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},         ],     ) In\u00a0[11]: Copied! <pre>extract(\"Harry Potter\")\n</pre> extract(\"Harry Potter\") Out[11]: <pre>MaybeCharacter(result=Character(age=17, name='Harry Potter'), error=False, message=None)</pre> In\u00a0[12]: Copied! <pre>user = extract(\"404 Error\")\n\nif user.error:\n    raise ValueError(user.message)\n</pre> user = extract(\"404 Error\")  if user.error:     raise ValueError(user.message) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb Cell 20 line 4\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb#X25sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; user = extract(\"404 Error\")\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb#X25sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt; if user.error:\n----&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb#X25sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt;     raise ValueError(user.message)\n\nValueError: 404 Error</pre>"},{"location":"tutorials/2-tips/#general-tips-on-prompting","title":"General Tips on Prompting\u00b6","text":"<p>Before we get into some big applications of schema engineering I want to equip you with the tools for success. This notebook is to share some general advice when using prompts to get the most of your models.</p> <p>Before you might think of prompt engineering as massaging this wall of text, almost like coding in a notepad. But with schema engineering you can get a lot more out of your prompts with a lot less work.</p>"},{"location":"tutorials/2-tips/#classification","title":"Classification\u00b6","text":"<p>For classification we've found theres generally two methods of modeling.</p> <ol> <li>using Enums</li> <li>using Literals</li> </ol> <p>Use an enum in Python when you need a set of named constants that are related and you want to ensure type safety, readability, and prevent invalid values. Enums are helpful for grouping and iterating over these constants.</p> <p>Use literals when you have a small, unchanging set of values that you don't need to group or iterate over, and when type safety and preventing invalid values is less of a concern. Literals are simpler and more direct for basic, one-off values.</p>"},{"location":"tutorials/2-tips/#arbitrary-properties","title":"Arbitrary properties\u00b6","text":"<p>Often times there are long properties that you might want to extract from data that we can not specify in advanced. We can get around this by defining an arbitrary key value store like so:</p>"},{"location":"tutorials/2-tips/#limiting-the-length-of-lists","title":"Limiting the length of lists\u00b6","text":"<p>In later chapters we'll talk about how to use validators to assert the length of lists but we can also use prompting tricks to enumerate values. Here we'll define a index to count the properties.</p> <p>In this following example instead of extraction we're going to work on generation instead.</p>"},{"location":"tutorials/2-tips/#defining-multiple-entities","title":"Defining Multiple Entities\u00b6","text":"<p>Now that we see a single entity with many properties we can continue to nest them into many users</p>"},{"location":"tutorials/2-tips/#defining-relationships","title":"Defining Relationships\u00b6","text":"<p>Now only can we define lists of users, with list of properties one of the more interesting things I've learned about prompting is that we can also easily define lists of references.</p>"},{"location":"tutorials/2-tips/#missing-data","title":"Missing Data\u00b6","text":"<p>The Maybe pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning None, you can use a Maybe type to encapsulate both the result and potential errors.</p> <p>This pattern is particularly useful when making LLM calls, as providing language models with an escape hatch can effectively reduce hallucinations.</p>"},{"location":"tutorials/3-0-applications-rag/","title":"Applications RAG","text":"<p>What is RAG?</p> <p>Retrieval Augmented Generation (RAG) models are the bridge between large language models and external knowledge databases. They fetch the relevant data for a given query. For example, if you have some documents and want to ask questions related to the content of those documents, RAG models help by retrieving data from those documents and passing it to the LLM in queries.</p> <p>How do RAG models work?</p> <p>The typical RAG process involves embedding a user query and searching a vector database to find the most relevant information to supplement the generated response. This approach is particularly effective when the database contains information closely matching the query but not more than that.</p> <p></p> <p>Why is there a need for them?</p> <p>Pre-trained large language models do not learn over time. If you ask them a question they have not been trained on, they will often hallucinate. Therefore, we need to embed our own data to achieve a better output.</p> <p>In the examples below, we're going to use the <code>instructor</code> library to simplify the interaction between the programmer and language models via the function-calling API.</p> In\u00a0[1]: Copied! <pre>import instructor\n\nfrom openai import OpenAI\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\nclient = instructor.patch(OpenAI())\n</pre> import instructor  from openai import OpenAI from typing import List from pydantic import BaseModel, Field  client = instructor.patch(OpenAI()) In\u00a0[2]: Copied! <pre>class Extraction(BaseModel):\n    topic: str\n    summary: str\n    hypothetical_questions: List[str] = Field(\n        default_factory=list,\n        description=\"Hypothetical questions that this document could answer\",\n    )\n    keywords: List[str] = Field(\n        default_factory=list, description=\"Keywords that this document is about\"\n    )\n</pre> class Extraction(BaseModel):     topic: str     summary: str     hypothetical_questions: List[str] = Field(         default_factory=list,         description=\"Hypothetical questions that this document could answer\",     )     keywords: List[str] = Field(         default_factory=list, description=\"Keywords that this document is about\"     ) In\u00a0[3]: Copied! <pre>from pprint import pprint\nfrom typing import Iterable\n\n\ntext_chunk = \"\"\"\n## Simple RAG\n\n**What is it?**\n\nThe simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.\n\n**What are the limitations?**\n\n- **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.\n    - Query: \"Tell me about climate change effects on marine life.\"\n    - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics.\n- **Monolithic Search Backend:** It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources.\n    - Query: \"Latest research in quantum computing.\"\n    - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources.\n- **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.\n    - Query: \"what problems did we fix last week\"\n    - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week.\n- **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.\n    - Query: \"Tips for first-time Europe travelers.\"\n    - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations.\n\"\"\"\n\nextractions = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    stream=True,\n    response_model=s,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Your role is to extract chunks from the following and create a set of topics.\",\n        },\n        {\"role\": \"user\", \"content\": text_chunk},\n    ],\n)\n\n\nfor extraction in extractions:\n    pprint(extraction.model_dump())\n</pre> from pprint import pprint from typing import Iterable   text_chunk = \"\"\" ## Simple RAG  **What is it?**  The simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.  **What are the limitations?**  - **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.     - Query: \"Tell me about climate change effects on marine life.\"     - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics. - **Monolithic Search Backend:** It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources.     - Query: \"Latest research in quantum computing.\"     - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources. - **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.     - Query: \"what problems did we fix last week\"     - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week. - **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.     - Query: \"Tips for first-time Europe travelers.\"     - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations. \"\"\"  extractions = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     stream=True,     response_model=s,     messages=[         {             \"role\": \"system\",             \"content\": \"Your role is to extract chunks from the following and create a set of topics.\",         },         {\"role\": \"user\", \"content\": text_chunk},     ], )   for extraction in extractions:     pprint(extraction.model_dump()) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb Cell 10 line 3\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt; from typing import Iterable\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt; text_chunk = \"\"\"\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt; ## Simple RAG\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt; \n   (...)\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=24'&gt;25&lt;/a&gt;     - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations.\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=25'&gt;26&lt;/a&gt; \"\"\"\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=27'&gt;28&lt;/a&gt; extractions = client.chat.completions.create(\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=28'&gt;29&lt;/a&gt;     model=\"gpt-4-1106-preview\",\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=29'&gt;30&lt;/a&gt;     stream=True,\n---&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=30'&gt;31&lt;/a&gt;     response_model=s,\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=31'&gt;32&lt;/a&gt;     messages=[\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=32'&gt;33&lt;/a&gt;         {\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=33'&gt;34&lt;/a&gt;             \"role\": \"system\",\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=34'&gt;35&lt;/a&gt;             \"content\": \"Your role is to extract chunks from the following and create a set of topics.\",\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=35'&gt;36&lt;/a&gt;         },\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=36'&gt;37&lt;/a&gt;         {\"role\": \"user\", \"content\": text_chunk},\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=37'&gt;38&lt;/a&gt;     ],\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=38'&gt;39&lt;/a&gt; )\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=41'&gt;42&lt;/a&gt; for extraction in extractions:\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X12sZmlsZQ%3D%3D?line=42'&gt;43&lt;/a&gt;     pprint(extraction.model_dump())\n\nNameError: name 's' is not defined</pre> <p>Now you can imagine if you were to embed the summaries, hypothetical questions, and keywords in a vector database, you can then use a vector search to find the best matching document for a given query. What you'll find is that the results are much better than if you were to just embed the text chunk!</p> In\u00a0[4]: Copied! <pre>from datetime import date\n\n\nclass DateRange(BaseModel):\n    start: date\n    end: date\n\n\nclass Query(BaseModel):\n    rewritten_query: str\n    published_daterange: DateRange\n</pre> from datetime import date   class DateRange(BaseModel):     start: date     end: date   class Query(BaseModel):     rewritten_query: str     published_daterange: DateRange <p>In this example, <code>DateRange</code> and <code>Query</code> are Pydantic models that structure the user's query with a date range and a list of domains to search within.</p> <p>These models restructure the user's query by including a rewritten query, a range of published dates, and a list of domains to search in.</p> <p>Using the new restructured query, we can apply this pattern to our function calls to obtain results that are optimized for our backend.</p> In\u00a0[5]: Copied! <pre>def expand_query(q) -&gt; Query:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Query,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n            },\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n        ],\n    )\n\n\nquery = expand_query(\"What are some recent developments in AI?\")\nquery\n</pre> def expand_query(q) -&gt; Query:     return client.chat.completions.create(         model=\"gpt-3.5-turbo\",         response_model=Query,         messages=[             {                 \"role\": \"system\",                 \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",             },             {\"role\": \"user\", \"content\": f\"query: {q}\"},         ],     )   query = expand_query(\"What are some recent developments in AI?\") query Out[5]: <pre>Query(rewritten_query='recent developments in AI', published_daterange=DateRange(start=datetime.date(2023, 2, 9), end=datetime.date(2024, 2, 9)))</pre> <p>This isn't just about adding some date ranges. We can even use some chain of thought prompting to generate tailored searches that are deeply integrated with our backend.</p> In\u00a0[6]: Copied! <pre>class DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n    start: date\n    end: date\n\n\nclass Query(BaseModel):\n    rewritten_query: str = Field(\n        description=\"Rewrite the query to make it more specific\"\n    )\n    published_daterange: DateRange = Field(\n        description=\"Effective date range to search in\"\n    )\n\n\ndef expand_query(q) -&gt; Query:\n    return client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        response_model=Query,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n            },\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n        ],\n    )\n\n\nexpand_query(\"What are some recent developments in AI?\")\n</pre> class DateRange(BaseModel):     chain_of_thought: str = Field(         description=\"Think step by step to plan what is the best time range to search in\"     )     start: date     end: date   class Query(BaseModel):     rewritten_query: str = Field(         description=\"Rewrite the query to make it more specific\"     )     published_daterange: DateRange = Field(         description=\"Effective date range to search in\"     )   def expand_query(q) -&gt; Query:     return client.chat.completions.create(         model=\"gpt-4-1106-preview\",         response_model=Query,         messages=[             {                 \"role\": \"system\",                 \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",             },             {\"role\": \"user\", \"content\": f\"query: {q}\"},         ],     )   expand_query(\"What are some recent developments in AI?\") Out[6]: <pre>Query(rewritten_query='Recent developments in Artificial Intelligence', published_daterange=DateRange(chain_of_thought=\"Considering 'recent' generally refers to the past few months or up to a year, the best date range to capture the most recent developments in AI would be from one year ago to today's date.\", start=datetime.date(2023, 2, 9), end=datetime.date(2024, 2, 9)))</pre> In\u00a0[7]: Copied! <pre>import json\nimport instructor\n\nfrom openai import AsyncOpenAI\nfrom helpers import dicts_to_df\nfrom datetime import date\nfrom pydantic import BaseModel, Field\n\n\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n    start: date\n    end: date\n\n\nclass Query(BaseModel):\n    rewritten_query: str = Field(\n        description=\"Rewrite the query to make it more specific\"\n    )\n    published_daterange: DateRange = Field(\n        description=\"Effective date range to search in\"\n    )\n\n    def report(self):\n        dct = self.model_dump()\n        dct[\"usage\"] = self._raw_response.usage.model_dump()\n        return dct\n\n\n\n# We'll use a different client for async calls\n# To highlight the difference and how we can use both\naclient = instructor.patch(AsyncOpenAI())\n\n\nasync def expand_query(\n    q, *, model: str = \"gpt-4-1106-preview\", temp: float = 0\n) -&gt; Query:\n    return await aclient.chat.completions.create(\n        model=model,\n        temperature=temp,\n        response_model=Query,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n            },\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n        ],\n    )\n</pre> import json import instructor  from openai import AsyncOpenAI from helpers import dicts_to_df from datetime import date from pydantic import BaseModel, Field   class DateRange(BaseModel):     chain_of_thought: str = Field(         description=\"Think step by step to plan what is the best time range to search in\"     )     start: date     end: date   class Query(BaseModel):     rewritten_query: str = Field(         description=\"Rewrite the query to make it more specific\"     )     published_daterange: DateRange = Field(         description=\"Effective date range to search in\"     )      def report(self):         dct = self.model_dump()         dct[\"usage\"] = self._raw_response.usage.model_dump()         return dct    # We'll use a different client for async calls # To highlight the difference and how we can use both aclient = instructor.patch(AsyncOpenAI())   async def expand_query(     q, *, model: str = \"gpt-4-1106-preview\", temp: float = 0 ) -&gt; Query:     return await aclient.chat.completions.create(         model=model,         temperature=temp,         response_model=Query,         messages=[             {                 \"role\": \"system\",                 \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",             },             {\"role\": \"user\", \"content\": f\"query: {q}\"},         ],     ) <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb Cell 20 line 5\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X25sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt; import instructor\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X25sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt; from openai import AsyncOpenAI\n----&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X25sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt; from helpers import dicts_to_df\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X25sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt; from datetime import date\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/3-0-applications-rag.ipynb#X25sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt; from pydantic import BaseModel, Field\n\nModuleNotFoundError: No module named 'helpers'</pre> In\u00a0[\u00a0]: Copied! <pre>import asyncio\nimport time\nimport pandas as pd\nimport wandb\n\nmodel = \"gpt-4-1106-preview\"\ntemp = 0\n\nrun = wandb.init(\n    project=\"query\",\n    config={\"model\": model, \"temp\": temp},\n)\n\ntest_queries = [\n    \"latest developments in artificial intelligence last 3 weeks\",\n    \"renewable energy trends past month\",\n    \"quantum computing advancements last 2 months\",\n    \"biotechnology updates last 10 days\",\n]\nstart = time.perf_counter()\nqueries = await asyncio.gather(\n    *[expand_query(q, model=model, temp=temp) for q in test_queries]\n)\nduration = time.perf_counter() - start\n\nwith open(\"schema.json\", \"w+\") as f:\n    schema = Query.model_json_schema()\n    json.dump(schema, f, indent=2)\n\nwith open(\"results.jsonlines\", \"w+\") as f:\n    for query in queries:\n        f.write(query.model_dump_json() + \"\\n\")\n\ndf = dicts_to_df([q.report() for q in queries])\ndf[\"input\"] = test_queries\ndf.to_csv(\"results.csv\")\n\n\nrun.log({\"schema\": wandb.Table(dataframe=pd.DataFrame([{\"schema\": schema}]))})\n\nrun.log(\n    {\n        \"usage_total_tokens\": df[\"usage_total_tokens\"].sum(),\n        \"usage_completion_tokens\": df[\"usage_completion_tokens\"].sum(),\n        \"usage_prompt_tokens\": df[\"usage_prompt_tokens\"].sum(),\n        \"duration (s)\": duration,\n        \"average duration (s)\": duration / len(queries),\n        \"n_queries\": len(queries),\n    }\n)\n\n\nrun.log(\n    {\n        \"results\": wandb.Table(dataframe=df),\n    }\n)\n\nfiles = wandb.Artifact(\"data\", type=\"dataset\")\n\nfiles.add_file(\"schema.json\")\nfiles.add_file(\"results.jsonlines\")\nfiles.add_file(\"results.csv\")\n\n\nrun.log_artifact(files)\nrun.finish()\n</pre> import asyncio import time import pandas as pd import wandb  model = \"gpt-4-1106-preview\" temp = 0  run = wandb.init(     project=\"query\",     config={\"model\": model, \"temp\": temp}, )  test_queries = [     \"latest developments in artificial intelligence last 3 weeks\",     \"renewable energy trends past month\",     \"quantum computing advancements last 2 months\",     \"biotechnology updates last 10 days\", ] start = time.perf_counter() queries = await asyncio.gather(     *[expand_query(q, model=model, temp=temp) for q in test_queries] ) duration = time.perf_counter() - start  with open(\"schema.json\", \"w+\") as f:     schema = Query.model_json_schema()     json.dump(schema, f, indent=2)  with open(\"results.jsonlines\", \"w+\") as f:     for query in queries:         f.write(query.model_dump_json() + \"\\n\")  df = dicts_to_df([q.report() for q in queries]) df[\"input\"] = test_queries df.to_csv(\"results.csv\")   run.log({\"schema\": wandb.Table(dataframe=pd.DataFrame([{\"schema\": schema}]))})  run.log(     {         \"usage_total_tokens\": df[\"usage_total_tokens\"].sum(),         \"usage_completion_tokens\": df[\"usage_completion_tokens\"].sum(),         \"usage_prompt_tokens\": df[\"usage_prompt_tokens\"].sum(),         \"duration (s)\": duration,         \"average duration (s)\": duration / len(queries),         \"n_queries\": len(queries),     } )   run.log(     {         \"results\": wandb.Table(dataframe=df),     } )  files = wandb.Artifact(\"data\", type=\"dataset\")  files.add_file(\"schema.json\") files.add_file(\"results.jsonlines\") files.add_file(\"results.csv\")   run.log_artifact(files) run.finish()  wandb version 0.16.1 is available!  To upgrade, please run:  $ pip install wandb --upgrade   Tracking run with wandb version 0.16.0   Run data is saved locally in <code>/Users/jasonliu/dev/instructor/tutorials/wandb/run-20231227_202003-7c9dxnfl</code>  Syncing run blooming-firefly-4 to Weights &amp; Biases (docs)   View project at https://wandb.ai/instructor/query   View run at https://wandb.ai/instructor/query/runs/7c9dxnfl <pre>VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded (0.001 MB deduped)\\r'), FloatProgress(value=1.0, max\u2026</pre> <pre>wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n</pre>  W&amp;B sync reduced upload amount by 6.6%               Run history:average duration (s)\u2581duration (s)\u2581n_queries\u2581usage_completion_tokens\u2581usage_prompt_tokens\u2581usage_total_tokens\u2581Run summary:average duration (s)2.28692duration (s)9.14768n_queries4usage_completion_tokens359usage_prompt_tokens780usage_total_tokens1139   View run blooming-firefly-4 at: https://wandb.ai/instructor/query/runs/7c9dxnflSynced 4 W&amp;B file(s), 2 media file(s), 5 artifact file(s) and 0 other file(s)   Find logs at: <code>./wandb/run-20231227_202003-7c9dxnfl/logs</code> In\u00a0[8]: Copied! <pre>from typing import Literal\n\n\nclass SearchClient(BaseModel):\n    query: str = Field(description=\"The search query that will go into the search bar\")\n    keywords: List[str]\n    email: str\n    source: Literal[\"gmail\", \"calendar\"]\n    date_range: DateRange\n\n\nclass Retrieval(BaseModel):\n    queries: List[SearchClient]\n</pre> from typing import Literal   class SearchClient(BaseModel):     query: str = Field(description=\"The search query that will go into the search bar\")     keywords: List[str]     email: str     source: Literal[\"gmail\", \"calendar\"]     date_range: DateRange   class Retrieval(BaseModel):     queries: List[SearchClient] <p>Now, we can utilize this with a straightforward query such as \"What do I have today?\".</p> <p>The system will attempt to asynchronously dispatch the query to the appropriate backend.</p> <p>However, it's still crucial to remember that effectively prompting the language model is still a key aspect.</p> In\u00a0[9]: Copied! <pre>retrieval = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Retrieval,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"You are Jason's personal assistant.\n                He has two emails jason@work.com jason@personal.com \n                Today is {date.today()}\"\"\",\n        },\n        {\"role\": \"user\", \"content\": \"What do I have today for work? any new emails?\"},\n    ],\n)\nprint(retrieval.model_dump_json(indent=4))\n</pre> retrieval = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     response_model=Retrieval,     messages=[         {             \"role\": \"system\",             \"content\": f\"\"\"You are Jason's personal assistant.                 He has two emails jason@work.com jason@personal.com                  Today is {date.today()}\"\"\",         },         {\"role\": \"user\", \"content\": \"What do I have today for work? any new emails?\"},     ], ) print(retrieval.model_dump_json(indent=4)) <pre>{\n    \"queries\": [\n        {\n            \"query\": \"work\",\n            \"keywords\": [],\n            \"email\": \"jason@work.com\",\n            \"source\": \"calendar\",\n            \"date_range\": {\n                \"chain_of_thought\": \"today\",\n                \"start\": \"2024-02-09\",\n                \"end\": \"2024-02-09\"\n            }\n        },\n        {\n            \"query\": \"\",\n            \"keywords\": [],\n            \"email\": \"jason@work.com\",\n            \"source\": \"gmail\",\n            \"date_range\": {\n                \"chain_of_thought\": \"today\",\n                \"start\": \"2024-02-09\",\n                \"end\": \"2024-02-09\"\n            }\n        }\n    ]\n}\n</pre> <p>To make it more challenging, we will assign it multiple tasks, followed by a list of queries that are routed to various search backends, such as email and calendar. Not only do we dispatch to different backends, over which we have no control, but we are also likely to render them to the user in different ways.</p> In\u00a0[10]: Copied! <pre>retrieval = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=Retrieval,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"You are Jason's personal assistant.\n                He has two emails jason@work.com jason@personal.com \n                Today is {date.today()}\"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What meetings do I have today and are there any important emails I should be aware of\",\n        },\n    ],\n)\nprint(retrieval.model_dump_json(indent=4))\n</pre> retrieval = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     response_model=Retrieval,     messages=[         {             \"role\": \"system\",             \"content\": f\"\"\"You are Jason's personal assistant.                 He has two emails jason@work.com jason@personal.com                  Today is {date.today()}\"\"\",         },         {             \"role\": \"user\",             \"content\": \"What meetings do I have today and are there any important emails I should be aware of\",         },     ], ) print(retrieval.model_dump_json(indent=4)) In\u00a0[\u00a0]: Copied! <pre>class Question(BaseModel):\n    id: int = Field(..., description=\"A unique identifier for the question\")\n    query: str = Field(..., description=\"The question decomposited as much as possible\")\n    subquestions: List[int] = Field(\n        default_factory=list,\n        description=\"The subquestions that this question is composed of\",\n    )\n\n\nclass QueryPlan(BaseModel):\n    root_question: str = Field(..., description=\"The root question that the user asked\")\n    plan: List[Question] = Field(\n        ..., description=\"The plan to answer the root question and its subquestions\"\n    )\n\n\nretrieval = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=QueryPlan,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a query understanding system capable of decomposing a question into subquestions.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the difference between the population of jason's home country and canada?\",\n        },\n    ],\n)\n\nprint(retrieval.model_dump_json(indent=4))\n</pre> class Question(BaseModel):     id: int = Field(..., description=\"A unique identifier for the question\")     query: str = Field(..., description=\"The question decomposited as much as possible\")     subquestions: List[int] = Field(         default_factory=list,         description=\"The subquestions that this question is composed of\",     )   class QueryPlan(BaseModel):     root_question: str = Field(..., description=\"The root question that the user asked\")     plan: List[Question] = Field(         ..., description=\"The plan to answer the root question and its subquestions\"     )   retrieval = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     response_model=QueryPlan,     messages=[         {             \"role\": \"system\",             \"content\": \"You are a query understanding system capable of decomposing a question into subquestions.\",         },         {             \"role\": \"user\",             \"content\": \"What is the difference between the population of jason's home country and canada?\",         },     ], )  print(retrieval.model_dump_json(indent=4)) <pre>{\n    \"root_question\": \"What is the difference between the population of jason's home country and canada?\",\n    \"plan\": [\n        {\n            \"id\": 1,\n            \"query\": \"What is Jason's home country?\",\n            \"subquestions\": []\n        },\n        {\n            \"id\": 2,\n            \"query\": \"What is the population of Canada?\",\n            \"subquestions\": []\n        },\n        {\n            \"id\": 3,\n            \"query\": \"What is the population of {Jason's home country}?\",\n            \"subquestions\": [\n                1\n            ]\n        },\n        {\n            \"id\": 4,\n            \"query\": \"What is the difference between the population of {Jason's home country} and the population of Canada?\",\n            \"subquestions\": [\n                2,\n                3\n            ]\n        }\n    ]\n}\n</pre> <p>I hope in this section I've exposed you to some ways we can be creative in modeling structured outputs to leverage LLMS in building some lightweight components for our systems.</p>"},{"location":"tutorials/3-0-applications-rag/#applying-structured-output-to-rag-applications","title":"Applying Structured Output to RAG applications\u00b6","text":""},{"location":"tutorials/3-0-applications-rag/#simple-rag","title":"Simple RAG\u00b6","text":"<p>What is it?</p> <p>The simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.</p> <ul> <li>Query-Document Mismatch: It assumes that the query and document embeddings will align in the vector space, which is often not the case.</li> <li>Text Search Limitations: The model is restricted to simple text queries without the nuances of advanced search features.</li> <li>Limited Planning Ability: It fails to consider additional contextual information that could refine the search results.</li> </ul>"},{"location":"tutorials/3-0-applications-rag/#improving-the-rag-model","title":"Improving the RAG model\u00b6","text":"<p>What's the solution?</p> <p>Enhancing RAG requires a more sophisticated approach known as query understanding.</p> <p>This process involves analyzing the user's query and transforming it to better match the backend's search capabilities.</p> <p>By doing so, we can significantly improve both the precision and recall of the search results, providing more accurate and relevant responses.</p> <p></p>"},{"location":"tutorials/3-0-applications-rag/#practical-examples","title":"Practical Examples\u00b6","text":""},{"location":"tutorials/3-0-applications-rag/#example-1-improving-extractions","title":"Example 1) Improving Extractions\u00b6","text":"<p>One of the big limitations is that often times the query we embed and the text A common method of using structured output is to extract information from a document and use it to answer a question. Directly, we can be creative in how we extract, summarize and generate potential questions in order for our embeddings to do better.</p> <p>For example, instead of using just a text chunk we could try to:</p> <ol> <li>extract key words and themes</li> <li>extract hypothetical questions</li> <li>generate a summary of the text</li> </ol> <p>In the example below, we use the <code>instructor</code> library to extract the key words and themes from a text chunk and use them to answer a question.</p>"},{"location":"tutorials/3-0-applications-rag/#example-2-understanding-recent-queries-to-add-temporal-context","title":"Example 2) Understanding 'recent queries' to add temporal context\u00b6","text":"<p>One common application of using structured outputs for query understanding is to identify the intent of a user's query. In this example we're going to use a simple schema to seperately process the query to add additional temporal context.</p>"},{"location":"tutorials/3-0-applications-rag/#using-weights-and-biases-to-track-experiments","title":"Using Weights and Biases to track experiments\u00b6","text":"<p>While running a function like this production is quite simple, a lot of time will be spend on iterating and improving the model. To do this, we can use Weights and Biases to track our experiments.</p> <p>In order to do so we wand manage a few things</p> <ol> <li>Save input and output pairs for later</li> <li>Save the JSON schema for the response_model</li> <li>Having snapshots of the model and data allow us to compare results over time, and as we make changes to the model we can see how the results change.</li> </ol> <p>This is particularly useful when we might want to blend a mix of synthetic and real data to evaluate our model. We can use the <code>wandb</code> library to track our experiments and save the results to a dashboard.</p>"},{"location":"tutorials/3-0-applications-rag/#example-3-personal-assistants-parallel-processing","title":"Example 3) Personal Assistants, parallel processing\u00b6","text":"<p>A personal assistant application needs to interpret vague queries and fetch information from multiple backends, such as emails and calendars. By modeling the assistant's capabilities using Pydantic, we can dispatch the query to the correct backend and retrieve a unified response.</p> <p>For instance, when you ask, \"What's on my schedule today?\", the application needs to fetch data from various sources like events, emails, and reminders. This data is stored across different backends, but the goal is to provide a consolidated summary of results.</p> <p>It's important to note that the data from these sources may not be embedded in a search backend. Instead, they could be accessed through different clients like a calendar or email, spanning both personal and professional accounts.</p>"},{"location":"tutorials/3-0-applications-rag/#example-4-decomposing-questions","title":"Example 4) Decomposing questions\u00b6","text":"<p>Lastly, a lightly more complex example of a problem that can be solved with structured output is decomposing questions. Where you ultimately want to decompose a question into a series of sub-questions that can be answered by a search backend. For example</p> <p>\"Whats the difference in populations of jason's home country and canada?\"</p> <p>You'd ultimately need to know a few things</p> <ol> <li>Jason's home country</li> <li>The population of Jason's home country</li> <li>The population of Canada</li> <li>The difference between the two</li> </ol> <p>This would not be done correctly as a single query, nor would it be done in parallel, however there are some opportunities try to be parallel since not all of the sub-questions are dependent on each other.</p>"},{"location":"tutorials/3-1-validation-rag/","title":"Applications RAG - 2","text":"<p>Pydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic docs on validators.</p> <p>Then we'll bring it all together into the context of RAG from the previous notebook.</p> <p>Validators will enable us to control outputs by defining a function like so:</p> <pre>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n</pre> <p>Before we get started lets go over the general shape of a validator:</p> In\u00a0[18]: Copied! <pre>from typing_extensions import Annotated\nfrom pydantic import BaseModel, AfterValidator, WithJsonSchema\n\n\ndef name_must_contain_space(v: str) -&gt; str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v\n\ndef uppercase_name(v: str) -&gt; str:\n    return v.upper()\n\nFullName = Annotated[\n    str, \n    AfterValidator(name_must_contain_space), \n    AfterValidator(uppercase_name),\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"The user's full name\",\n        }\n    )]\n\nclass UserDetail(BaseModel):\n    age: int\n    name: FullName\n</pre> from typing_extensions import Annotated from pydantic import BaseModel, AfterValidator, WithJsonSchema   def name_must_contain_space(v: str) -&gt; str:     if \" \" not in v:         raise ValueError(\"Name must contain a space.\")     return v  def uppercase_name(v: str) -&gt; str:     return v.upper()  FullName = Annotated[     str,      AfterValidator(name_must_contain_space),      AfterValidator(uppercase_name),     WithJsonSchema(         {             \"type\": \"string\",             \"description\": \"The user's full name\",         }     )]  class UserDetail(BaseModel):     age: int     name: FullName In\u00a0[19]: Copied! <pre>UserDetail(age=30, name=\"Jason Liu\")\n</pre> UserDetail(age=30, name=\"Jason Liu\") Out[19]: <pre>UserDetail(age=30, name='JASON LIU')</pre> In\u00a0[20]: Copied! <pre>UserDetail.model_json_schema()\n</pre> UserDetail.model_json_schema() Out[20]: <pre>{'properties': {'age': {'title': 'Age', 'type': 'integer'},\n  'name': {'description': \"The user's full name\",\n   'title': 'Name',\n   'type': 'string'}},\n 'required': ['age', 'name'],\n 'title': 'UserDetail',\n 'type': 'object'}</pre> In\u00a0[21]: Copied! <pre>try:\n    person = UserDetail.model_validate({\"age\": 24, \"name\": \"Jason\"})\nexcept Exception as e:\n    print(e)\n</pre> try:     person = UserDetail.model_validate({\"age\": 24, \"name\": \"Jason\"}) except Exception as e:     print(e) <pre>1 validation error for UserDetail\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n</pre> In\u00a0[22]: Copied! <pre>from pydantic import Field\n\n\nAge = Annotated[int, Field(gt=0)]\n\nclass UserDetail(BaseModel):\n    age: Age\n    name: FullName\n\ntry:\n    person = UserDetail(age=-10, name=\"Jason\")\nexcept Exception as e:\n    print(e)\n</pre> from pydantic import Field   Age = Annotated[int, Field(gt=0)]  class UserDetail(BaseModel):     age: Age     name: FullName  try:     person = UserDetail(age=-10, name=\"Jason\") except Exception as e:     print(e) <pre>2 validation errors for UserDetail\nage\n  Input should be greater than 0 [type=greater_than, input_value=-10, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.5/v/greater_than\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n</pre> In\u00a0[7]: Copied! <pre>from pydantic import ValidationInfo\n\n\ndef message_cannot_have_blacklisted_words(v: str, info: ValidationInfo) -&gt; str:\n    blacklist = info.context.get(\"blacklist\", [])\n    for word in blacklist:\n        assert word not in v.lower(), f\"`{word}` was found in the message `{v}`\"\n    return v\n\nModeratedStr = Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]\n\nclass Response(BaseModel):\n    message: ModeratedStr\n\n\ntry:\n    Response.model_validate(\n        {\"message\": \"I will hurt them.\"},\n        context={\n            \"blacklist\": {\n                \"rob\",\n                \"steal\",\n                \"hurt\",\n                \"kill\",\n                \"attack\",\n            }\n        },\n    )\nexcept Exception as e:\n    print(e)\n</pre> from pydantic import ValidationInfo   def message_cannot_have_blacklisted_words(v: str, info: ValidationInfo) -&gt; str:     blacklist = info.context.get(\"blacklist\", [])     for word in blacklist:         assert word not in v.lower(), f\"`{word}` was found in the message `{v}`\"     return v  ModeratedStr = Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]  class Response(BaseModel):     message: ModeratedStr   try:     Response.model_validate(         {\"message\": \"I will hurt them.\"},         context={             \"blacklist\": {                 \"rob\",                 \"steal\",                 \"hurt\",                 \"kill\",                 \"attack\",             }         },     ) except Exception as e:     print(e) <pre>1 validation error for Response\nmessage\n  Assertion failed, `hurt` was found in the message `I will hurt them.` [type=assertion_error, input_value='I will hurt them.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/assertion_error\n</pre> <p>To enhance our validation measures, we'll extend the scope to flag any answer that contains hateful content, harassment, or similar issues. OpenAI offers a moderation endpoint that addresses these concerns, and it's freely available when using OpenAI models.</p> <p>With the <code>instructor</code> library, this is just one function edit away:</p> In\u00a0[13]: Copied! <pre>from typing import Annotated\nfrom pydantic import AfterValidator\nfrom instructor import openai_moderation\n\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\n# This uses Annotated which is a new feature in Python 3.9\n# To define custom metadata for a type hint.\nModeratedStr = Annotated[str, AfterValidator(openai_moderation(client=client))]\n\n\nclass Response(BaseModel):\n    message: ModeratedStr\n\n\ntry:\n    Response(message=\"I want to make them suffer the consequences\")\nexcept Exception as e:\n    print(e)\n</pre> from typing import Annotated from pydantic import AfterValidator from instructor import openai_moderation  import instructor from openai import OpenAI  client = instructor.patch(OpenAI())  # This uses Annotated which is a new feature in Python 3.9 # To define custom metadata for a type hint. ModeratedStr = Annotated[str, AfterValidator(openai_moderation(client=client))]   class Response(BaseModel):     message: ModeratedStr   try:     Response(message=\"I want to make them suffer the consequences\") except Exception as e:     print(e) <pre>1 validation error for Response\nmessage\n  Value error, `I want to make them suffer the consequences` was flagged for harassment, harassment_threatening, violence, harassment/threatening [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n</pre> In\u00a0[\u00a0]: Copied! <pre>from instructor import llm_validator\n\nHealthTopicStr = Annotated[\n    str,\n    AfterValidator(\n        llm_validator(\n            \"don't talk about any other topic except health best practices and topics\",\n            openai_client=client,\n        )\n    ),\n]\n\n\nclass AssistantMessage(BaseModel):\n    message: HealthTopicStr\n\n\nAssistantMessage(\n    message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\"\n)\n</pre> from instructor import llm_validator  HealthTopicStr = Annotated[     str,     AfterValidator(         llm_validator(             \"don't talk about any other topic except health best practices and topics\",             openai_client=client,         )     ), ]   class AssistantMessage(BaseModel):     message: HealthTopicStr   AssistantMessage(     message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\" ) <p>When incorporating external knowledge bases, it's crucial to ensure that the agent uses the provided context accurately and doesn't fabricate responses. Validators can be effectively used for this purpose. We can illustrate this with an example where we validate that a provided citation is actually included in the referenced text chunk:</p> In\u00a0[27]: Copied! <pre>from pydantic import ValidationInfo\n\ndef citation_exists(v: str, info: ValidationInfo):\n    context = info.context\n    if context:\n        context = context.get(\"text_chunk\")\n        if v not in context:\n            raise ValueError(f\"Citation `{v}` not found in text, only use citations from the text.\")\n    return v\n\nCitation = Annotated[str, AfterValidator(citation_exists)]\n\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: Citation\n\ntry:\n    AnswerWithCitation.model_validate(\n        {\n            \"answer\": \"Blueberries are packed with protein\",\n            \"citation\": \"Blueberries contain high levels of protein\",\n        },\n        context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"},\n    )\nexcept Exception as e:\n    print(e)\n</pre> from pydantic import ValidationInfo  def citation_exists(v: str, info: ValidationInfo):     context = info.context     if context:         context = context.get(\"text_chunk\")         if v not in context:             raise ValueError(f\"Citation `{v}` not found in text, only use citations from the text.\")     return v  Citation = Annotated[str, AfterValidator(citation_exists)]   class AnswerWithCitation(BaseModel):     answer: str     citation: Citation  try:     AnswerWithCitation.model_validate(         {             \"answer\": \"Blueberries are packed with protein\",             \"citation\": \"Blueberries contain high levels of protein\",         },         context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"},     ) except Exception as e:     print(e) <pre>1 validation error for AnswerWithCitation\ncitation\n  Value error, Citation `Blueberries contain high levels of protein` not found in text, only use citations from the text. [type=value_error, input_value='Blueberries contain high levels of protein', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n</pre> <p>Here we assume that there is a \"text_chunk\" field that contains the text that the model is supposed to use as context. We then use the <code>field_validator</code> decorator to define a validator that checks if the citation is included in the text chunk. If it's not, we raise a <code>ValueError</code> with a message that will be returned to the user.</p> <p>If we want to pass in the context through the <code>chat.completions.create`` endpoint, we can use the </code>validation_context` parameter</p> <pre>resp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=AnswerWithCitation,\n    messages=[\n        {\"role\": \"user\", \"content\": f\"Answer the question `{q}` using the text chunk\\n`{text_chunk}`\"},\n    ],\n    validation_context={\"text_chunk\": text_chunk},\n)\n</pre> <p>In practice there are many ways to implement this: we could use a regex to check if the citation is included in the text chunk, or we could use a more sophisticated approach like a semantic similarity check. The important thing is that we have a way to validate that the model is using the provided context accurately.</p> In\u00a0[15]: Copied! <pre>class QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\n\nquestion = \"What is the meaning of life?\"\ncontext = (\n    \"The according to the devil the meaning of life is a life of sin and debauchery.\"\n)\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\n        },\n    ],\n)\n\nprint(resp.model_dump_json(indent=2))\n</pre> class QuestionAnswer(BaseModel):     question: str     answer: str   question = \"What is the meaning of life?\" context = (     \"The according to the devil the meaning of life is a life of sin and debauchery.\" )   resp = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     response_model=QuestionAnswer,     messages=[         {             \"role\": \"system\",             \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",         },         {             \"role\": \"user\",             \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",         },     ], )  print(resp.model_dump_json(indent=2)) <pre>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"According to the devil, the meaning of life is a life of sin and debauchery.\"\n}\n</pre> In\u00a0[20]: Copied! <pre>from instructor import llm_validator\n\n\nNotEvilAnswer = Annotated[\n    str,\n    AfterValidator(\n        llm_validator(\"don't say objectionable things\", openai_client=client)\n    ),\n]\n\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: NotEvilAnswer\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    max_retries=2,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\n        },\n    ],\n)\n</pre> from instructor import llm_validator   NotEvilAnswer = Annotated[     str,     AfterValidator(         llm_validator(\"don't say objectionable things\", openai_client=client)     ), ]   class QuestionAnswer(BaseModel):     question: str     answer: NotEvilAnswer   resp = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     response_model=QuestionAnswer,     max_retries=2,     messages=[         {             \"role\": \"system\",             \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",         },         {             \"role\": \"user\",             \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",         },     ], ) <pre>Retrying, exception: 1 validation error for QuestionAnswer\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which can be considered objectionable. [type=assertion_error, input_value='The meaning of life, acc... of sin and debauchery.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/assertion_error\nTraceback (most recent call last):\n  File \"/Users/jasonliu/dev/instructor/instructor/patch.py\", line 277, in retry_sync\n    return process_response(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/jasonliu/dev/instructor/instructor/patch.py\", line 164, in process_response\n    model = response_model.from_response(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jasonliu/dev/instructor/instructor/function_calls.py\", line 137, in from_response\n    return cls.model_validate_json(\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jasonliu/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py\", line 532, in model_validate_json\n    return cls.__pydantic_validator__.validate_json(json_data, strict=strict, context=context)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for QuestionAnswer\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which can be considered objectionable. [type=assertion_error, input_value='The meaning of life, acc... of sin and debauchery.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/assertion_error\n</pre> In\u00a0[21]: Copied! <pre>print(resp.model_dump_json(indent=2))\n</pre> print(resp.model_dump_json(indent=2)) <pre>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life is subjective and can vary depending on one's beliefs and perspectives. According to the devil, it is a life of sin and debauchery. However, this viewpoint may not be universally accepted and should be evaluated critically.\"\n}\n</pre>"},{"location":"tutorials/3-1-validation-rag/#understanding-validators","title":"Understanding Validators\u00b6","text":""},{"location":"tutorials/3-1-validation-rag/#defining-validator-functions","title":"Defining Validator Functions\u00b6","text":""},{"location":"tutorials/3-1-validation-rag/#using-field","title":"Using Field\u00b6","text":"<p>We can also use the <code>Field</code> class to define validators. This is useful when we want to define a validator for a field that is primative, like a string or integer which supports a limited number of validators.</p>"},{"location":"tutorials/3-1-validation-rag/#providing-context","title":"Providing Context\u00b6","text":""},{"location":"tutorials/3-1-validation-rag/#using-openai-moderation","title":"Using OpenAI Moderation\u00b6","text":""},{"location":"tutorials/3-1-validation-rag/#general-validator","title":"General Validator\u00b6","text":""},{"location":"tutorials/3-1-validation-rag/#avoiding-hallucination-with-citations","title":"Avoiding hallucination with citations\u00b6","text":""},{"location":"tutorials/3-1-validation-rag/#reasking-with-validators","title":"Reasking with validators\u00b6","text":"<p>For most of these examples all we've done we've mostly only defined the validation logic. Which can be seperate from generation, however when we are given validation errors, we shouldn't end there! Instead instructor allows us to collect all the validation errors and reask the llm to rewrite their answer.</p> <p>Lets try to use a extreme example to illustrate this point:</p>"},{"location":"tutorials/4-validation/","title":"Validation","text":"<p>Instead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the systen can use to self correct.</p> <p>Pydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic docs on validators.</p> <p>Note: For the majority of this notebook we won't be calling openai, just using validators to see how we can control the validation of the objects.</p> <p>Validators will enable us to control outputs by defining a function like so:</p> <pre>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n</pre> <p>Before we get started lets go over the general shape of a validator:</p> In\u00a0[61]: Copied! <pre>from pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\ndef name_must_contain_space(v: str) -&gt; str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\nperson = UserDetail(age=29, name=\"Jason\")\n</pre> from pydantic import BaseModel, ValidationError from typing_extensions import Annotated from pydantic import AfterValidator  def name_must_contain_space(v: str) -&gt; str:     if \" \" not in v:         raise ValueError(\"Name must contain a space.\")     return v.lower()  class UserDetail(BaseModel):     age: int     name: Annotated[str, AfterValidator(name_must_contain_space)]  person = UserDetail(age=29, name=\"Jason\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 4 line 1\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#W3sZmlsZQ%3D%3D?line=10'&gt;11&lt;/a&gt;     age: int\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#W3sZmlsZQ%3D%3D?line=11'&gt;12&lt;/a&gt;     name: Annotated[str, AfterValidator(name_must_contain_space)]\n---&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#W3sZmlsZQ%3D%3D?line=13'&gt;14&lt;/a&gt; person = UserDetail(age=29, name=\"Jason\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for UserDetail\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error</pre> <p>Validation Applications</p> <p>Validators are essential in tackling the unpredictabile nature of LLMs.</p> <p>Straightforward examples include:</p> <ul> <li>Flagging outputs containing blacklisted words.</li> <li>Identifying outputs with tones like racism or violence.</li> </ul> <p>For more complex tasks:</p> <ul> <li>Ensuring citations directly come from provided content.</li> <li>Checking that the model's responses align with given context.</li> <li>Validating the syntax of SQL queries before execution.</li> </ul> <p>Using the instructor library, we streamline the integration of these validators. <code>instructor</code> manages the parsing and validation of outputs and automates retries for compliant responses. This simplifies the process for developers to implement new validation logic, minimizing extra overhead.</p> <p>To use instructor in our api calls, we just need to patch the openai client:</p> In\u00a0[5]: Copied! <pre>import instructor \nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n</pre> import instructor  from openai import OpenAI  client = instructor.patch(OpenAI()) <p>Deterministic validation, characterized by its rule-based logic, ensures consistent outcomes for the same input. Let's explore how we can apply this concept through some examples.</p> <p>To begin with, we aim to prevent engagement in topics involving explicit violence.</p> <p>We will define a blacklist of violent words that cannot be mentioned in any messages:</p> In\u00a0[63]: Copied! <pre>blacklist = {\n    \"rob\",\n    \"steal\",\n    \"hurt\",\n    \"kill\",\n    \"attack\",\n}\n</pre> blacklist = {     \"rob\",     \"steal\",     \"hurt\",     \"kill\",     \"attack\", } <p>To validate if the message contains a blacklisted word we will use a field_validator over the 'message' field:</p> In\u00a0[64]: Copied! <pre>from pydantic import BaseModel, ValidationError, field_validator\nfrom pydantic.fields import Field\n\nclass Response(BaseModel):\n    message: str\n\n    @field_validator('message')\n    def message_cannot_have_blacklisted_words(cls, v: str) -&gt; str:\n        for word in v.split(): \n            if word.lower() in blacklist:\n                raise ValueError(f\"`{word}` was found in the message `{v}`\")\n        return v\n\nResponse(message=\"I will hurt him\")\n</pre> from pydantic import BaseModel, ValidationError, field_validator from pydantic.fields import Field  class Response(BaseModel):     message: str      @field_validator('message')     def message_cannot_have_blacklisted_words(cls, v: str) -&gt; str:         for word in v.split():              if word.lower() in blacklist:                 raise ValueError(f\"`{word}` was found in the message `{v}`\")         return v  Response(message=\"I will hurt him\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 17 line 1\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X23sZmlsZQ%3D%3D?line=10'&gt;11&lt;/a&gt;                 raise ValueError(f\"`{word}` was found in the message `{v}`\")\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X23sZmlsZQ%3D%3D?line=11'&gt;12&lt;/a&gt;         return v\n---&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X23sZmlsZQ%3D%3D?line=13'&gt;14&lt;/a&gt; Response(message=\"I will hurt him\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for Response\nmessage\n  Value error, `hurt` was found in the message `I will hurt him` [type=value_error, input_value='I will hurt him', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error</pre> <p>To enhance our validation measures, we'll extend the scope to flag any answer that contains hateful content, harassment, or similar issues. OpenAI offers a moderation endpoint that addresses these concerns, and it's freely available when using OpenAI models.</p> <p>With the <code>instructor</code> library, this is just one function edit away:</p> In\u00a0[1]: Copied! <pre>from typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n</pre> from typing import Annotated from pydantic.functional_validators import AfterValidator In\u00a0[6]: Copied! <pre>from instructor import openai_moderation\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(openai_moderation(client=client))]\n</pre> from instructor import openai_moderation  class Response(BaseModel):     message: Annotated[str, AfterValidator(openai_moderation(client=client))] <p>Now we have a more comprehensive flagging for violence and we can outsource the moderation of our messages.</p> In\u00a0[7]: Copied! <pre>Response(message=\"I want to make them suffer the consequences\")\n</pre> Response(message=\"I want to make them suffer the consequences\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 Response(message=\"I want to make them suffer the consequences\")\n\nFile ~/.virtualenvs/pampa-labs/lib/python3.10/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for Response\nmessage\n  Value error, `I want to make them suffer the consequences` was flagged for harassment, harassment_threatening, violence, harassment/threatening [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error</pre> <p>And as an extra, we get flagging for other topics like religion, race etc.</p> In\u00a0[26]: Copied! <pre>Response(message=\"I will mock their religion\")\n</pre> Response(message=\"I will mock their religion\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 Response(message=\"I will mock their religion\")\n\nFile ~/.virtualenvs/pampa-labs/lib/python3.10/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for Response\nmessage\n  Value error, `I will mock their religion` was flagged for ['harassment'] [type=value_error, input_value='I will mock their religion', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error</pre> <p>In addition to content-based flags, we can also set criteria based on other aspects of the input text. For instance, to maintain user engagement, we might want to prevent the assistant from returning excessively long texts.</p> <p>Here, noticed that <code>Field</code> has built-in validators for <code>min_length</code> and <code>max_length</code>. to learn more checkout Field Contraints</p> In\u00a0[68]: Copied! <pre>class AssistantMessage(BaseModel):\n    message: str = Field(..., max_length=100)\n</pre> class AssistantMessage(BaseModel):     message: str = Field(..., max_length=100) In\u00a0[69]: Copied! <pre>AssistantMessage(message=\"Certainly! Lorem ipsum is a placeholder text commonly used in the printing and typesetting industry. Here's a sample of Lorem ipsum text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam euismod velit vel tellus tempor, non viverra eros iaculis. Sed vel nisl nec mauris bibendum tincidunt. Vestibulum sed libero euismod, eleifend tellus id, laoreet elit. Donec auctor arcu ac mi feugiat, vel lobortis justo efficitur. Fusce vel odio vitae justo varius dignissim. Integer sollicitudin mi a justo bibendum ultrices. Quisque id nisl a lectus venenatis luctus. Please note that Lorem ipsum text is a nonsensical Latin-like text used as a placeholder for content, and it has no specific meaning. It's often used in design and publishing to demonstrate the visual aspects of a document without focusing on the actual content.\")\n</pre> AssistantMessage(message=\"Certainly! Lorem ipsum is a placeholder text commonly used in the printing and typesetting industry. Here's a sample of Lorem ipsum text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam euismod velit vel tellus tempor, non viverra eros iaculis. Sed vel nisl nec mauris bibendum tincidunt. Vestibulum sed libero euismod, eleifend tellus id, laoreet elit. Donec auctor arcu ac mi feugiat, vel lobortis justo efficitur. Fusce vel odio vitae justo varius dignissim. Integer sollicitudin mi a justo bibendum ultrices. Quisque id nisl a lectus venenatis luctus. Please note that Lorem ipsum text is a nonsensical Latin-like text used as a placeholder for content, and it has no specific meaning. It's often used in design and publishing to demonstrate the visual aspects of a document without focusing on the actual content.\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 29 line 1\n----&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X41sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; AssistantMessage(message=\"Certainly! Lorem ipsum is a placeholder text commonly used in the printing and typesetting industry. Here's a sample of Lorem ipsum text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam euismod velit vel tellus tempor, non viverra eros iaculis. Sed vel nisl nec mauris bibendum tincidunt. Vestibulum sed libero euismod, eleifend tellus id, laoreet elit. Donec auctor arcu ac mi feugiat, vel lobortis justo efficitur. Fusce vel odio vitae justo varius dignissim. Integer sollicitudin mi a justo bibendum ultrices. Quisque id nisl a lectus venenatis luctus. Please note that Lorem ipsum text is a nonsensical Latin-like text used as a placeholder for content, and it has no specific meaning. It's often used in design and publishing to demonstrate the visual aspects of a document without focusing on the actual content.\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for AssistantMessage\nmessage\n  String should have at most 100 characters [type=string_too_long, input_value=\"Certainly! Lorem ipsum i... on the actual content.\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/string_too_long</pre> <p>When incorporating external knowledge bases, it's crucial to ensure that the agent uses the provided context accurately and doesn't fabricate responses. Validators can be effectively used for this purpose. We can illustrate this with an example where we validate that a provided citation is actually included in the referenced text chunk:</p> In\u00a0[70]: Copied! <pre>from pydantic import ValidationInfo\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: str\n\n    @field_validator('citation')\n    @classmethod\n    def citation_exists(cls, v: str, info: ValidationInfo): \n        context = info.context\n        if context:\n            context = context.get('text_chunk')\n            if v not in context:\n                raise ValueError(f\"Citation `{v}` not found in text\")\n        return v\n</pre> from pydantic import ValidationInfo  class AnswerWithCitation(BaseModel):     answer: str     citation: str      @field_validator('citation')     @classmethod     def citation_exists(cls, v: str, info: ValidationInfo):          context = info.context         if context:             context = context.get('text_chunk')             if v not in context:                 raise ValueError(f\"Citation `{v}` not found in text\")         return v <p>Here we assume that there is a \"text_chunk\" field that contains the text that the model is supposed to use as context. We then use the <code>field_validator</code> decorator to define a validator that checks if the citation is included in the text chunk. If it's not, we raise a <code>ValueError</code> with a message that will be returned to the user.</p> In\u00a0[71]: Copied! <pre>AnswerWithCitation.model_validate(\n    {\n        \"answer\": \"Blueberries are packed with protein\", \n        \"citation\": \"Blueberries contain high levels of protein\"\n    },\n    context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"}, \n)\n</pre> AnswerWithCitation.model_validate(     {         \"answer\": \"Blueberries are packed with protein\",          \"citation\": \"Blueberries contain high levels of protein\"     },     context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"},  ) <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 34 line 1\n----&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; AnswerWithCitation.model_validate(\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt;     {\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt;         \"answer\": \"Blueberries are packed with protein\", \n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt;         \"citation\": \"Blueberries contain high levels of protein\"\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt;     },\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt;     context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"}, \n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt; )\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:503, in BaseModel.model_validate(cls, obj, strict, from_attributes, context)\n    501 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    502 __tracebackhide__ = True\n--&gt; 503 return cls.__pydantic_validator__.validate_python(\n    504     obj, strict=strict, from_attributes=from_attributes, context=context\n    505 )\n\nValidationError: 1 validation error for AnswerWithCitation\ncitation\n  Value error, Citation `Blueberries contain high levels of protein` not found in text [type=value_error, input_value='Blueberries contain high levels of protein', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error</pre> <p>For scenarios requiring more nuanced validation than rule-based methods, we use probabilistic validation. This approach incorporates LLMs into the validation workflow for a sophisticated assessment of outputs.</p> <p>The <code>instructor</code> library offers the <code>llm_validator</code> utility for this purpose. By specifying the desired directive, we can use LLMs for complex validation tasks. Let's explore some intriguing use cases enabled by LLMs.</p> <p>This LLM will be tasked with determining whether the agent's responses are exclusively related to health topics. For this, we will use the <code>llm_validator</code> from <code>instructor</code> like so:</p> In\u00a0[73]: Copied! <pre>from instructor import llm_validator\n\nclass AssistantMessage(BaseModel):\n    message: Annotated[str, \n                       AfterValidator(\n                           llm_validator(\"don't talk about any other topic except health best practices and topics\", \n                                         openai_client=client))]\n\nAssistantMessage(message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\")\n</pre> from instructor import llm_validator  class AssistantMessage(BaseModel):     message: Annotated[str,                         AfterValidator(                            llm_validator(\"don't talk about any other topic except health best practices and topics\",                                           openai_client=client))]  AssistantMessage(message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 38 line 1\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt; class AssistantMessage(BaseModel):\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt;     message: Annotated[str, \n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt;                        AfterValidator(\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=7'&gt;8&lt;/a&gt;                            llm_validator(\"don't talk about any other topic except health best practices and topics\", \n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=8'&gt;9&lt;/a&gt;                                          openai_client=client))]\n---&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=10'&gt;11&lt;/a&gt; AssistantMessage(message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for AssistantMessage\nmessage\n  Assertion failed, The statement is not related to health best practices or topics. [type=assertion_error, input_value='I would suggest you to v...is very nice in winter.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/assertion_error</pre> <p>Important that for these examples we're not waiting for the messages, to get this message we would need to call the openai with <code>response_model=AssistantMessage</code>.</p> <p>Using probabilistic validation, we can also assess the agent's reasoning process to ensure it's logical before providing a response. With chain of thought prompting, the model is expected to think in steps and arrive at an answer following its logical progression. If there are errors in this logic, the final response may be incorrect.</p> <p>Here we will use Pydantic's model_validator which allows us to apply validation over all the properties of the <code>AIResponse</code> at once.</p> <p>To make this easier we'll make a simple validation class that we can reuse for all our validation:</p> In\u00a0[74]: Copied! <pre>from typing import Optional\n\nclass Validation(BaseModel):\n    is_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\")\n    error_message: Optional[str] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\")\n</pre> from typing import Optional  class Validation(BaseModel):     is_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\")     error_message: Optional[str] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\") <p>The function we will call will integrate an LLM and will ask it to determine whether the answer the model provided follows from the chain of thought:</p> In\u00a0[75]: Copied! <pre>def validate_chain_of_thought(values):\n    chain_of_thought = values[\"chain_of_thought\"]\n    answer = values[\"answer\"]\n    resp = client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value follows from the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",\n            },\n        ],\n        response_model=Validation,\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return values\n</pre> def validate_chain_of_thought(values):     chain_of_thought = values[\"chain_of_thought\"]     answer = values[\"answer\"]     resp = client.chat.completions.create(         model=\"gpt-4-1106-preview\",         messages=[             {                 \"role\": \"system\",                 \"content\": \"You are a validator. Determine if the value follows from the statement. If it is not, explain why.\",             },             {                 \"role\": \"user\",                 \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",             },         ],         response_model=Validation,     )     if not resp.is_valid:         raise ValueError(resp.error_message)     return values <p>The use of the 'before' argument in this context is significant. It means that the validator will receive the complete dictionary of inputs in their raw form, before any parsing by Pydantic.</p> In\u00a0[76]: Copied! <pre>from typing import Any\nfrom pydantic import model_validator\n\nclass AIResponse(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n    @model_validator(mode='before')\n    @classmethod\n    def chain_of_thought_makes_sense(cls, data: Any) -&gt; Any:\n        # here we assume data is the dict representation of the model\n        # since we use 'before' mode.\n        return validate_chain_of_thought(data)\n</pre> from typing import Any from pydantic import model_validator  class AIResponse(BaseModel):     chain_of_thought: str     answer: str      @model_validator(mode='before')     @classmethod     def chain_of_thought_makes_sense(cls, data: Any) -&gt; Any:         # here we assume data is the dict representation of the model         # since we use 'before' mode.         return validate_chain_of_thought(data) In\u00a0[77]: Copied! <pre>AIResponse(chain_of_thought=\"The user suffers from diabetes.\", answer=\"The user has a broken leg.\")\n</pre> AIResponse(chain_of_thought=\"The user suffers from diabetes.\", answer=\"The user has a broken leg.\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 47 line 1\n----&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#Y103sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; AIResponse(chain_of_thought=\"The user suffers from diabetes.\", answer=\"The user has a broken leg.\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for AIResponse\n  Value error, The statement about the user having a broken leg does not logically follow from the information provided about the user suffering from diabetes. These are two separate health conditions and one does not imply the other. [type=value_error, input_value={'chain_of_thought': 'The...user has a broken leg.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error</pre> <p>Integrating these validation examples with the OpenAI API is streamlined using <code>instructor</code>. After patching the OpenAI client with <code>instructor</code>, you simply need to specify a <code>response_model</code> for your requests. This setup ensures that all the validation processes occur automatically.</p> <p>To enable reasking you can set a maximum number of retries. When calling the OpenAI client, the system can re-attempt to generate a correct answer. It does this by resending the original query along with feedback on why the previous response was rejected, guiding the LLM towards a more accurate answer in subsequent attempts.</p> In\u00a0[79]: Copied! <pre>class QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\nquestion = \"What is the meaning of life?\"\ncontext = \"The according to the devil the meaning of life is a life of sin and debauchery.\"\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\n        },\n    ],\n)\n\nresp.answer\n</pre> class QuestionAnswer(BaseModel):     question: str     answer: str  question = \"What is the meaning of life?\" context = \"The according to the devil the meaning of life is a life of sin and debauchery.\"   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     response_model=QuestionAnswer,     messages=[         {             \"role\": \"system\",             \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",         },         {             \"role\": \"user\",             \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",         },     ], )  resp.answer Out[79]: <pre>'a life of sin and debauchery'</pre> In\u00a0[80]: Copied! <pre>from pydantic import BeforeValidator\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\")\n        ),\n    ]\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    max_retries=2,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\n        },\n    ],\n)\n\nresp.answer\n</pre> from pydantic import BeforeValidator  class QuestionAnswer(BaseModel):     question: str     answer: Annotated[         str,         BeforeValidator(             llm_validator(\"don't say objectionable things\")         ),     ]  resp = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     response_model=QuestionAnswer,     max_retries=2,     messages=[         {             \"role\": \"system\",             \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",         },         {             \"role\": \"user\",             \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",         },     ], )  resp.answer Out[80]: <pre>'The meaning of life is a concept that varies depending on individual perspectives and beliefs.'</pre> <p>This guide explains how to use deterministic and probabilistic validation techniques with Large Language Models (LLMs). We discussed using an instructor to establish validation processes for content filtering, context relevance maintenance, and model reasoning verification. These methods enhance the performance of LLMs across different tasks.</p> <p>For those interested in further exploration, here's a to-do list:</p> <ol> <li>SQL Syntax Checker: Create a validator to check the syntax of SQL queries before executing them.</li> <li>Context-Based Response Validation: Design a method to flag responses based on the model's own knowledge rather than the provided context.</li> <li>PII Detection: Implement a mechanism to identify and handle Personally Identifiable Information in responses while prioritizing user privacy.</li> <li>Targeted Rule-Based Filtering: Develop filters to remove specific content types, such as responses mentioning named entities.</li> </ol> <p>Completing these tasks will enable users to acquire practical skills in improving LLMs through advanced validation methods.</p>"},{"location":"tutorials/4-validation/#validators","title":"Validators\u00b6","text":""},{"location":"tutorials/4-validation/#setup-and-dependencies","title":"Setup and Dependencies\u00b6","text":""},{"location":"tutorials/4-validation/#software-20-rule-based-validators","title":"Software 2.0: Rule-based validators\u00b6","text":""},{"location":"tutorials/4-validation/#flagging-bad-keywords","title":"Flagging bad keywords\u00b6","text":""},{"location":"tutorials/4-validation/#flagging-using-openai-moderation","title":"Flagging using OpenAI Moderation\u00b6","text":""},{"location":"tutorials/4-validation/#filtering-very-long-messages","title":"Filtering very long messages\u00b6","text":""},{"location":"tutorials/4-validation/#avoiding-hallucination-with-citations","title":"Avoiding hallucination with citations\u00b6","text":""},{"location":"tutorials/4-validation/#software-30-probabilistic-validators","title":"Software 3.0: Probabilistic validators\u00b6","text":""},{"location":"tutorials/4-validation/#keeping-an-agent-on-topic","title":"Keeping an agent on topic\u00b6","text":"<p>When creating an agent focused on health improvement, providing answers and daily practice suggestions, it's crucial to ensure strict adherence to health-related topics. This is important because the knowledge base is limited to health topics, and veering off-topic could result in fabricated responses.</p> <p>To achieve this focus, we'll follow a similar process as before, but with an important addition: integrating an LLM into our validator.</p>"},{"location":"tutorials/4-validation/#validating-agent-thinking-with-cot","title":"Validating agent thinking with CoT\u00b6","text":""},{"location":"tutorials/4-validation/#reasking-with-validators","title":"Reasking with validators\u00b6","text":"<p>For most of these examples all we've done we've mostly only defined the validation logic.</p> <p>We'eve covered field validators and model validators and even used LLMs to validate our outputs. But we haven't actually used the validators to reask the model! One of the most powerful features of <code>instructor</code> is that it will automatically reask the model when it receives a validation error. This means that we can use the same validation logic for both code-based and LLM-based validation.</p> <p>This also means that our 'prompt' is not only the prompt we send, but the code that runs the validator, and the error message we send back to the model.</p>"},{"location":"tutorials/4-validation/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"tutorials/5-knowledge-graphs/","title":"Knowledge Graphs","text":"<p>Today, we're going to use the <code>instructor</code> library to simplify the interaction between OpenAI and our code. Along with Graphviz library to bring structure to our intricate subjects and have a graph visualization.</p> In\u00a0[2]: Copied! <pre>import instructor \nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n</pre> import instructor  from openai import OpenAI  client = instructor.patch(OpenAI()) <p>Install the Graphviz based on your operation system https://graphviz.org/download/</p> In\u00a0[3]: Copied! <pre>from pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n</pre> from pydantic import BaseModel, Field from typing import List, Optional  class Node(BaseModel):     id: int     label: str     color: str  class Edge(BaseModel):     source: int     target: int     label: str     color: str = \"black\" <p>The <code>KnowledgeGraph</code> class combines nodes and edges to create a comprehensive graph structure. It includes lists of nodes and edges, where each node represents a key concept or entity, and each edge represents a relationship between two nodes.</p> <p>Later on, you'll see that we designed this class to match the graph object in the graphviz library, which makes it easier to visualize our graph.</p> <p>The <code>visualize_knowledge_graph</code> function is used to visualize a knowledge graph. It takes a <code>KnowledgeGraph</code> object as input, which contains nodes and edges. The function utilizes the <code>graphviz</code> library to generate a directed graph (<code>Digraph</code>). Each node and edge from the <code>KnowledgeGraph</code> is added to the <code>Digraph</code> with their respective attributes (id, label, color). Finally, the graph is rendered and displayed.</p> In\u00a0[4]: Copied! <pre>from graphviz import Digraph\nfrom IPython.display import display\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(..., default_factory=list)  # A list of nodes in the knowledge graph.\n    edges: List[Edge] = Field(..., default_factory=list)  # A list of edges in the knowledge graph.\n\n\n    def visualize_knowledge_graph(self):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:\n            dot.node(name=str(node.id), label=node.label, color=node.color)\n        for edge in self.edges:\n            dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n        \n        return display(dot)\n</pre> from graphviz import Digraph from IPython.display import display  class KnowledgeGraph(BaseModel):     nodes: List[Node] = Field(..., default_factory=list)  # A list of nodes in the knowledge graph.     edges: List[Edge] = Field(..., default_factory=list)  # A list of edges in the knowledge graph.       def visualize_knowledge_graph(self):         dot = Digraph(comment=\"Knowledge Graph\")          for node in self.nodes:             dot.node(name=str(node.id), label=node.label, color=node.color)         for edge in self.edges:             dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)                  return display(dot)  In\u00a0[8]: Copied! <pre>def generate_graph(input) -&gt; KnowledgeGraph:\n    return client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Help me understand the following by describing it as small knowledge graph: {input}\",\n            }\n        ],\n        response_model=KnowledgeGraph,\n    )\n</pre> def generate_graph(input) -&gt; KnowledgeGraph:     return client.chat.completions.create(         model=\"gpt-4-1106-preview\",         messages=[             {                 \"role\": \"user\",                 \"content\": f\"Help me understand the following by describing it as small knowledge graph: {input}\",             }         ],         response_model=KnowledgeGraph,     ) In\u00a0[9]: Copied! <pre>generate_graph(\"Explain quantum mechanics\").visualize_knowledge_graph()\n</pre> generate_graph(\"Explain quantum mechanics\").visualize_knowledge_graph() In\u00a0[10]: Copied! <pre>class Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\n    def __hash__(self) -&gt; int:\n        return hash((id, self.label))\n    \nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\n    def __hash__(self) -&gt; int:\n        return hash((self.source, self.target, self.label))\n</pre> class Node(BaseModel):     id: int     label: str     color: str      def __hash__(self) -&gt; int:         return hash((id, self.label))      class Edge(BaseModel):     source: int     target: int     label: str     color: str = \"black\"      def __hash__(self) -&gt; int:         return hash((self.source, self.target, self.label)) In\u00a0[11]: Copied! <pre>class KnowledgeGraph(BaseModel):\n    # Optional list of nodes and edges in the knowledge graph\n    nodes: Optional[List[Node]] = Field(..., default_factory=list)\n    edges: Optional[List[Edge]] = Field(..., default_factory=list)\n\n    def update(self, other: \"KnowledgeGraph\") -&gt; \"KnowledgeGraph\":\n        # This method updates the current graph with the other graph, deduplicating nodes and edges.\n        return KnowledgeGraph(\n            nodes=list(set(self.nodes + other.nodes)),  # Combine and deduplicate nodes\n            edges=list(set(self.edges + other.edges)),  # Combine and deduplicate edges\n        )\n    \n\n    def visualize_knowledge_graph(self):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:\n            dot.node(str(node.id), node.label, color=node.color)\n        for edge in self.edges:\n            dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n        \n        return display(dot)\n</pre> class KnowledgeGraph(BaseModel):     # Optional list of nodes and edges in the knowledge graph     nodes: Optional[List[Node]] = Field(..., default_factory=list)     edges: Optional[List[Edge]] = Field(..., default_factory=list)      def update(self, other: \"KnowledgeGraph\") -&gt; \"KnowledgeGraph\":         # This method updates the current graph with the other graph, deduplicating nodes and edges.         return KnowledgeGraph(             nodes=list(set(self.nodes + other.nodes)),  # Combine and deduplicate nodes             edges=list(set(self.edges + other.edges)),  # Combine and deduplicate edges         )           def visualize_knowledge_graph(self):         dot = Digraph(comment=\"Knowledge Graph\")          for node in self.nodes:             dot.node(str(node.id), node.label, color=node.color)         for edge in self.edges:             dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)                  return display(dot)  In\u00a0[12]: Copied! <pre>def generate_graph(input: List[str]) -&gt; KnowledgeGraph:\n    # Initialize an empty KnowledgeGraph\n    cur_state = KnowledgeGraph()\n\n    # Iterate over the input list\n    for i, inp in enumerate(input):\n        new_updates = client.chat.completions.create(\n            model=\"gpt-4-1106-preview\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an iterative knowledge graph builder.\n                    You are given the current state of the graph, and you must append the nodes and edges \n                    to it Do not procide any duplcates and try to reuse nodes as much as possible.\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Extract any new nodes and edges from the following:\n                    # Part {i}/{len(input)} of the input:\n\n                    {inp}\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Here is the current state of the graph:\n                    {cur_state.model_dump_json(indent=2)}\"\"\",\n                },\n            ],\n            response_model=KnowledgeGraph,\n        )  # type: ignore\n\n        # Update the current state with the new updates\n        cur_state = cur_state.update(new_updates)\n\n        # Draw the current state of the graph\n        cur_state.visualize_knowledge_graph() \n        \n    # Return the final state of the KnowledgeGraph\n    return cur_state\n</pre> def generate_graph(input: List[str]) -&gt; KnowledgeGraph:     # Initialize an empty KnowledgeGraph     cur_state = KnowledgeGraph()      # Iterate over the input list     for i, inp in enumerate(input):         new_updates = client.chat.completions.create(             model=\"gpt-4-1106-preview\",             messages=[                 {                     \"role\": \"system\",                     \"content\": \"\"\"You are an iterative knowledge graph builder.                     You are given the current state of the graph, and you must append the nodes and edges                      to it Do not procide any duplcates and try to reuse nodes as much as possible.\"\"\",                 },                 {                     \"role\": \"user\",                     \"content\": f\"\"\"Extract any new nodes and edges from the following:                     # Part {i}/{len(input)} of the input:                      {inp}\"\"\",                 },                 {                     \"role\": \"user\",                     \"content\": f\"\"\"Here is the current state of the graph:                     {cur_state.model_dump_json(indent=2)}\"\"\",                 },             ],             response_model=KnowledgeGraph,         )  # type: ignore          # Update the current state with the new updates         cur_state = cur_state.update(new_updates)          # Draw the current state of the graph         cur_state.visualize_knowledge_graph()               # Return the final state of the KnowledgeGraph     return cur_state  <p>In this approach, we process the text in manageable chunks, one at a time.</p> <p>This method is particularly beneficial when dealing with extensive text that may not fit into a single prompt.</p> <p>It is especially useful in scenarios such as constructing a knowledge graph for a complex topic, where the information is distributed across multiple documents or sections.</p> In\u00a0[13]: Copied! <pre>text_chunks = [\n    \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\",\n    \"Professors are smart.\",\n    \"Sarah knows Jason and is a student of his.\",\n    \"Sarah is a student at the University of Toronto. and UofT is in Canada.\",\n]\n\ngraph: KnowledgeGraph = generate_graph(text_chunks)\n</pre> text_chunks = [     \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\",     \"Professors are smart.\",     \"Sarah knows Jason and is a student of his.\",     \"Sarah is a student at the University of Toronto. and UofT is in Canada.\", ]  graph: KnowledgeGraph = generate_graph(text_chunks) <p>This tutorial shows how to generate and visualize a knowledge graph for complex topics. It also demonstrates how to extract graphic knowledge from the language model or provided text. The tutorial highlights the iterative process of building the knowledge graph by processing text in smaller chunks and updating the graph with new information.</p> <p>Using this approach, we can extract various things, including:</p> <ol> <li>People and their relationships in a story.</li> </ol> <pre>class People(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Relationship(BaseModel):\n    id: str\n    source: str\n    target: str\n    label: str\n    description: str\n\nclass Story(BaseModel):\n    people: List[People]\n    relationships: List[Relationship]\n</pre> <ol> <li>Task dependencies and action items from a transcript.</li> </ol> <pre>class Task(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Participant(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Assignment(BaseModel):\n    id: str\n    source: str\n    target: str\n    label: str\n    description: str\n\nclass Transcript(BaseModel):\n    tasks: List[Task]\n    participants: List[Participant]\n    assignments: List[Assignment]\n</pre> <ol> <li>Key concepts and their relationships from a research paper.</li> <li>Entities and their relationships from a news article.</li> </ol> <p>As an exercise, try to implement one of the above examples.</p> <p>All of them will follow an idea of iteratively extracting more and more information and accumulating it into some state.</p>"},{"location":"tutorials/5-knowledge-graphs/#knowledge-graphs-for-complex-topics","title":"Knowledge Graphs for Complex Topics\u00b6","text":""},{"location":"tutorials/5-knowledge-graphs/#introduction","title":"Introduction\u00b6","text":"<p>What is a knowledge graph?</p> <p>A knowledge graph, also known as a semantic network, represents real-world entities and their relationships. It consists of nodes, edges, and labels. Nodes can represent any entity, while edges define the connections between them. For example, a node representing an author like \"J.K. Rowling\" can be connected to another node representing one of her books, \"Harry Potter\", with the edge \"author of\".</p> <p>Applications of knowledge graphs</p> <p>Knowledge graphs have various applications, including:</p> <ul> <li>Search Engines: They enhance search results by incorporating semantic-search information from diverse sources.</li> <li>Recommendation Systems: They suggest products or services based on user behavior and preferences.</li> <li>Natural Language Processing: They aid in understanding and generating human language.</li> <li>Data Integration: They facilitate the integration of data from different sources by identifying relationships.</li> <li>Artificial Intelligence and Machine Learning: They provide contextual information to improve decision-making.</li> </ul>"},{"location":"tutorials/5-knowledge-graphs/#setup-and-dependencies","title":"Setup and Dependencies\u00b6","text":""},{"location":"tutorials/5-knowledge-graphs/#node-and-edge-classes","title":"Node and Edge Classes\u00b6","text":"<p>We begin by modeling our knowledge graph with Node and Edge objects.</p> <p>Node objects represent key concepts or entities, while Edge objects signify the relationships between them.</p>"},{"location":"tutorials/5-knowledge-graphs/#knowledgegraph-class","title":"<code>KnowledgeGraph</code> Class\u00b6","text":""},{"location":"tutorials/5-knowledge-graphs/#generating-the-knowledge-graph","title":"Generating the Knowledge Graph\u00b6","text":""},{"location":"tutorials/5-knowledge-graphs/#generate_graph-function","title":"generate_graph function\u00b6","text":"<p>The <code>generate_graph</code> function uses OpenAI's model to create a KnowledgeGraph object from an input string.</p> <p>It requests the model to interpret the input as a detailed knowledge graph and uses the response to form the KnowledgeGraph object.</p>"},{"location":"tutorials/5-knowledge-graphs/#advanced-accumulating-knowledge-graphs","title":"Advanced: Accumulating Knowledge Graphs\u00b6","text":"<p>When dealing with larger datasets, or knowledge that grows over time, processing them all at once can be challenging due to limitations in prompt length or the complexity of the content. In such cases, an iterative approach to building the knowledge graph can be beneficial. This method involves processing the text in smaller, manageable chunks and updating the graph with new information from each chunk.</p>"},{"location":"tutorials/5-knowledge-graphs/#what-are-the-benefits-of-this-approach","title":"What are the benefits of this approach?\u00b6","text":"<ul> <li><p>Scalability: This approach can handle large datasets by breaking them down into smaller, more manageable pieces.</p> </li> <li><p>Flexibility: It allows for dynamic updates to the graph, accommodating new information as it becomes available.</p> </li> <li><p>Efficiency: Processing smaller chunks of text can be more efficient and less prone to errors or omissions.</p> </li> </ul>"},{"location":"tutorials/5-knowledge-graphs/#what-has-changed","title":"What has changed?\u00b6","text":"<p>The previous example provided a basic structure, while this new example introduces additional complexity and functionality. The Node and Edge classes now have a hash method, allowing them to be used in sets and simplifying duplicate handling.</p> <p>The KnowledgeGraph class has been enhanced with two new methods: <code>update</code> and <code>draw</code>.</p> <p>In the KnowledgeGraph class, the nodes and edges fields are now optional, offering greater flexibility.</p> <p>The <code>update</code> method enables the merging and removal of duplicates from two graphs.</p> <p>The <code>draw</code> method includes a prefix parameter, making it easier to create different graph versions during iterations.</p>"},{"location":"tutorials/5-knowledge-graphs/#generate-iterative-graphs","title":"Generate iterative graphs\u00b6","text":"<p>The updated <code>generate_graph</code> function is specifically designed to handle a list of inputs iteratively. It updates the graph with each new piece of information.</p> <p>Upon closer inspection, this pattern resembles a common programming technique known as a \"reduce\" or \"fold\" function. A simple example of this would be iterating over a list to find the sum of all the elements squared.</p> <p>Here's an example in Python:</p> <pre>cur_state = 0\nfor i in [1, 2, 3, 4, 5]:\n    cur_state += i**2\nprint(cur_state)\n</pre>"},{"location":"tutorials/5-knowledge-graphs/#examples-use-case","title":"Examples Use Case\u00b6","text":""},{"location":"tutorials/5-knowledge-graphs/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"tutorials/6-chain-of-density/","title":"6 chain of density","text":"<p>Article: {{ARTICLE}}</p> <p>You will generate increasingly concise, entity-dense summaries of the above Article.</p> <p>Repeat the following 2 steps 5 times.</p> <p>Step 1. Identify 1-3 informative Entities (\";\" delimited) from the Article which are missing from the previously generated summary. Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.</p> <p>A Missing Entity is:</p> <ul> <li>Relevant: to the main story.</li> <li>Specific: descriptive yet concise (5 words or fewer).</li> <li>Novel; not in the previous summary.</li> <li>Faithful: present in the Article.</li> <li>Anywhere: located anywhere in the Article.</li> </ul> <p>Guidelines: phrases like \"the article discusses\"</p> <ul> <li>The first summary should be long (4-5 sentences, -80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach -80 words.</li> <li>Make every word count: re-write the previous summary to improve flow and make space for additional entities.</li> <li>Make space with fusion, compression, and removal of uninformative</li> </ul> <ul> <li>The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.</li> <li>Missing entities can appear anywhere in the new summary.</li> <li>Never drop entities from the previous summary. If space cannot be made, add fewer new entities.</li> </ul> <p>Remember, use the exact same number of words for each summary.</p> <p>Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\"</p> <p>While the original paper used a single prompt to generate the iterative generations, we can go one step better with <code>Instructor</code> and break down the process into smaller API calls - with validation along the way.</p> <p>The process can be broken down as seen below.</p> <p></p> <p>We'll need to install the tokenizer packages and the spacy english library before we can proceed with the rest of the lesson</p> In\u00a0[1]: Copied! <pre>import nltk\nnltk.download('punkt')\n\n!python -m spacy download en_core_web_sm --quiet\n</pre> import nltk nltk.download('punkt')  !python -m spacy download en_core_web_sm --quiet <pre>[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n</pre> <pre>\u2714 Download and installation successful\nYou can now load the package via spacy.load('en_core_web_sm')\n</pre> <p>Once that's done, let's now move on to writing some code.</p> <p>There are a few different definitions which we'll need to understand in the tutorial. They are</p> <ol> <li>Tokens and tokenizers</li> <li>Entities</li> <li>Entity-Dense</li> </ol> <p>Once we've gotten a hang of these concepts, we'll walk through a simple implementation of a Chain Of Density summarizer</p> In\u00a0[2]: Copied! <pre>import nltk\nsentence = \"My favourite type of Sashimi is Toro\"\n\nnltk.word_tokenize(sentence)\n</pre> import nltk sentence = \"My favourite type of Sashimi is Toro\"  nltk.word_tokenize(sentence) Out[2]: <pre>['My', 'favourite', 'type', 'of', 'Sashimi', 'is', 'Toro']</pre> <p>NLTK's word tokenizer does more than just split by empty whitespace. It handles a lot of nice edge cases and contractions such as <code>don't</code> or <code>I'm</code>.</p> In\u00a0[3]: Copied! <pre>sentence = \"I'm fascinated by machine learning!\"\n\nnltk.word_tokenize(sentence)\n</pre> sentence = \"I'm fascinated by machine learning!\"  nltk.word_tokenize(sentence) Out[3]: <pre>['I', \"'m\", 'fascinated', 'by', 'machine', 'learning', '!']</pre> <p>We can then calculate the number of tokens by simply finding the <code>len</code> of the generated sequence.</p> In\u00a0[4]: Copied! <pre>sentence = \"I'm fascinated by machine learning!\"\ntokens = nltk.word_tokenize(sentence)\nprint(tokens)\nprint(len(tokens))\n</pre> sentence = \"I'm fascinated by machine learning!\" tokens = nltk.word_tokenize(sentence) print(tokens) print(len(tokens)) <pre>['I', \"'m\", 'fascinated', 'by', 'machine', 'learning', '!']\n7\n</pre> In\u00a0[5]: Copied! <pre># First we load in the library\nimport spacy\n\n# Then we initialise an NLP object. \nnlp = spacy.load(\"en_core_web_sm\")\n</pre> # First we load in the library import spacy  # Then we initialise an NLP object.  nlp = spacy.load(\"en_core_web_sm\") In\u00a0[6]: Copied! <pre>sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n\ndoc = nlp(sentence)\ndoc.ents\n</pre> sentence = \"Apple is looking at buying U.K. startup for $1 billion\"  doc = nlp(sentence) doc.ents Out[6]: <pre>(Apple, U.K., $1 billion)</pre> <p>We can see that Spacy was able to identify unique and named entities that were present within the sentence using the <code>doc.ents</code> property. Let's see a few more examples.</p> In\u00a0[7]: Copied! <pre>sentence = \"A knowledge graph, also known as a semantic network\\\n, represents real-world entities and their relationships\"\n\ndoc = nlp(sentence)\ndoc.ents\n</pre> sentence = \"A knowledge graph, also known as a semantic network\\ , represents real-world entities and their relationships\"  doc = nlp(sentence) doc.ents Out[7]: <pre>()</pre> In\u00a0[8]: Copied! <pre>sentence = \"For example, a node representing an author like 'J.K. Rowling'\\\ncan be connected to another node representing one of her books, 'Harry Potter'\\\n, with the edge 'author of'\"\n\ndoc = nlp(sentence)\ndoc.ents\n</pre> sentence = \"For example, a node representing an author like 'J.K. Rowling'\\ can be connected to another node representing one of her books, 'Harry Potter'\\ , with the edge 'author of'\"  doc = nlp(sentence) doc.ents Out[8]: <pre>(J.K., one, Harry Potter')</pre> <p>As we can see from the examples above, entities are not nouns. They're direct or indirect references to people, places, concepts.</p> In\u00a0[9]: Copied! <pre>import math\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef calculate_entity_density(sentence:str):\n    tokens = nltk.word_tokenize(sentence)\n    entities = nlp(sentence).ents\n    entity_density = round(len(entities)/len(tokens),3)\n\n    return len(tokens),len(entities),entity_density\n</pre> import math nlp = spacy.load(\"en_core_web_sm\")  def calculate_entity_density(sentence:str):     tokens = nltk.word_tokenize(sentence)     entities = nlp(sentence).ents     entity_density = round(len(entities)/len(tokens),3)      return len(tokens),len(entities),entity_density In\u00a0[10]: Copied! <pre>sentence_1 = \"A knowledge graph, also known as a semantic network\\\n, represents real-world entities and their relationships\"\n\ncalculate_entity_density(sentence_1)\n</pre> sentence_1 = \"A knowledge graph, also known as a semantic network\\ , represents real-world entities and their relationships\"  calculate_entity_density(sentence_1) Out[10]: <pre>(17, 0, 0.0)</pre> In\u00a0[11]: Copied! <pre>sentence_2 = \"Apple is looking at buying U.K. startup for $1 billion\"\n\ncalculate_entity_density(sentence_2)\n</pre> sentence_2 = \"Apple is looking at buying U.K. startup for $1 billion\"  calculate_entity_density(sentence_2) Out[11]: <pre>(11, 3, 0.273)</pre> <p>This gives us a quantitative method to be able to understand and compare two different sentences/summaries.</p> <p>We want summaries that are more entity-dense</p> In\u00a0[12]: Copied! <pre>summary_1 = \"\"\"\nThis article discusses an incident that occurred during the Chinese Grand Prix\ninvolving two racing drivers, Jenson Button and Pastor Maldonado. The two were \ncompeting for the 13th place when Button collided with Maldonado's vehicle, \ncausing damage to both cars. The incident resulted in a penalty for Button, \nwho was demoted to 14th place. Maldonado, on the other hand, had to retire from \nthe race due to the damage his car sustained.\n\"\"\"\n\nsummary_2 = \"\"\"\nJenson Button's McLaren collided with Pastor Maldonado's Lotus during the Chinese \nGrand Prix, causing front wing damage to Button's car and rear-end damage to \nMaldonado's, forcing his retirement. Button received a five-second penalty and \ntwo superlicence points, dropping himto 14th. Fernando Alonso advanced two places, \nwhile Button was lapped by Nico Rosberg and Alonso by Sebastian Vettel and \nKimi Raikkonen.\n\"\"\"\n\ncalculate_entity_density(summary_1),calculate_entity_density(summary_2)\n</pre> summary_1 = \"\"\" This article discusses an incident that occurred during the Chinese Grand Prix involving two racing drivers, Jenson Button and Pastor Maldonado. The two were  competing for the 13th place when Button collided with Maldonado's vehicle,  causing damage to both cars. The incident resulted in a penalty for Button,  who was demoted to 14th place. Maldonado, on the other hand, had to retire from  the race due to the damage his car sustained. \"\"\"  summary_2 = \"\"\" Jenson Button's McLaren collided with Pastor Maldonado's Lotus during the Chinese  Grand Prix, causing front wing damage to Button's car and rear-end damage to  Maldonado's, forcing his retirement. Button received a five-second penalty and  two superlicence points, dropping himto 14th. Fernando Alonso advanced two places,  while Button was lapped by Nico Rosberg and Alonso by Sebastian Vettel and  Kimi Raikkonen. \"\"\"  calculate_entity_density(summary_1),calculate_entity_density(summary_2) Out[12]: <pre>((82, 11, 0.134), (71, 17, 0.239))</pre> <p>We can see that the final summary is almost twice as dense as the first summary and is hence more entity dense.</p> In\u00a0[13]: Copied! <pre>from pydantic import BaseModel,Field,field_validator\nfrom typing import List\n</pre> from pydantic import BaseModel,Field,field_validator from typing import List In\u00a0[14]: Copied! <pre>class InitialSummary(BaseModel):\n    \"\"\"\n    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n    yet highly non-specific, containing little information beyond the entities marked as missing.\n    Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a summary of the article provided which is overly verbose and uses fillers. \\\n        It should be roughly 80 words in length\",\n    )\n</pre> class InitialSummary(BaseModel):     \"\"\"     This is an initial summary which should be long ( 4-5 sentences, ~80 words)     yet highly non-specific, containing little information beyond the entities marked as missing.     Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.     \"\"\"      summary: str = Field(         ...,         description=\"This is a summary of the article provided which is overly verbose and uses fillers. \\         It should be roughly 80 words in length\",     ) <p>Pydantic is extremely handy because it allows us to do two things</p> <ol> <li>We can validate that our generated outputs are consistent with what we want, and write vanilla python to validate so</li> <li>We can export the generated class definition into a simple schema that fits in perfectly with OpenAI's function calling</li> </ol> In\u00a0[15]: Copied! <pre>InitialSummary.model_json_schema()\n</pre> InitialSummary.model_json_schema() Out[15]: <pre>{'description': 'This is an initial summary which should be long ( 4-5 sentences, ~80 words)\\nyet highly non-specific, containing little information beyond the entities marked as missing.\\nUse overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.',\n 'properties': {'summary': {'description': 'This is a summary of the article provided which is overly verbose and uses fillers.         It should be roughly 80 words in length',\n   'title': 'Summary',\n   'type': 'string'}},\n 'required': ['summary'],\n 'title': 'InitialSummary',\n 'type': 'object'}</pre> <p>It's important here to provide a good description of the overall class and the respective fields. This is because all of the descriptions that we write for the individual fields and the class itself are directly used by the llm when generating outputs.</p> <p>Now, as a quick recap, when we rewrite our summaries at each step, we're performing a few things</p> <ol> <li>We identify any entities from the original article that are relevant which are missing from our current summary</li> <li>We then rewrite our summary, making sure to include as many of these new entities as possible with the goal of increasing the entity density of the new summary</li> <li>We then make sure that we have included all of the entities in our previous summary in the new rewritten summary.</li> </ol> <p>We can express this in the form of the data model seen below called <code>RewrittenSummary</code>.</p> In\u00a0[16]: Copied! <pre>class RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: List[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: List[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n</pre> class RewrittenSummary(BaseModel):     \"\"\"     This is a new, denser summary of identical length which covers every entity     and detail from the previous summary plus the Missing Entities.      Guidelines     - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities     - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.     - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.     - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"     - Missing entities can appear anywhere in the new summary      An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.     \"\"\"      summary: str = Field(         ...,         description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",     )     absent: List[str] = Field(         ...,         default_factory=list,         description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",     )     missing: List[str] = Field(         default_factory=list,         description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",     ) <p>We'd also want our rewritten summary to have</p> <ol> <li>No missing entities =&gt; <code>absent</code> should have a length of 0</li> <li>New entities to be added in the next rewrite -&gt; <code>missing</code> should have at least 1 entry</li> <li>A minimum length of 60 tokens and to have a density of at least 0.08 ( NOTE: 60 tokens and the 0.08 cut off are chosen arbitrarily, feel free to adjust them even higher if you wish. However, this might require you to add more retries in your code )</li> </ol> <p>We can do so using the <code>field_validator</code> that we learnt in the previous lesson. This allows us to add in a validator for a specific field to ensure it meets our requirements.</p> <p>This gives us the final definition of our <code>RewrittenSummary</code> class as seen below</p> In\u00a0[17]: Copied! <pre>class RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: List[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: List[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n        \n    \n    @field_validator(\"summary\")\n    def min_length(cls, v: str):\n        tokens = nltk.word_tokenize(v) \n        num_tokens = len(tokens)\n        if num_tokens &lt; 60:\n            raise ValueError(\n                \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"\n            )\n        return v\n    \n    @field_validator(\"missing\")\n    def has_missing_entities(cls, missing_entities: List[str]):\n        if len(missing_entities) == 0:\n            raise ValueError(\n                \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"\n            )\n        return missing_entities\n    \n    @field_validator(\"absent\")\n    def has_no_absent_entities(cls, absent_entities: List[str]):\n        absent_entity_string = \",\".join(absent_entities)\n        if len(absent_entities) &gt; 0:\n            print(f\"Detected absent entities of {absent_entity_string}\")\n            raise ValueError(\n                f\"Do not omit the following Entities {absent_entity_string} from the new summary\"\n            )\n        return absent_entities\n    \n    @field_validator(\"summary\")\n    def min_entity_density(cls, v: str):\n        tokens = nltk.word_tokenize(v)\n        num_tokens = len(tokens)\n    \n        # Extract Entities\n        doc = nlp(v) \n        num_entities = len(doc.ents)\n    \n        density = num_entities / num_tokens\n        if density &lt; 0.08: \n            raise ValueError(\n                f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"\n            )\n    \n        return v\n</pre> class RewrittenSummary(BaseModel):     \"\"\"     This is a new, denser summary of identical length which covers every entity     and detail from the previous summary plus the Missing Entities.      Guidelines     - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities     - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.     - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.     - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"     - Missing entities can appear anywhere in the new summary      An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.     \"\"\"      summary: str = Field(         ...,         description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",     )     absent: List[str] = Field(         ...,         default_factory=list,         description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",     )     missing: List[str] = Field(         default_factory=list,         description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",     )                   @field_validator(\"summary\")     def min_length(cls, v: str):         tokens = nltk.word_tokenize(v)          num_tokens = len(tokens)         if num_tokens &lt; 60:             raise ValueError(                 \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"             )         return v          @field_validator(\"missing\")     def has_missing_entities(cls, missing_entities: List[str]):         if len(missing_entities) == 0:             raise ValueError(                 \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"             )         return missing_entities          @field_validator(\"absent\")     def has_no_absent_entities(cls, absent_entities: List[str]):         absent_entity_string = \",\".join(absent_entities)         if len(absent_entities) &gt; 0:             print(f\"Detected absent entities of {absent_entity_string}\")             raise ValueError(                 f\"Do not omit the following Entities {absent_entity_string} from the new summary\"             )         return absent_entities          @field_validator(\"summary\")     def min_entity_density(cls, v: str):         tokens = nltk.word_tokenize(v)         num_tokens = len(tokens)              # Extract Entities         doc = nlp(v)          num_entities = len(doc.ents)              density = num_entities / num_tokens         if density &lt; 0.08:              raise ValueError(                 f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"             )              return v In\u00a0[18]: Copied! <pre>from openai import OpenAI\nimport instructor\n\nclient = instructor.patch(OpenAI()) \n\ndef summarize_article(article: str, summary_steps: int = 3):\n    summary_chain = []\n    # We first generate an initial summary\n    summary: InitialSummary = client.chat.completions.create(  \n        model=\"gpt-4-1106-preview\",\n        response_model=InitialSummary,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",\n            },\n            {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n            {\n                \"role\": \"user\",\n                \"content\": \"The generated summary should be about 80 words.\",\n            },\n        ],\n        max_retries=2,\n    )\n    prev_summary = None\n    summary_chain.append(summary.summary)\n    for i in range(summary_steps):\n        missing_entity_message = (\n            []\n            if prev_summary is None\n            else [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",\n                },\n            ]\n        )\n        new_summary: RewrittenSummary = client.chat.completions.create( \n            model=\"gpt-4-1106-preview\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                You are going to generate an increasingly concise,entity-dense summary of the following article.\n\n                Perform the following two tasks\n                - Identify 1-3 informative entities from the following article which is missing from the previous summary\n                - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities\n\n                Guidelines\n                - Make every word count: re-write the previous summary to improve flow and make space for additional entities\n                - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n                - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n                - Missing entities can appear anywhere in the new summary\n                - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n                \"\"\",\n                },\n                {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\n                },\n                *missing_entity_message,\n            ],\n            max_retries=3, \n            max_tokens=1000,\n            response_model=RewrittenSummary,\n        )\n        summary_chain.append(new_summary.summary)\n        prev_summary = new_summary\n\n    return summary_chain\n</pre> from openai import OpenAI import instructor  client = instructor.patch(OpenAI())   def summarize_article(article: str, summary_steps: int = 3):     summary_chain = []     # We first generate an initial summary     summary: InitialSummary = client.chat.completions.create(           model=\"gpt-4-1106-preview\",         response_model=InitialSummary,         messages=[             {                 \"role\": \"system\",                 \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",             },             {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},             {                 \"role\": \"user\",                 \"content\": \"The generated summary should be about 80 words.\",             },         ],         max_retries=2,     )     prev_summary = None     summary_chain.append(summary.summary)     for i in range(summary_steps):         missing_entity_message = (             []             if prev_summary is None             else [                 {                     \"role\": \"user\",                     \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",                 },             ]         )         new_summary: RewrittenSummary = client.chat.completions.create(              model=\"gpt-4-1106-preview\",             messages=[                 {                     \"role\": \"system\",                     \"content\": \"\"\"                 You are going to generate an increasingly concise,entity-dense summary of the following article.                  Perform the following two tasks                 - Identify 1-3 informative entities from the following article which is missing from the previous summary                 - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities                  Guidelines                 - Make every word count: re-write the previous summary to improve flow and make space for additional entities                 - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".                 - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.                 - Missing entities can appear anywhere in the new summary                 - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.                 \"\"\",                 },                 {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},                 {                     \"role\": \"user\",                     \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",                 },                 *missing_entity_message,             ],             max_retries=3,              max_tokens=1000,             response_model=RewrittenSummary,         )         summary_chain.append(new_summary.summary)         prev_summary = new_summary      return summary_chain In\u00a0[19]: Copied! <pre>with open(\"./assets/article.txt\",\"r+\") as file:\n    article = file.readline()\n</pre> with open(\"./assets/article.txt\",\"r+\") as file:     article = file.readline() In\u00a0[\u00a0]: Copied! <pre>%%time\n\nsummaries = summarize_article(article)\n</pre> %%time  summaries = summarize_article(article) <p>We can see that it took roughly 40 seconds to do an iterative chain of density using this article. But does our approach increase the density of each individual summary? We can check by calculating the entity density of each summary in our list of summaries using the <code>calculate_entity_density</code> function we defined above.</p> In\u00a0[\u00a0]: Copied! <pre>for index,summary in enumerate(summaries):\n    tokens,entity,density = calculate_entity_density(summary)\n    print(f\"Article {index+1} -&gt; Results (Tokens: {tokens}, Entity Count: {entity}, Density: {density})\")\n</pre> for index,summary in enumerate(summaries):     tokens,entity,density = calculate_entity_density(summary)     print(f\"Article {index+1} -&gt; Results (Tokens: {tokens}, Entity Count: {entity}, Density: {density})\") <p>We can take a look at the articles themselves to see if they qualitatively show improvement</p> In\u00a0[\u00a0]: Copied! <pre>for summary in summaries:\n    print(f\"\\n{summary}\\n\")\n</pre> for summary in summaries:     print(f\"\\n{summary}\\n\") <p>As we can see, the articles progressively introduce more entities and become more entity dense. We've performed 4 rounds of summarization here but you could definitely do with maybe 2-3 if latency is a significant issue.</p> <p>This guide showed how to to generate complex summaries using chain of density summarization. We spent some time covering how to apply more complex validators - using <code>spaCy</code> and <code>NLTK</code> to ensure we had a minimum number of tokens and entity density as well as how you might apply instructor in a multi-stage process.</p> <p>By building in validation at each step of the proccess, this helps to improve the performance of your LLM across various tasks.</p> <p>For those looking to delve deeper, here are some to-do lists to explore.</p> <ul> <li>Validate Increasing Entity Density: <code>Pydantic</code> exposes a more complex validator that can take in an arbitrary python dictionary. Use the validation context to check the entity density of the previous summary and the new summary to validate that our model has generated a more entity-dense rewrite</li> <li>Fine-Tuning : <code>Instructor</code> comes with a simple to use interface to help you fine-tune other OpenAI models for your needs. This can be accomplished by capturing the outputs of LLMs using the <code>Instructions</code> module to generate training data for fine-tuning. In this specific case, finetuning a model to generate dense summaries could decrease latency and cost significantly by replacing the iterative LLM calls that we make .</li> </ul> <p>By accomplishing these tasks, you'll gain practical experience in tuning your models to suit your specific tasks as well as build in more complex validation processes when working with LLMs to ensure more reliable, accurate and consistent outputs.</p>"},{"location":"tutorials/6-chain-of-density/#chain-of-density-summarization","title":"Chain Of Density Summarization\u00b6","text":""},{"location":"tutorials/6-chain-of-density/#introduction","title":"Introduction\u00b6","text":"<p>What is Chain Of Density summarization?</p> <p>Summarizing extensive texts with AI can be challenging. Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density.</p> <p>It was first introduced in the paper - From Sparse to Dense : GPT-4 Summarization with Chain of Density prompting.</p> <p>This was done in the original paper by asking GPT-4 to generate all of the rewritten summaries in a single go with the following prompt below.</p>"},{"location":"tutorials/6-chain-of-density/#setup-and-dependencies","title":"Setup and Dependencies\u00b6","text":"<p>We'll be using two new libraries for our demonstration</p> <ol> <li><code>spaCy</code> : This provides a handful of useful utilities to do generic NLP tasks with</li> <li><code>nltk</code> : This was used by the original paper to count the number of tokens in our generated summaries</li> </ol>"},{"location":"tutorials/6-chain-of-density/#definitions","title":"Definitions\u00b6","text":""},{"location":"tutorials/6-chain-of-density/#tokens-and-tokenizers","title":"Tokens and Tokenizers\u00b6","text":"<p>In the original paper, the authors used <code>NLTK</code> to split the generated summary into tokens. These represent the smallest units that each sentence could be broken into where each hold semantic meaning.</p> <p>Let's walk through a simple example to see how the <code>NLTK</code> tokenizer might work</p>"},{"location":"tutorials/6-chain-of-density/#entities","title":"Entities\u00b6","text":"<p>A named entity is an object in the real-world that we identify using a name. Common examples include people, countries, products or even books that we know and love. We can use the <code>spaCy</code> library for us to be able to detect the number of entities in a given sentence.</p>"},{"location":"tutorials/6-chain-of-density/#entity-density","title":"Entity Density\u00b6","text":"<p>Now that we know what tokens and tokens are, we can move on to our last concept - that of entity density. Entity density is simply the mean number of entities present per token within your string of text.</p>"},{"location":"tutorials/6-chain-of-density/#implementation","title":"Implementation\u00b6","text":""},{"location":"tutorials/6-chain-of-density/#data-classes","title":"Data Classes\u00b6","text":"<p>Let's start by walking through some of the data models that we'll be using as the response_model for our open ai function calls. We'll need a total of two different classes</p> <ol> <li>Initial Summary: which is the lengthy and overly verbose article</li> <li>Rewritten Summary : which represents</li> </ol>"},{"location":"tutorials/6-chain-of-density/#putting-it-all-together","title":"Putting it all together\u00b6","text":"<p>Now that we have our models, let's implement a function to summarize a piece of text using a Chain Of Density summarization</p>"},{"location":"tutorials/6-chain-of-density/#trial-run","title":"Trial Run\u00b6","text":"<p>Let's try running this on some sample text which we can import in from our repository. We've provided a sample article in a file called <code>article.txt</code></p>"},{"location":"tutorials/6-chain-of-density/#future-steps","title":"Future Steps\u00b6","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/page/2/","title":"Welcome to the Instructor Blog","text":""}]}